{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting metrics for the first model\n",
    "\n",
    "Let's try to update my report with the metrics for the first model.\n",
    "\n",
    "As per the tests conducted on the first notebook, regarding the datasets (to find which datasets can be run in a reasonable time), we are going to use these default datasets:\n",
    "\n",
    "* `birds`\n",
    "* `emotions`\n",
    "* `scene`\n",
    "\n",
    "The **baseline** will be the regular Binary Relevance. Then we will compare those to the Basic Stacking approach. Finally, we will run the Stacking With F-Test, with `alpha=0.5`. All other parameters, of the other models, will be the default ones, using the `SVC` as the base classifier.\n",
    "\n",
    "The metrics we will use, as per what was defined in my report, are the hamming loss and the f1 score. We will use the `EvaluationPipeline` class to run the experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.dataset import load_dataset, available_data_sets\n",
    "from sklearn.svm import SVC\n",
    "from skmultilearn.base.problem_transformation import ProblemTransformationBase\n",
    "from typing import List, Optional, Any, Tuple, Dict\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import f_classif\n",
    "from evaluation import EvaluationPipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move this to an actual python file\n",
    "\n",
    "class BasicStacking(ProblemTransformationBase):\n",
    "    first_layer_classifiers: BinaryRelevance\n",
    "    second_layer_classifiers: BinaryRelevance\n",
    "\n",
    "    def __init__(self, classifier: Any = None, require_dense: Optional[List[bool]] = None):\n",
    "        super(BasicStacking, self).__init__(classifier, require_dense)\n",
    "\n",
    "        self.first_layer_classifiers = BinaryRelevance(\n",
    "            classifier=SVC(),\n",
    "            require_dense=[False, True]\n",
    "        )\n",
    "\n",
    "        self.second_layer_classifiers = BinaryRelevance(\n",
    "            classifier=SVC(),\n",
    "            require_dense=[False, True]\n",
    "        )\n",
    "    \n",
    "    def fit(self, X: Any, y: Any):\n",
    "        self.first_layer_classifiers.fit(X, y)\n",
    "\n",
    "        first_layer_predictions = self.first_layer_classifiers.predict(X)\n",
    "        X_expanded = np.hstack([X.todense(), first_layer_predictions.todense()])\n",
    "\n",
    "        self.second_layer_classifiers.fit(X_expanded, y)\n",
    "    \n",
    "    def predict(self, X: Any):\n",
    "        first_layer_predictions = self.first_layer_classifiers.predict(X)\n",
    "        X_expanded = np.hstack([X.todense(), first_layer_predictions.todense()])\n",
    "        return self.second_layer_classifiers.predict(X_expanded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO move this to an actual python file\n",
    "\n",
    "class StackingWithFTests(ProblemTransformationBase):\n",
    "    alpha: float\n",
    "    use_first_layer_to_calculate_correlations: bool\n",
    "    \n",
    "    first_layer_classifiers: BinaryRelevance\n",
    "    second_layer_classifiers: List[Any] # TODO should be any generic type of classifier\n",
    "    labels_count: int\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha: float = 0.5,\n",
    "        use_first_layer_to_calculate_correlations: bool = False,\n",
    "        classifier: Any = None,\n",
    "        require_dense: Optional[List[bool]] = None\n",
    "    ):\n",
    "        super(StackingWithFTests, self).__init__(classifier, require_dense)\n",
    "\n",
    "        if alpha < 0.0 or alpha > 1.0:\n",
    "            raise Exception(\"alpha must be >= 0.0 and <= 1.0\")\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.use_first_layer_to_calculate_correlations = use_first_layer_to_calculate_correlations\n",
    "        \n",
    "        self.first_layer_classifiers = BinaryRelevance(\n",
    "            classifier=SVC(),\n",
    "            require_dense=[False, True]\n",
    "        )\n",
    "        # TODO: allow for any base model (base classifier) to be used\n",
    "        # right now I am forcing the use of SVC\n",
    "\n",
    "        self.second_layer_classifiers = []\n",
    "        self.correlated_labels_map = pd.DataFrame()\n",
    "        self.labels_count = 0\n",
    "\n",
    "\n",
    "    def fit(self, X: Any, y: Any):\n",
    "        self.labels_count = y.shape[1]\n",
    "\n",
    "        self.first_layer_classifiers.fit(X, y)\n",
    "        \n",
    "        label_classifications = y\n",
    "        if self.use_first_layer_to_calculate_correlations:\n",
    "            label_classifications = self.first_layer_classifiers.predict(X)\n",
    "\n",
    "        f_tested_label_pairs = self.calculate_f_test_for_all_label_pairs(label_classifications)\n",
    "        self.correlated_labels_map = self.get_map_of_correlated_labels(f_tested_label_pairs)\n",
    "\n",
    "        for i in range(self.labels_count):\n",
    "            mask = self.correlated_labels_map[\"for_label\"] == i\n",
    "            split_df = self.correlated_labels_map[mask].reset_index(drop=True)\n",
    "            labels_to_expand = split_df[\"expand_this_label\"].to_list()\n",
    "\n",
    "            additional_input = label_classifications.todense()[:, labels_to_expand]\n",
    "            \n",
    "            X_expanded = np.hstack([X.todense(), additional_input])\n",
    "            X_expanded = np.asarray(X_expanded)\n",
    "\n",
    "            y_label_specific = y.todense()[:, i]\n",
    "            y_label_specific = self.convert_matrix_to_vector(y_label_specific)\n",
    "\n",
    "            meta_classifier = SVC()\n",
    "            meta_classifier.fit(X_expanded, y_label_specific)\n",
    "\n",
    "            self.second_layer_classifiers.append(meta_classifier)\n",
    "            print(f\"finished training meta classifier for label {i}\")\n",
    "    \n",
    "    def calculate_f_test_for_all_label_pairs(self, label_classifications: Any) -> List[Dict[str, Any]]:\n",
    "        results = []\n",
    "\n",
    "        for i in range(0, self.labels_count):\n",
    "            for j in range(0, self.labels_count):\n",
    "                if i == j:\n",
    "                    continue\n",
    "\n",
    "                X = label_classifications.todense()[:, i]\n",
    "                base_label = self.convert_matrix_to_array(X)\n",
    "\n",
    "                y = label_classifications.todense()[:, j]\n",
    "                against_label = self.convert_matrix_to_vector(y)\n",
    "\n",
    "                f_test_result = f_classif(base_label, against_label)[0]\n",
    "\n",
    "                results.append({\n",
    "                    \"label_being_tested\": i,\n",
    "                    \"against_label\": j,\n",
    "                    \"f_test_result\": float(f_test_result)\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def convert_matrix_to_array(self, matrix: Any):\n",
    "        return np.asarray(matrix).reshape(-1, 1)\n",
    "\n",
    "    def convert_matrix_to_vector(self, matrix: Any):\n",
    "        return np.asarray(matrix).reshape(-1)\n",
    "    \n",
    "    def get_map_of_correlated_labels(self, f_test_results: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "        temp_df = pd.DataFrame(f_test_results)\n",
    "        \n",
    "        sorted_temp_df = temp_df.sort_values(\n",
    "            by=[\"label_being_tested\", \"f_test_result\"],\n",
    "            ascending=[True, False])\n",
    "        # ordering in descending order by the F-test result,\n",
    "        # following what the main article describes\n",
    "\n",
    "        selected_features = []\n",
    "\n",
    "        for i in range(0, self.labels_count):\n",
    "            mask = sorted_temp_df[\"label_being_tested\"] == i\n",
    "            split_df = sorted_temp_df[mask].reset_index(drop=True)\n",
    "\n",
    "            big_f = split_df[\"f_test_result\"].sum()\n",
    "            max_cum_f = self.alpha * big_f\n",
    "\n",
    "            cum_f = 0\n",
    "            for _, row in split_df.iterrows():\n",
    "                cum_f += row[\"f_test_result\"]\n",
    "                if cum_f > max_cum_f:\n",
    "                    break\n",
    "\n",
    "                selected_features.append({\n",
    "                    \"for_label\": i,\n",
    "                    \"expand_this_label\": int(row[\"against_label\"]),\n",
    "                    \"f_test_result\": float(row[\"f_test_result\"]),\n",
    "                })\n",
    "        \n",
    "        cols = [\"for_label\", \"expand_this_label\", \"f_test_result\"]\n",
    "        return pd.DataFrame(selected_features, columns=cols)\n",
    "    \n",
    "    def predict(self, X: Any) -> np.ndarray[Any,Any]:\n",
    "        if self.correlated_labels_map.columns.size == 0:\n",
    "            raise Exception(\"model was not trained yet\")\n",
    "\n",
    "        predictions = self.first_layer_classifiers.predict(X)\n",
    "        local_labels_count = predictions.shape[1]\n",
    "\n",
    "        second_layer_predictions = []\n",
    "\n",
    "        for i in range(local_labels_count):\n",
    "            mask = self.correlated_labels_map[\"for_label\"] == i\n",
    "            split_df = self.correlated_labels_map[mask].reset_index(drop=True)\n",
    "            labels_to_expand = split_df[\"expand_this_label\"].to_list()\n",
    "\n",
    "            additional_input = predictions.todense()[:, labels_to_expand]\n",
    "\n",
    "            X_expanded = np.hstack([X.todense(), additional_input])\n",
    "            X_expanded = np.asarray(X_expanded)\n",
    "\n",
    "            temp_preds = self.second_layer_classifiers[i].predict(X_expanded)\n",
    "            second_layer_predictions.append(temp_preds)\n",
    "\n",
    "        reshaped_array = np.asarray(second_layer_predictions).T\n",
    "        return reshaped_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting dataset `scene`\n",
      "scene:undivided - exists, not redownloading\n",
      "getting dataset `emotions`\n",
      "emotions:undivided - exists, not redownloading\n",
      "getting dataset `birds`\n",
      "birds:undivided - exists, not redownloading\n",
      "===\n",
      "information for dataset `scene`\n",
      "rows: 2407, labels: 6\n",
      "===\n",
      "information for dataset `emotions`\n",
      "rows: 593, labels: 6\n",
      "===\n",
      "information for dataset `birds`\n",
      "rows: 645, labels: 19\n"
     ]
    }
   ],
   "source": [
    "desired_datasets = [\"scene\", \"emotions\", \"birds\"]\n",
    "\n",
    "datasets = {}\n",
    "for dataset_name in desired_datasets:\n",
    "    print(f\"getting dataset `{dataset_name}`\")\n",
    "    \n",
    "    full_dataset = load_dataset(dataset_name, \"undivided\")\n",
    "    X, y, _, _ = full_dataset\n",
    "\n",
    "    datasets[dataset_name] = {\n",
    "        \"X\": X,\n",
    "        \"y\": y,\n",
    "        \"rows\": X.shape[0],\n",
    "        \"labels_count\": y.shape[1]\n",
    "    }\n",
    "\n",
    "\n",
    "for name, info in datasets.items():\n",
    "    print(\"===\")\n",
    "    print(f\"information for dataset `{name}`\")\n",
    "    print(f\"rows: {info['rows']}, labels: {info['labels_count']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_binary_relevance_model = BinaryRelevance(\n",
    "    classifier=SVC(),\n",
    "    require_dense=[False, True]\n",
    ")\n",
    "\n",
    "basic_stacking_model = BasicStacking()\n",
    "stacking_with_f_tests_model = StackingWithFTests(alpha=0.5)\n",
    "\n",
    "models = {\n",
    "    \"baseline_binary_relevance_model\": baseline_binary_relevance_model,\n",
    "    \"basic_stacking_model\": basic_stacking_model,\n",
    "    \"stacking_with_f_tests_model\": stacking_with_f_tests_model\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# running model `baseline_binary_relevance_model`\n",
      "## running dataset `scene`\n",
      "results obtained:\n",
      "Accuracy: 0.5268 ± 0.13\n",
      "Hamming Loss: -0.1020 ± 0.03\n",
      "## running dataset `emotions`\n",
      "results obtained:\n",
      "Accuracy: 0.0135 ± 0.01\n",
      "Hamming Loss: -0.3033 ± 0.02\n",
      "## running dataset `birds`\n",
      "results obtained:\n",
      "Accuracy: 0.4636 ± 0.05\n",
      "Hamming Loss: -0.0534 ± 0.00\n",
      "# running model `basic_stacking_model`\n",
      "## running dataset `scene`\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models.items():\n",
    "    print(f\"# running model `{model_name}`\")\n",
    "\n",
    "    evaluation_results[model_name] = {}\n",
    "\n",
    "    n_folds = 5\n",
    "    evaluation_pipeline = EvaluationPipeline(model, n_folds)\n",
    "\n",
    "    for dataset_name, info in datasets.items():\n",
    "        print(f\"## running dataset `{dataset_name}`\")\n",
    "\n",
    "        result = evaluation_pipeline.run(info[\"X\"], info[\"y\"])\n",
    "        evaluation_results[model_name][dataset_name] = result\n",
    "\n",
    "        print(f\"results obtained:\")\n",
    "        result.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([2.0240283 , 1.66600084]),\n",
       " 'score_time': array([1.19197035, 1.17500257]),\n",
       " 'test_accuracy': array([0.60880399, 0.58686617]),\n",
       " 'train_accuracy': array([0.719867  , 0.71345515]),\n",
       " 'test_hamming_loss': array([-0.08388704, -0.08464949]),\n",
       " 'train_hamming_loss': array([-0.05444722, -0.05647841])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_results[\"baseline_binary_relevance_model\"][\"scene\"].raw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

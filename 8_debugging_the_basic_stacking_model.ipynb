{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Debugging the `BasicStacking` model\n",
    "\n",
    "For some reason, and this is known ever since the first notebook, that the `BasicStacking` is producing the same results as the `BinaryRelevance` model, which is really weird. Also, the `StackingWithFTest`, when used with `alpha=1`, therefore working just like a `BasicStacking`, does **not** produce the same results.\n",
    "\n",
    "The `BasicStacking` is most likely not leveraging its second layer of classifiers as it should, and this is what we are going to debug in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.dataset import load_dataset\n",
    "from sklearn.svm import SVC\n",
    "from skmultilearn.base.problem_transformation import ProblemTransformationBase\n",
    "from typing import List, Optional, Any, Tuple, Dict\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import f_classif\n",
    "from evaluation import EvaluationPipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. `BasicStacking` code\n",
    "\n",
    "After this code is successfully debugged, it should be moved to a python file of its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move this to an actual python file\n",
    "\n",
    "class BasicStacking(ProblemTransformationBase):\n",
    "    first_layer_classifiers: BinaryRelevance\n",
    "    second_layer_classifiers: BinaryRelevance\n",
    "\n",
    "    def __init__(self, classifier: Any = None, require_dense: Optional[List[bool]] = None):\n",
    "        super(BasicStacking, self).__init__(classifier, require_dense)\n",
    "\n",
    "        self.first_layer_classifiers = BinaryRelevance(\n",
    "            classifier=SVC(),\n",
    "            require_dense=[False, True]\n",
    "        )\n",
    "\n",
    "        self.second_layer_classifiers = BinaryRelevance(\n",
    "            classifier=SVC(),\n",
    "            require_dense=[False, True]\n",
    "        )\n",
    "    \n",
    "    def fit(self, X: Any, y: Any):\n",
    "        self.first_layer_classifiers.fit(X, y)\n",
    "\n",
    "        first_layer_predictions = self.first_layer_classifiers.predict(X)\n",
    "        X_expanded = np.hstack([X.todense(), first_layer_predictions.todense()])\n",
    "\n",
    "        self.second_layer_classifiers.fit(X_expanded, y)\n",
    "    \n",
    "    def predict(self, X: Any):\n",
    "        first_layer_predictions = self.first_layer_classifiers.predict(X)\n",
    "        X_expanded = np.hstack([X.todense(), first_layer_predictions.todense()])\n",
    "        return self.second_layer_classifiers.predict(X_expanded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. Baseline results\n",
    "\n",
    "Let's get the results again for the `BinaryRelevance` and the `BasicStacking`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting dataset `scene`\n",
      "scene:undivided - exists, not redownloading\n",
      "scene:train - exists, not redownloading\n",
      "scene:test - exists, not redownloading\n",
      "getting dataset `emotions`\n",
      "emotions:undivided - exists, not redownloading\n",
      "emotions:train - exists, not redownloading\n",
      "emotions:test - exists, not redownloading\n",
      "getting dataset `birds`\n",
      "birds:undivided - exists, not redownloading\n",
      "birds:train - exists, not redownloading\n",
      "birds:test - exists, not redownloading\n",
      "===\n",
      "information for dataset `scene`\n",
      "rows: 2407, labels: 6\n",
      "===\n",
      "information for dataset `emotions`\n",
      "rows: 593, labels: 6\n",
      "===\n",
      "information for dataset `birds`\n",
      "rows: 645, labels: 19\n"
     ]
    }
   ],
   "source": [
    "desired_datasets = [\"scene\", \"emotions\", \"birds\"]\n",
    "\n",
    "datasets = {}\n",
    "for dataset_name in desired_datasets:\n",
    "    print(f\"getting dataset `{dataset_name}`\")\n",
    "    \n",
    "    full_dataset = load_dataset(dataset_name, \"undivided\")\n",
    "    X, y, _, _ = full_dataset\n",
    "\n",
    "    train_dataset = load_dataset(dataset_name, \"train\")\n",
    "    X_train, y_train, _, _ = train_dataset\n",
    "\n",
    "    test_dataset = load_dataset(dataset_name, \"test\")\n",
    "    X_test, y_test, _, _ = test_dataset\n",
    "\n",
    "    datasets[dataset_name] = {\n",
    "        \"X\": X,\n",
    "        \"y\": y,\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_test\": y_test,\n",
    "        \"rows\": X.shape[0],\n",
    "        \"labels_count\": y.shape[1]\n",
    "    }\n",
    "\n",
    "\n",
    "for name, info in datasets.items():\n",
    "    print(\"===\")\n",
    "    print(f\"information for dataset `{name}`\")\n",
    "    print(f\"rows: {info['rows']}, labels: {info['labels_count']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_binary_relevance_model = BinaryRelevance(\n",
    "    classifier=SVC(),\n",
    "    require_dense=[False, True]\n",
    ")\n",
    "\n",
    "basic_stacking_model = BasicStacking()\n",
    "\n",
    "models = {\n",
    "    \"baseline_binary_relevance_model\": baseline_binary_relevance_model,\n",
    "    \"basic_stacking_model\": basic_stacking_model,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# running model `baseline_binary_relevance_model`\n",
      "## running dataset `scene`\n",
      "results obtained:\n",
      "Accuracy: 0.5268 ± 0.13\n",
      "Hamming Loss: -0.1020 ± 0.03\n",
      "F1 score: 0.4207 ± 0.09\n",
      "## running dataset `emotions`\n",
      "results obtained:\n",
      "Accuracy: 0.0135 ± 0.01\n",
      "Hamming Loss: -0.3033 ± 0.02\n",
      "F1 score: 0.0576 ± 0.01\n",
      "## running dataset `birds`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results obtained:\n",
      "Accuracy: 0.4636 ± 0.05\n",
      "Hamming Loss: -0.0534 ± 0.00\n",
      "F1 score: 0.0128 ± 0.00\n",
      "# running model `basic_stacking_model`\n",
      "## running dataset `scene`\n",
      "results obtained:\n",
      "Accuracy: 0.5268 ± 0.13\n",
      "Hamming Loss: -0.1020 ± 0.03\n",
      "F1 score: 0.4207 ± 0.09\n",
      "## running dataset `emotions`\n",
      "results obtained:\n",
      "Accuracy: 0.0135 ± 0.01\n",
      "Hamming Loss: -0.3033 ± 0.02\n",
      "F1 score: 0.0576 ± 0.01\n",
      "## running dataset `birds`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results obtained:\n",
      "Accuracy: 0.4636 ± 0.05\n",
      "Hamming Loss: -0.0534 ± 0.00\n",
      "F1 score: 0.0128 ± 0.00\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"# running model `{model_name}`\")\n",
    "\n",
    "    evaluation_results[model_name] = {}\n",
    "\n",
    "    n_folds = 5\n",
    "    evaluation_pipeline = EvaluationPipeline(model, n_folds)\n",
    "\n",
    "    for dataset_name, info in datasets.items():\n",
    "        print(f\"## running dataset `{dataset_name}`\")\n",
    "\n",
    "        result = evaluation_pipeline.run(info[\"X\"], info[\"y\"])\n",
    "        evaluation_results[model_name][dataset_name] = result\n",
    "\n",
    "        print(f\"results obtained:\")\n",
    "        result.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model `baseline_binary_relevance_model`, dataset `scene`\n",
      "Accuracy: 0.5268 ± 0.13\n",
      "Hamming Loss: -0.1020 ± 0.03\n",
      "F1 score: 0.4207 ± 0.09\n",
      "\n",
      "model `baseline_binary_relevance_model`, dataset `emotions`\n",
      "Accuracy: 0.0135 ± 0.01\n",
      "Hamming Loss: -0.3033 ± 0.02\n",
      "F1 score: 0.0576 ± 0.01\n",
      "\n",
      "model `baseline_binary_relevance_model`, dataset `birds`\n",
      "Accuracy: 0.4636 ± 0.05\n",
      "Hamming Loss: -0.0534 ± 0.00\n",
      "F1 score: 0.0128 ± 0.00\n",
      "\n",
      "model `basic_stacking_model`, dataset `scene`\n",
      "Accuracy: 0.5268 ± 0.13\n",
      "Hamming Loss: -0.1020 ± 0.03\n",
      "F1 score: 0.4207 ± 0.09\n",
      "\n",
      "model `basic_stacking_model`, dataset `emotions`\n",
      "Accuracy: 0.0135 ± 0.01\n",
      "Hamming Loss: -0.3033 ± 0.02\n",
      "F1 score: 0.0576 ± 0.01\n",
      "\n",
      "model `basic_stacking_model`, dataset `birds`\n",
      "Accuracy: 0.4636 ± 0.05\n",
      "Hamming Loss: -0.0534 ± 0.00\n",
      "F1 score: 0.0128 ± 0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model, data in evaluation_results.items():\n",
    "    for dataset, result in data.items():\n",
    "        print(f\"model `{model}`, dataset `{dataset}`\")\n",
    "        result.describe()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, results are truly identical among the two models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4. Actually debugging the `BasicStacking`\n",
    "\n",
    "Let's start by making sure that the second layer is being used, and that it receive more features than the first layer (it is supposed to get the predictions of the first layer as features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebuggingBasicStacking(ProblemTransformationBase):\n",
    "    first_layer_classifiers: BinaryRelevance\n",
    "    second_layer_classifiers: BinaryRelevance\n",
    "\n",
    "    def __init__(self, classifier: Any = None, require_dense: Optional[List[bool]] = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.first_layer_classifiers = BinaryRelevance(\n",
    "            classifier=SVC(),\n",
    "            require_dense=[False, True]\n",
    "        )\n",
    "\n",
    "        self.second_layer_classifiers = BinaryRelevance(\n",
    "            classifier=SVC(),\n",
    "            require_dense=[False, True]\n",
    "        )\n",
    "    \n",
    "    def fit(self, X: Any, y: Any):\n",
    "        print(f\"X shape is {X.shape}\")\n",
    "        self.first_layer_classifiers.fit(X, y)\n",
    "\n",
    "        first_layer_predictions = self.first_layer_classifiers.predict(X)\n",
    "        X_expanded = np.hstack([X.todense(), first_layer_predictions.todense()])\n",
    "\n",
    "        print(f\"X_extended shape is {X_expanded.shape}\")\n",
    "        self.second_layer_classifiers.fit(X_expanded, y)\n",
    "    \n",
    "    def predict(self, X: Any):\n",
    "        print(f\"PREDICT: X shape is {X.shape}\")\n",
    "        first_layer_predictions = self.first_layer_classifiers.predict(X)\n",
    "        X_expanded = np.hstack([X.todense(), first_layer_predictions.todense()])\n",
    "        print(f\"PREDICT: X_extended shape is {X_expanded.shape}\")\n",
    "        return self.second_layer_classifiers.predict(X_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape is (1211, 294)\n",
      "X_extended shape is (1211, 300)\n"
     ]
    }
   ],
   "source": [
    "# first test\n",
    "# m = DebuggingBasicStacking()\n",
    "# m.fit(datasets[\"scene\"][\"X_train\"], datasets[\"scene\"][\"y_train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape is (1211, 294)\n",
      "X_extended shape is (1211, 300)\n"
     ]
    }
   ],
   "source": [
    "# second test\n",
    "\n",
    "m = DebuggingBasicStacking()\n",
    "m.fit(datasets[\"scene\"][\"X_train\"], datasets[\"scene\"][\"y_train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first layer\n",
      "294\n",
      "294\n",
      "294\n",
      "294\n",
      "294\n",
      "294\n",
      "second layer\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(\"first layer\")\n",
    "for clf in m.first_layer_classifiers.classifiers_:\n",
    "    print(clf.n_features_in_)\n",
    "\n",
    "print(\"second layer\")\n",
    "for clf in m.second_layer_classifiers.classifiers_:\n",
    "    print(clf.n_features_in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First test: check shape of the features (shape of X)\n",
    "\n",
    "A few `print`s were added to the code to revel the shape of `X`, both at the first layer and at the second layer. Result:\n",
    "\n",
    "```\n",
    "X shape is (1211, 294)\n",
    "X_extended shape is (1211, 300)\n",
    "```\n",
    "\n",
    "**So, the second layer is indeed receiving more features than the first layer. This is good.**\n",
    "\n",
    "### Second test: check if the base classifier itself is being trained with the new features\n",
    "\n",
    "The existing properties being set by `BinaryRelevance` already make it possible to investigate each classifier of either the first or the second layer.\n",
    "\n",
    "The property `n_features_in`, from the [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html), inform how many features were observed during `fit()`. This is the property we are going to use to check if the base classifiers are being trained with the new features.\n",
    "\n",
    "```\n",
    "first layer\n",
    "294\n",
    "294\n",
    "294\n",
    "294\n",
    "294\n",
    "294\n",
    "second layer\n",
    "300\n",
    "300\n",
    "300\n",
    "300\n",
    "300\n",
    "300\n",
    "```\n",
    "\n",
    "**The base classifiers from the second layer are really receiving all the new features. This is good.**\n",
    "\n",
    "### Third test: check if the new features have values\n",
    "\n",
    "...\n",
    "\n",
    "### Fourth test: attempt to use `Stacking` from scikit to reimplement the `BasicStacking`\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

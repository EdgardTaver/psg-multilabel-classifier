{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Debugging the `BasicStacking` model\n",
    "\n",
    "For some reason, and this is known ever since the first notebook, that the `BasicStacking` is producing the same results as the `BinaryRelevance` model, which is really weird. Also, the `StackingWithFTest`, when used with `alpha=1`, therefore working just like a `BasicStacking`, does **not** produce the same results.\n",
    "\n",
    "The `BasicStacking` is most likely not leveraging its second layer of classifiers as it should, and this is what we are going to debug in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.dataset import load_dataset\n",
    "from sklearn.svm import SVC\n",
    "from skmultilearn.base.problem_transformation import ProblemTransformationBase\n",
    "from typing import List, Optional, Any, Tuple, Dict\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import f_classif\n",
    "from evaluation import EvaluationPipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. `BasicStacking` code\n",
    "\n",
    "After this code is successfully debugged, it should be moved to a python file of its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move this to an actual python file\n",
    "\n",
    "class BasicStacking(ProblemTransformationBase):\n",
    "    first_layer_classifiers: BinaryRelevance\n",
    "    second_layer_classifiers: BinaryRelevance\n",
    "\n",
    "    def __init__(self, classifier: Any = None, require_dense: Optional[List[bool]] = None):\n",
    "        super(BasicStacking, self).__init__(classifier, require_dense)\n",
    "\n",
    "        self.first_layer_classifiers = BinaryRelevance(\n",
    "            classifier=SVC(),\n",
    "            require_dense=[False, True]\n",
    "        )\n",
    "\n",
    "        self.second_layer_classifiers = BinaryRelevance(\n",
    "            classifier=SVC(),\n",
    "            require_dense=[False, True]\n",
    "        )\n",
    "    \n",
    "    def fit(self, X: Any, y: Any):\n",
    "        self.first_layer_classifiers.fit(X, y)\n",
    "\n",
    "        first_layer_predictions = self.first_layer_classifiers.predict(X)\n",
    "        X_expanded = np.hstack([X.todense(), first_layer_predictions.todense()])\n",
    "\n",
    "        self.second_layer_classifiers.fit(X_expanded, y)\n",
    "    \n",
    "    def predict(self, X: Any):\n",
    "        first_layer_predictions = self.first_layer_classifiers.predict(X)\n",
    "        X_expanded = np.hstack([X.todense(), first_layer_predictions.todense()])\n",
    "        return self.second_layer_classifiers.predict(X_expanded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. Baseline results\n",
    "\n",
    "Let's get the results again for the `BinaryRelevance` and the `BasicStacking`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting dataset `scene`\n",
      "scene:undivided - exists, not redownloading\n",
      "scene:train - exists, not redownloading\n",
      "scene:test - exists, not redownloading\n",
      "getting dataset `emotions`\n",
      "emotions:undivided - exists, not redownloading\n",
      "emotions:train - exists, not redownloading\n",
      "emotions:test - exists, not redownloading\n",
      "getting dataset `birds`\n",
      "birds:undivided - exists, not redownloading\n",
      "birds:train - exists, not redownloading\n",
      "birds:test - exists, not redownloading\n",
      "===\n",
      "information for dataset `scene`\n",
      "rows: 2407, labels: 6\n",
      "===\n",
      "information for dataset `emotions`\n",
      "rows: 593, labels: 6\n",
      "===\n",
      "information for dataset `birds`\n",
      "rows: 645, labels: 19\n"
     ]
    }
   ],
   "source": [
    "desired_datasets = [\"scene\", \"emotions\", \"birds\"]\n",
    "\n",
    "datasets = {}\n",
    "for dataset_name in desired_datasets:\n",
    "    print(f\"getting dataset `{dataset_name}`\")\n",
    "    \n",
    "    full_dataset = load_dataset(dataset_name, \"undivided\")\n",
    "    X, y, _, _ = full_dataset\n",
    "\n",
    "    train_dataset = load_dataset(dataset_name, \"train\")\n",
    "    X_train, y_train, _, _ = train_dataset\n",
    "\n",
    "    test_dataset = load_dataset(dataset_name, \"test\")\n",
    "    X_test, y_test, _, _ = test_dataset\n",
    "\n",
    "    datasets[dataset_name] = {\n",
    "        \"X\": X,\n",
    "        \"y\": y,\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_test\": y_test,\n",
    "        \"rows\": X.shape[0],\n",
    "        \"labels_count\": y.shape[1]\n",
    "    }\n",
    "\n",
    "\n",
    "for name, info in datasets.items():\n",
    "    print(\"===\")\n",
    "    print(f\"information for dataset `{name}`\")\n",
    "    print(f\"rows: {info['rows']}, labels: {info['labels_count']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_binary_relevance_model = BinaryRelevance(\n",
    "    classifier=SVC(),\n",
    "    require_dense=[False, True]\n",
    ")\n",
    "\n",
    "basic_stacking_model = BasicStacking()\n",
    "\n",
    "models = {\n",
    "    \"baseline_binary_relevance_model\": baseline_binary_relevance_model,\n",
    "    \"basic_stacking_model\": basic_stacking_model,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# running model `baseline_binary_relevance_model`\n",
      "## running dataset `scene`\n",
      "results obtained:\n",
      "Accuracy: 0.5268 ± 0.13\n",
      "Hamming Loss: -0.1020 ± 0.03\n",
      "F1 score: 0.4207 ± 0.09\n",
      "## running dataset `emotions`\n",
      "results obtained:\n",
      "Accuracy: 0.0135 ± 0.01\n",
      "Hamming Loss: -0.3033 ± 0.02\n",
      "F1 score: 0.0576 ± 0.01\n",
      "## running dataset `birds`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results obtained:\n",
      "Accuracy: 0.4636 ± 0.05\n",
      "Hamming Loss: -0.0534 ± 0.00\n",
      "F1 score: 0.0128 ± 0.00\n",
      "# running model `basic_stacking_model`\n",
      "## running dataset `scene`\n",
      "results obtained:\n",
      "Accuracy: 0.5268 ± 0.13\n",
      "Hamming Loss: -0.1020 ± 0.03\n",
      "F1 score: 0.4207 ± 0.09\n",
      "## running dataset `emotions`\n",
      "results obtained:\n",
      "Accuracy: 0.0135 ± 0.01\n",
      "Hamming Loss: -0.3033 ± 0.02\n",
      "F1 score: 0.0576 ± 0.01\n",
      "## running dataset `birds`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results obtained:\n",
      "Accuracy: 0.4636 ± 0.05\n",
      "Hamming Loss: -0.0534 ± 0.00\n",
      "F1 score: 0.0128 ± 0.00\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"# running model `{model_name}`\")\n",
    "\n",
    "    evaluation_results[model_name] = {}\n",
    "\n",
    "    n_folds = 5\n",
    "    evaluation_pipeline = EvaluationPipeline(model, n_folds)\n",
    "\n",
    "    for dataset_name, info in datasets.items():\n",
    "        print(f\"## running dataset `{dataset_name}`\")\n",
    "\n",
    "        result = evaluation_pipeline.run(info[\"X\"], info[\"y\"])\n",
    "        evaluation_results[model_name][dataset_name] = result\n",
    "\n",
    "        print(f\"results obtained:\")\n",
    "        result.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model `baseline_binary_relevance_model`, dataset `scene`\n",
      "Accuracy: 0.5268 ± 0.13\n",
      "Hamming Loss: -0.1020 ± 0.03\n",
      "F1 score: 0.4207 ± 0.09\n",
      "\n",
      "model `baseline_binary_relevance_model`, dataset `emotions`\n",
      "Accuracy: 0.0135 ± 0.01\n",
      "Hamming Loss: -0.3033 ± 0.02\n",
      "F1 score: 0.0576 ± 0.01\n",
      "\n",
      "model `baseline_binary_relevance_model`, dataset `birds`\n",
      "Accuracy: 0.4636 ± 0.05\n",
      "Hamming Loss: -0.0534 ± 0.00\n",
      "F1 score: 0.0128 ± 0.00\n",
      "\n",
      "model `basic_stacking_model`, dataset `scene`\n",
      "Accuracy: 0.5268 ± 0.13\n",
      "Hamming Loss: -0.1020 ± 0.03\n",
      "F1 score: 0.4207 ± 0.09\n",
      "\n",
      "model `basic_stacking_model`, dataset `emotions`\n",
      "Accuracy: 0.0135 ± 0.01\n",
      "Hamming Loss: -0.3033 ± 0.02\n",
      "F1 score: 0.0576 ± 0.01\n",
      "\n",
      "model `basic_stacking_model`, dataset `birds`\n",
      "Accuracy: 0.4636 ± 0.05\n",
      "Hamming Loss: -0.0534 ± 0.00\n",
      "F1 score: 0.0128 ± 0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model, data in evaluation_results.items():\n",
    "    for dataset, result in data.items():\n",
    "        print(f\"model `{model}`, dataset `{dataset}`\")\n",
    "        result.describe()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, results are truly identical among the two models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4. Actually debugging the `BasicStacking`\n",
    "\n",
    "Let's start by making sure that the second layer is being used, and that it receive more features than the first layer (it is supposed to get the predictions of the first layer as features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class DebuggingBasicStacking(ProblemTransformationBase):\n",
    "    first_layer_classifiers: BinaryRelevance\n",
    "    second_layer_classifiers: BinaryRelevance\n",
    "\n",
    "    def __init__(self, classifier: Any = SVC(), require_dense: Optional[List[bool]] = None):\n",
    "        super().__init__()\n",
    "\n",
    "        first_base_classifier = copy.deepcopy(classifier)\n",
    "        second_base_classifier = copy.deepcopy(classifier)\n",
    "\n",
    "        print(\"Same object check\", first_base_classifier is second_base_classifier)\n",
    "\n",
    "        self.first_layer_classifiers = BinaryRelevance(\n",
    "            classifier=first_base_classifier,\n",
    "            require_dense=[False, True]\n",
    "        )\n",
    "\n",
    "        self.second_layer_classifiers = BinaryRelevance(\n",
    "            classifier=second_base_classifier,\n",
    "            require_dense=[False, True]\n",
    "        )\n",
    "    \n",
    "    def fit(self, X: Any, y: Any):\n",
    "        print(f\"FIT: X shape is {X.shape}\")\n",
    "        self.first_layer_classifiers.fit(X, y)\n",
    "\n",
    "        first_layer_predictions = self.first_layer_classifiers.predict(X)\n",
    "        formatted_first_layer_predictions = first_layer_predictions.todense()\n",
    "        X_expanded = np.hstack([X.todense(), formatted_first_layer_predictions])\n",
    "\n",
    "        first_layer_sum = np.sum(np.sum(formatted_first_layer_predictions, axis=1))\n",
    "        print(\"FIT: summing the values (for first layer):\", first_layer_sum)\n",
    "\n",
    "        print(f\"FIT: X_extended shape is {X_expanded.shape}\")\n",
    "        self.second_layer_classifiers.fit(X_expanded, y)\n",
    "    \n",
    "    def predict(self, X: Any): # type: ignore\n",
    "        print(f\"PREDICT: X shape is {X.shape}\")\n",
    "        first_layer_predictions = self.first_layer_classifiers.predict(X)\n",
    "        formatted_first_layer_predictions = first_layer_predictions.todense()\n",
    "\n",
    "        X_expanded = np.hstack([X.todense(), formatted_first_layer_predictions])\n",
    "\n",
    "        print(\"PREDICT: summing the values (for first layer):\", np.sum(np.sum(formatted_first_layer_predictions, axis=1)))\n",
    "        print(f\"PREDICT: X_extended shape is {X_expanded.shape}\")\n",
    "\n",
    "        second_layer_predictions = self.second_layer_classifiers.predict(X_expanded)\n",
    "        formatted_second_layer_predictions = second_layer_predictions.todense()\n",
    "\n",
    "        print(\"PREDICT: summing the values (for second layer):\", np.sum(np.sum(formatted_second_layer_predictions, axis=1)))\n",
    "\n",
    "        return second_layer_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape is (1211, 294)\n",
      "X_extended shape is (1211, 300)\n"
     ]
    }
   ],
   "source": [
    "# first test\n",
    "\n",
    "m = DebuggingBasicStacking()\n",
    "m.fit(datasets[\"scene\"][\"X_train\"], datasets[\"scene\"][\"y_train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape is (1211, 294)\n",
      "X_extended shape is (1211, 300)\n"
     ]
    }
   ],
   "source": [
    "# second test\n",
    "\n",
    "m = DebuggingBasicStacking()\n",
    "m.fit(datasets[\"scene\"][\"X_train\"], datasets[\"scene\"][\"y_train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first layer\n",
      "294\n",
      "294\n",
      "294\n",
      "294\n",
      "294\n",
      "294\n",
      "second layer\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(\"first layer\")\n",
    "for clf in m.first_layer_classifiers.classifiers_:\n",
    "    print(clf.n_features_in_)\n",
    "\n",
    "print(\"second layer\")\n",
    "for clf in m.second_layer_classifiers.classifiers_:\n",
    "    print(clf.n_features_in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIT: X shape is (1211, 294)\n",
      "FIT: Summing the values: 1019\n",
      "FIT: X_extended shape is (1211, 300)\n",
      "PREDICT: X shape is (1196, 294)\n",
      "Summing the values: 901\n",
      "PREDICT: X_extended shape is (1196, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1196x6 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 901 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# third test\n",
    "\n",
    "m = DebuggingBasicStacking()\n",
    "m.fit(datasets[\"scene\"][\"X_train\"], datasets[\"scene\"][\"y_train\"])\n",
    "m.predict(datasets[\"scene\"][\"X_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14024222405725295"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1019/(1211 * 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12555741360089187"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "901/(1196 * 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2585"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- checking amount of values found in the actual dataset\n",
    "# that is, values for the labels\n",
    "\n",
    "np.sum(np.sum(datasets[\"scene\"][\"y\"].todense(), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2407, 6)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"scene\"][\"y\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17899182938651156"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2585/(2407 * 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "901"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1196, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fourth test\n",
    "\n",
    "pure_br = BinaryRelevance(\n",
    "    classifier=SVC(random_state=42),\n",
    "    require_dense=[False, True]\n",
    ")\n",
    "\n",
    "pure_br.fit(datasets[\"scene\"][\"X_train\"], datasets[\"scene\"][\"y_train\"])\n",
    "preds = pure_br.predict(datasets[\"scene\"][\"X_test\"])\n",
    "\n",
    "display(np.sum(np.sum(preds.todense(), axis=1)))\n",
    "display(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12555741360089187"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "901/(1196*6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "901"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1196, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pure_br = BinaryRelevance(\n",
    "    classifier=SVC(random_state=94589045),\n",
    "    require_dense=[False, True]\n",
    ")\n",
    "\n",
    "pure_br.fit(datasets[\"scene\"][\"X_train\"], datasets[\"scene\"][\"y_train\"])\n",
    "preds = pure_br.predict(datasets[\"scene\"][\"X_test\"])\n",
    "\n",
    "display(np.sum(np.sum(preds.todense(), axis=1)))\n",
    "display(preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is really weird. Regardless of the `random_state`, the results are always the same. Let's try another base classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "791"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1196, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_br = BinaryRelevance(\n",
    "    classifier=RandomForestClassifier(random_state=42),\n",
    "    require_dense=[False, True]\n",
    ")\n",
    "\n",
    "rf_br.fit(datasets[\"scene\"][\"X_train\"], datasets[\"scene\"][\"y_train\"])\n",
    "preds = rf_br.predict(datasets[\"scene\"][\"X_test\"])\n",
    "\n",
    "display(np.sum(np.sum(preds.todense(), axis=1)))\n",
    "display(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "811"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1196, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf_br = BinaryRelevance(\n",
    "    classifier=RandomForestClassifier(random_state=43434242),\n",
    "    require_dense=[False, True]\n",
    ")\n",
    "\n",
    "rf_br.fit(datasets[\"scene\"][\"X_train\"], datasets[\"scene\"][\"y_train\"])\n",
    "preds = rf_br.predict(datasets[\"scene\"][\"X_test\"])\n",
    "\n",
    "display(np.sum(np.sum(preds.todense(), axis=1)))\n",
    "display(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "792"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1196, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf_br = BinaryRelevance(\n",
    "    classifier=RandomForestClassifier(),\n",
    "    require_dense=[False, True]\n",
    ")\n",
    "\n",
    "rf_br.fit(datasets[\"scene\"][\"X_train\"], datasets[\"scene\"][\"y_train\"])\n",
    "preds = rf_br.predict(datasets[\"scene\"][\"X_test\"])\n",
    "\n",
    "display(np.sum(np.sum(preds.todense(), axis=1)))\n",
    "display(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIT: X shape is (1211, 294)\n",
      "FIT: summing the values (for first layer): 1019\n",
      "FIT: X_extended shape is (1211, 300)\n",
      "PREDICT: X shape is (1196, 294)\n",
      "PREDICT: summing the values (for first layer): 901\n",
      "PREDICT: X_extended shape is (1196, 300)\n",
      "PREDICT: summing the values (for second layer): 901\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1196x6 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 901 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = DebuggingBasicStacking()\n",
    "m.fit(datasets[\"scene\"][\"X_train\"], datasets[\"scene\"][\"y_train\"])\n",
    "m.predict(datasets[\"scene\"][\"X_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same object check False\n",
      "FIT: X shape is (1211, 294)\n",
      "FIT: summing the values (for first layer): 1286\n",
      "FIT: X_extended shape is (1211, 300)\n",
      "PREDICT: X shape is (1196, 294)\n",
      "PREDICT: summing the values (for first layer): 797\n",
      "PREDICT: X_extended shape is (1196, 300)\n",
      "PREDICT: summing the values (for second layer): 793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1196x6 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 793 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fifth test\n",
    "\n",
    "m = DebuggingBasicStacking(classifier=RandomForestClassifier(random_state=42))\n",
    "m.fit(datasets[\"scene\"][\"X_train\"], datasets[\"scene\"][\"y_train\"])\n",
    "m.predict(datasets[\"scene\"][\"X_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same object check False\n",
      "FIT: X shape is (1211, 294)\n",
      "FIT: summing the values (for first layer): 1286\n",
      "FIT: X_extended shape is (1211, 300)\n",
      "PREDICT: X shape is (1196, 294)\n",
      "PREDICT: summing the values (for first layer): 791\n",
      "PREDICT: X_extended shape is (1196, 300)\n",
      "PREDICT: summing the values (for second layer): 792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1196x6 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 792 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = DebuggingBasicStacking(classifier=RandomForestClassifier(random_state=94389473))\n",
    "m.fit(datasets[\"scene\"][\"X_train\"], datasets[\"scene\"][\"y_train\"])\n",
    "m.predict(datasets[\"scene\"][\"X_test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First test: check shape of the features (shape of X)\n",
    "\n",
    "A few `print`s were added to the code to revel the shape of `X`, both at the first layer and at the second layer. Result:\n",
    "\n",
    "```\n",
    "X shape is (1211, 294)\n",
    "X_extended shape is (1211, 300)\n",
    "```\n",
    "\n",
    "**So, the second layer is indeed receiving more features than the first layer. This is good.**\n",
    "\n",
    "### Second test: check if the base classifier itself is being trained with the new features\n",
    "\n",
    "The existing properties being set by `BinaryRelevance` already make it possible to investigate each classifier of either the first or the second layer.\n",
    "\n",
    "The property `n_features_in`, from the [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html), inform how many features were observed during `fit()`. This is the property we are going to use to check if the base classifiers are being trained with the new features.\n",
    "\n",
    "```\n",
    "first layer\n",
    "294\n",
    "294\n",
    "294\n",
    "294\n",
    "294\n",
    "294\n",
    "second layer\n",
    "300\n",
    "300\n",
    "300\n",
    "300\n",
    "300\n",
    "300\n",
    "```\n",
    "\n",
    "**The base classifiers from the second layer are really receiving all the new features. This is good.**\n",
    "\n",
    "### Third test: check if the new features have values\n",
    "\n",
    "```\n",
    "FIT: X shape is (1211, 294)\n",
    "FIT: Summing the values: 1019\n",
    "FIT: X_extended shape is (1211, 300)\n",
    "PREDICT: X shape is (1196, 294)\n",
    "Summing the values: 901\n",
    "PREDICT: X_extended shape is (1196, 300)\n",
    "```\n",
    "\n",
    "**The extended features really have values, and they are not all zeros. This is good**. We can also see that they differ from `fit` to `predict`, which is also showing that we are _not_ doing a simple mistake such as using the same data in both places.\n",
    "\n",
    "The share of zeros, however, is something to consider. For the `fit` phase, only 14% of the values have a value different from zero. For the `predict` phase, this share is 12%. Both values are less than the share found in the full dataset, which is around ~17%.\n",
    "\n",
    "Is it possible that the \"stacking part\" is making the model drop its performance? **It might be worthy to check a pure `BinaryRelevance` output**.\n",
    "\n",
    "At first, iy might be tempting to think that the problem is the dataset being used. But remember that in the first notebook, we conduct a very similar test, comparing the output of `BinaryRelevance` and `BasicStacking`, and the results were identical for all datasets tested.\n",
    "\n",
    "### Fourth test: testing a pure `BinaryRelevance` model\n",
    "\n",
    "~A pure `BinaryRelevance` manages to be _worse_ than the first layer of the `BasicStacking` model. This is really weird. It only gets 901 labels, instead of 1019.~ A pure `BinaryRelevance` is performing exactly equal to the first layer of the `BasicStacking`, which is actually expected. What is weird is that the number of labels found is exactly the same: 901. **What is weird is that both the first and second layer return the exact same number of labels, regardless of the extra features being added to the second layer.**\n",
    "\n",
    "Could it be something with the `random_stage`? **But even after changing the `random_state` to absurd numbers, the results are always the same**. This is totally weird. It is like `SVC` is caching the results or something like that. Or maybe `BinaryRelevance` is keeping a cache of some sort. This is weird, as the source code of `scikit-multilearn` does _not_ describe any cache. It uses `copy.deepcopy`. Maybe this function is the one doing the cache thing? I made a quick Google search about this, but I found nothing conclusive.\n",
    "\n",
    "Or maybe this is down to how `SVC` works. With all its default parameters, it really _is_ stopping at the same point. ~But why it finds more labels when being used via the `BasicStacking` model? This cannot be explained by the `SVC` internal workings alone.~ Actually it is finding the same quantity of labels. I made some confusion because I compared the labels found during `fit` to those found during `predict`. If I compare the labels from `predict` only, then the results are exactly the same.\n",
    "\n",
    "When using different base classifiers, such as the `RandomForestClassifier`, the results are different. **It finds _less_ labels than `SVC`, but at least the results differ when running with different `random_state`s**.\n",
    "\n",
    "Perhaps the way forward is to use `RandomForestClassifier` instead of `SVC` as the base classifier.\n",
    "\n",
    "### Fifth test: using `RandomForestClassifier` as the base classifier\n",
    "\n",
    "The usage of `RandomForestClassifier` show that, by using other base classifier, it is possible to achieve different results for the first and second layer. In this case, the `BasicStacking` really differs from the `BinaryRelevance` model, which is what we wanted to achieve in the first place.\n",
    "\n",
    "The results are still **not** that great, as even the `RandomForestClassifier` generate actually little difference from the first to the second layer.\n",
    "\n",
    "### Sixth test: use scikit's `StackingClassifier`\n",
    "\n",
    "...\n",
    "\n",
    "### Seventh test: compare metrics to `StackingWithFTest` when alpha=1\n",
    "\n",
    "...\n",
    "\n",
    "### Eighth test: make sure that the `BasicStacking` expands all labels but the label being predicted\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

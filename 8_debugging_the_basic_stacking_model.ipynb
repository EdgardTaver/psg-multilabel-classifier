{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Debugging the `BasicStacking` model\n",
    "\n",
    "For some reason, and this is known ever since the first notebook, that the `BasicStacking` is producing the same results as the `BinaryRelevance` model, which is really weird. Also, the `StackingWithFTest`, when used with `alpha=1`, therefore working just like a `BasicStacking`, does **not** produce the same results.\n",
    "\n",
    "The `BasicStacking` is most likely not leveraging its second layer of classifiers as it should, and this is what we are going to debug in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.dataset import load_dataset\n",
    "from sklearn.svm import SVC\n",
    "from skmultilearn.base.problem_transformation import ProblemTransformationBase\n",
    "from typing import List, Optional, Any, Tuple, Dict\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import f_classif\n",
    "from evaluation import EvaluationPipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. `BasicStacking` code\n",
    "\n",
    "After this code is successfully debugged, it should be moved to a python file of its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move this to an actual python file\n",
    "\n",
    "class BasicStacking(ProblemTransformationBase):\n",
    "    first_layer_classifiers: BinaryRelevance\n",
    "    second_layer_classifiers: BinaryRelevance\n",
    "\n",
    "    def __init__(self, classifier: Any = None, require_dense: Optional[List[bool]] = None):\n",
    "        super(BasicStacking, self).__init__(classifier, require_dense)\n",
    "\n",
    "        self.first_layer_classifiers = BinaryRelevance(\n",
    "            classifier=SVC(),\n",
    "            require_dense=[False, True]\n",
    "        )\n",
    "\n",
    "        self.second_layer_classifiers = BinaryRelevance(\n",
    "            classifier=SVC(),\n",
    "            require_dense=[False, True]\n",
    "        )\n",
    "    \n",
    "    def fit(self, X: Any, y: Any):\n",
    "        self.first_layer_classifiers.fit(X, y)\n",
    "\n",
    "        first_layer_predictions = self.first_layer_classifiers.predict(X)\n",
    "        X_expanded = np.hstack([X.todense(), first_layer_predictions.todense()])\n",
    "\n",
    "        self.second_layer_classifiers.fit(X_expanded, y)\n",
    "    \n",
    "    def predict(self, X: Any):\n",
    "        first_layer_predictions = self.first_layer_classifiers.predict(X)\n",
    "        X_expanded = np.hstack([X.todense(), first_layer_predictions.todense()])\n",
    "        return self.second_layer_classifiers.predict(X_expanded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. Baseline results\n",
    "\n",
    "Let's get the results again for the `BinaryRelevance` and the `BasicStacking`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting dataset `scene`\n",
      "scene:undivided - exists, not redownloading\n",
      "scene:train - exists, not redownloading\n",
      "scene:test - exists, not redownloading\n",
      "getting dataset `emotions`\n",
      "emotions:undivided - exists, not redownloading\n",
      "emotions:train - exists, not redownloading\n",
      "emotions:test - exists, not redownloading\n",
      "getting dataset `birds`\n",
      "birds:undivided - exists, not redownloading\n",
      "birds:train - exists, not redownloading\n",
      "birds:test - exists, not redownloading\n",
      "===\n",
      "information for dataset `scene`\n",
      "rows: 2407, labels: 6\n",
      "===\n",
      "information for dataset `emotions`\n",
      "rows: 593, labels: 6\n",
      "===\n",
      "information for dataset `birds`\n",
      "rows: 645, labels: 19\n"
     ]
    }
   ],
   "source": [
    "desired_datasets = [\"scene\", \"emotions\", \"birds\"]\n",
    "\n",
    "datasets = {}\n",
    "for dataset_name in desired_datasets:\n",
    "    print(f\"getting dataset `{dataset_name}`\")\n",
    "    \n",
    "    full_dataset = load_dataset(dataset_name, \"undivided\")\n",
    "    X, y, _, _ = full_dataset\n",
    "\n",
    "    train_dataset = load_dataset(dataset_name, \"train\")\n",
    "    X_train, y_train, _, _ = train_dataset\n",
    "\n",
    "    test_dataset = load_dataset(dataset_name, \"test\")\n",
    "    X_test, y_test, _, _ = test_dataset\n",
    "\n",
    "    datasets[dataset_name] = {\n",
    "        \"X\": X,\n",
    "        \"y\": y,\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_test\": y_test,\n",
    "        \"rows\": X.shape[0],\n",
    "        \"labels_count\": y.shape[1]\n",
    "    }\n",
    "\n",
    "\n",
    "for name, info in datasets.items():\n",
    "    print(\"===\")\n",
    "    print(f\"information for dataset `{name}`\")\n",
    "    print(f\"rows: {info['rows']}, labels: {info['labels_count']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_binary_relevance_model = BinaryRelevance(\n",
    "    classifier=SVC(),\n",
    "    require_dense=[False, True]\n",
    ")\n",
    "\n",
    "basic_stacking_model = BasicStacking()\n",
    "\n",
    "models = {\n",
    "    \"baseline_binary_relevance_model\": baseline_binary_relevance_model,\n",
    "    \"basic_stacking_model\": basic_stacking_model,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# running model `baseline_binary_relevance_model`\n",
      "## running dataset `scene`\n",
      "results obtained:\n",
      "Accuracy: 0.5268 ± 0.13\n",
      "Hamming Loss: -0.1020 ± 0.03\n",
      "F1 score: 0.4207 ± 0.09\n",
      "## running dataset `emotions`\n",
      "results obtained:\n",
      "Accuracy: 0.0135 ± 0.01\n",
      "Hamming Loss: -0.3033 ± 0.02\n",
      "F1 score: 0.0576 ± 0.01\n",
      "## running dataset `birds`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results obtained:\n",
      "Accuracy: 0.4636 ± 0.05\n",
      "Hamming Loss: -0.0534 ± 0.00\n",
      "F1 score: 0.0128 ± 0.00\n",
      "# running model `basic_stacking_model`\n",
      "## running dataset `scene`\n",
      "results obtained:\n",
      "Accuracy: 0.5268 ± 0.13\n",
      "Hamming Loss: -0.1020 ± 0.03\n",
      "F1 score: 0.4207 ± 0.09\n",
      "## running dataset `emotions`\n",
      "results obtained:\n",
      "Accuracy: 0.0135 ± 0.01\n",
      "Hamming Loss: -0.3033 ± 0.02\n",
      "F1 score: 0.0576 ± 0.01\n",
      "## running dataset `birds`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results obtained:\n",
      "Accuracy: 0.4636 ± 0.05\n",
      "Hamming Loss: -0.0534 ± 0.00\n",
      "F1 score: 0.0128 ± 0.00\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"# running model `{model_name}`\")\n",
    "\n",
    "    evaluation_results[model_name] = {}\n",
    "\n",
    "    n_folds = 5\n",
    "    evaluation_pipeline = EvaluationPipeline(model, n_folds)\n",
    "\n",
    "    for dataset_name, info in datasets.items():\n",
    "        print(f\"## running dataset `{dataset_name}`\")\n",
    "\n",
    "        result = evaluation_pipeline.run(info[\"X\"], info[\"y\"])\n",
    "        evaluation_results[model_name][dataset_name] = result\n",
    "\n",
    "        print(f\"results obtained:\")\n",
    "        result.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model `baseline_binary_relevance_model`, dataset `scene`\n",
      "Accuracy: 0.5268 ± 0.13\n",
      "Hamming Loss: -0.1020 ± 0.03\n",
      "F1 score: 0.4207 ± 0.09\n",
      "\n",
      "model `baseline_binary_relevance_model`, dataset `emotions`\n",
      "Accuracy: 0.0135 ± 0.01\n",
      "Hamming Loss: -0.3033 ± 0.02\n",
      "F1 score: 0.0576 ± 0.01\n",
      "\n",
      "model `baseline_binary_relevance_model`, dataset `birds`\n",
      "Accuracy: 0.4636 ± 0.05\n",
      "Hamming Loss: -0.0534 ± 0.00\n",
      "F1 score: 0.0128 ± 0.00\n",
      "\n",
      "model `basic_stacking_model`, dataset `scene`\n",
      "Accuracy: 0.5268 ± 0.13\n",
      "Hamming Loss: -0.1020 ± 0.03\n",
      "F1 score: 0.4207 ± 0.09\n",
      "\n",
      "model `basic_stacking_model`, dataset `emotions`\n",
      "Accuracy: 0.0135 ± 0.01\n",
      "Hamming Loss: -0.3033 ± 0.02\n",
      "F1 score: 0.0576 ± 0.01\n",
      "\n",
      "model `basic_stacking_model`, dataset `birds`\n",
      "Accuracy: 0.4636 ± 0.05\n",
      "Hamming Loss: -0.0534 ± 0.00\n",
      "F1 score: 0.0128 ± 0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model, data in evaluation_results.items():\n",
    "    for dataset, result in data.items():\n",
    "        print(f\"model `{model}`, dataset `{dataset}`\")\n",
    "        result.describe()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, results are truly identical among the two models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4. Actually debugging the `BasicStacking`\n",
    "\n",
    "Let's start by making sure that the second layer is being used, and that it receive more features than the first layer (it is supposed to get the predictions of the first layer as features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class DebuggingBasicStacking(ProblemTransformationBase):\n",
    "    first_layer_classifiers: BinaryRelevance\n",
    "    second_layer_classifiers: BinaryRelevance\n",
    "\n",
    "    def __init__(self, classifier: Any = SVC(), require_dense: Optional[List[bool]] = None):\n",
    "        super().__init__()\n",
    "\n",
    "        first_base_classifier = copy.deepcopy(classifier)\n",
    "        second_base_classifier = copy.deepcopy(classifier)\n",
    "\n",
    "        print(\"Same object check\", first_base_classifier is second_base_classifier)\n",
    "\n",
    "        self.first_layer_classifiers = BinaryRelevance(\n",
    "            classifier=first_base_classifier,\n",
    "            require_dense=[False, True]\n",
    "        )\n",
    "\n",
    "        self.second_layer_classifiers = BinaryRelevance(\n",
    "            classifier=second_base_classifier,\n",
    "            require_dense=[False, True]\n",
    "        )\n",
    "    \n",
    "    def fit(self, X: Any, y: Any):\n",
    "        print(f\"FIT: X shape is {X.shape}\")\n",
    "        self.first_layer_classifiers.fit(X, y)\n",
    "\n",
    "        first_layer_predictions = self.first_layer_classifiers.predict(X)\n",
    "        formatted_first_layer_predictions = first_layer_predictions.todense()\n",
    "        X_expanded = np.hstack([X.todense(), formatted_first_layer_predictions])\n",
    "\n",
    "        first_layer_sum = np.sum(np.sum(formatted_first_layer_predictions, axis=1))\n",
    "        print(\"FIT: summing the values (for first layer):\", first_layer_sum)\n",
    "\n",
    "        print(f\"FIT: X_extended shape is {X_expanded.shape}\")\n",
    "        self.second_layer_classifiers.fit(X_expanded, y)\n",
    "    \n",
    "    def predict(self, X: Any): # type: ignore\n",
    "        print(f\"PREDICT: X shape is {X.shape}\")\n",
    "        first_layer_predictions = self.first_layer_classifiers.predict(X)\n",
    "        formatted_first_layer_predictions = first_layer_predictions.todense()\n",
    "\n",
    "        X_expanded = np.hstack([X.todense(), formatted_first_layer_predictions])\n",
    "\n",
    "        print(\"PREDICT: summing the values (for first layer):\", np.sum(np.sum(formatted_first_layer_predictions, axis=1)))\n",
    "        print(f\"PREDICT: X_extended shape is {X_expanded.shape}\")\n",
    "\n",
    "        second_layer_predictions = self.second_layer_classifiers.predict(X_expanded)\n",
    "        formatted_second_layer_predictions = second_layer_predictions.todense()\n",
    "\n",
    "        print(\"PREDICT: summing the values (for second layer):\", np.sum(np.sum(formatted_second_layer_predictions, axis=1)))\n",
    "\n",
    "        return second_layer_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape is (1211, 294)\n",
      "X_extended shape is (1211, 300)\n"
     ]
    }
   ],
   "source": [
    "# first test\n",
    "\n",
    "m = DebuggingBasicStacking()\n",
    "m.fit(datasets[\"scene\"][\"X_train\"], datasets[\"scene\"][\"y_train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape is (1211, 294)\n",
      "X_extended shape is (1211, 300)\n"
     ]
    }
   ],
   "source": [
    "# second test\n",
    "\n",
    "m = DebuggingBasicStacking()\n",
    "m.fit(datasets[\"scene\"][\"X_train\"], datasets[\"scene\"][\"y_train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first layer\n",
      "294\n",
      "294\n",
      "294\n",
      "294\n",
      "294\n",
      "294\n",
      "second layer\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "print(\"first layer\")\n",
    "for clf in m.first_layer_classifiers.classifiers_:\n",
    "    print(clf.n_features_in_)\n",
    "\n",
    "print(\"second layer\")\n",
    "for clf in m.second_layer_classifiers.classifiers_:\n",
    "    print(clf.n_features_in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIT: X shape is (1211, 294)\n",
      "FIT: Summing the values: 1019\n",
      "FIT: X_extended shape is (1211, 300)\n",
      "PREDICT: X shape is (1196, 294)\n",
      "Summing the values: 901\n",
      "PREDICT: X_extended shape is (1196, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1196x6 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 901 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# third test\n",
    "\n",
    "m = DebuggingBasicStacking()\n",
    "m.fit(datasets[\"scene\"][\"X_train\"], datasets[\"scene\"][\"y_train\"])\n",
    "m.predict(datasets[\"scene\"][\"X_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14024222405725295"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1019/(1211 * 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12555741360089187"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "901/(1196 * 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2585"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- checking amount of values found in the actual dataset\n",
    "# that is, values for the labels\n",
    "\n",
    "np.sum(np.sum(datasets[\"scene\"][\"y\"].todense(), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2407, 6)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"scene\"][\"y\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17899182938651156"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2585/(2407 * 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "901"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1196, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fourth test\n",
    "\n",
    "pure_br = BinaryRelevance(\n",
    "    classifier=SVC(random_state=42),\n",
    "    require_dense=[False, True]\n",
    ")\n",
    "\n",
    "pure_br.fit(datasets[\"scene\"][\"X_train\"], datasets[\"scene\"][\"y_train\"])\n",
    "preds = pure_br.predict(datasets[\"scene\"][\"X_test\"])\n",
    "\n",
    "display(np.sum(np.sum(preds.todense(), axis=1)))\n",
    "display(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12555741360089187"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "901/(1196*6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "901"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1196, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pure_br = BinaryRelevance(\n",
    "    classifier=SVC(random_state=94589045),\n",
    "    require_dense=[False, True]\n",
    ")\n",
    "\n",
    "pure_br.fit(datasets[\"scene\"][\"X_train\"], datasets[\"scene\"][\"y_train\"])\n",
    "preds = pure_br.predict(datasets[\"scene\"][\"X_test\"])\n",
    "\n",
    "display(np.sum(np.sum(preds.todense(), axis=1)))\n",
    "display(preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is really weird. Regardless of the `random_state`, the results are always the same. Let's try another base classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "791"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1196, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_br = BinaryRelevance(\n",
    "    classifier=RandomForestClassifier(random_state=42),\n",
    "    require_dense=[False, True]\n",
    ")\n",
    "\n",
    "rf_br.fit(datasets[\"scene\"][\"X_train\"], datasets[\"scene\"][\"y_train\"])\n",
    "preds = rf_br.predict(datasets[\"scene\"][\"X_test\"])\n",
    "\n",
    "display(np.sum(np.sum(preds.todense(), axis=1)))\n",
    "display(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "811"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1196, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf_br = BinaryRelevance(\n",
    "    classifier=RandomForestClassifier(random_state=43434242),\n",
    "    require_dense=[False, True]\n",
    ")\n",
    "\n",
    "rf_br.fit(datasets[\"scene\"][\"X_train\"], datasets[\"scene\"][\"y_train\"])\n",
    "preds = rf_br.predict(datasets[\"scene\"][\"X_test\"])\n",
    "\n",
    "display(np.sum(np.sum(preds.todense(), axis=1)))\n",
    "display(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "792"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1196, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf_br = BinaryRelevance(\n",
    "    classifier=RandomForestClassifier(),\n",
    "    require_dense=[False, True]\n",
    ")\n",
    "\n",
    "rf_br.fit(datasets[\"scene\"][\"X_train\"], datasets[\"scene\"][\"y_train\"])\n",
    "preds = rf_br.predict(datasets[\"scene\"][\"X_test\"])\n",
    "\n",
    "display(np.sum(np.sum(preds.todense(), axis=1)))\n",
    "display(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIT: X shape is (1211, 294)\n",
      "FIT: summing the values (for first layer): 1019\n",
      "FIT: X_extended shape is (1211, 300)\n",
      "PREDICT: X shape is (1196, 294)\n",
      "PREDICT: summing the values (for first layer): 901\n",
      "PREDICT: X_extended shape is (1196, 300)\n",
      "PREDICT: summing the values (for second layer): 901\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1196x6 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 901 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = DebuggingBasicStacking()\n",
    "m.fit(datasets[\"scene\"][\"X_train\"], datasets[\"scene\"][\"y_train\"])\n",
    "m.predict(datasets[\"scene\"][\"X_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same object check False\n",
      "FIT: X shape is (1211, 294)\n",
      "FIT: summing the values (for first layer): 1286\n",
      "FIT: X_extended shape is (1211, 300)\n",
      "PREDICT: X shape is (1196, 294)\n",
      "PREDICT: summing the values (for first layer): 797\n",
      "PREDICT: X_extended shape is (1196, 300)\n",
      "PREDICT: summing the values (for second layer): 793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1196x6 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 793 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fifth test\n",
    "\n",
    "m = DebuggingBasicStacking(classifier=RandomForestClassifier(random_state=42))\n",
    "m.fit(datasets[\"scene\"][\"X_train\"], datasets[\"scene\"][\"y_train\"])\n",
    "m.predict(datasets[\"scene\"][\"X_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same object check False\n",
      "FIT: X shape is (1211, 294)\n",
      "FIT: summing the values (for first layer): 1286\n",
      "FIT: X_extended shape is (1211, 300)\n",
      "PREDICT: X shape is (1196, 294)\n",
      "PREDICT: summing the values (for first layer): 791\n",
      "PREDICT: X_extended shape is (1196, 300)\n",
      "PREDICT: summing the values (for second layer): 792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1196x6 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 792 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = DebuggingBasicStacking(classifier=RandomForestClassifier(random_state=94389473))\n",
    "m.fit(datasets[\"scene\"][\"X_train\"], datasets[\"scene\"][\"y_train\"])\n",
    "m.predict(datasets[\"scene\"][\"X_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:1093: RuntimeWarning: Number of classes in training fold (6) does not match total number of classes (2). Results may not be appropriate for your use case. To fix this, use a cross-validation technique resulting in properly stratified folds\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'lil_matrix'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-e1a4461161a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# seen here: https://stackoverflow.com/questions/61309527/unable-to-do-stacking-for-a-multi-label-classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mwrapped_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"scene\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"X_train\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"scene\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"y_train\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapped_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"scene\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"X_test\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\multiclass.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    335\u001b[0m         \u001b[1;31m# n_jobs > 1 in can results in slower performance due to the overhead\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \u001b[1;31m# of spawning threads.  See joblib issue #112.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m         self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n\u001b[0m\u001b[0;32m    338\u001b[0m             delayed(_fit_binary)(\n\u001b[0;32m    339\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1039\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1041\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\multiclass.py\u001b[0m in \u001b[0;36m_fit_binary\u001b[1;34m(estimator, X, y, classes)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_stacking.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_le\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    485\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"final_estimator_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\ensemble\\_stacking.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    191\u001b[0m             \u001b[1;33m{\u001b[0m\u001b[1;34m\"sample_weight\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m         )\n\u001b[1;32m--> 193\u001b[1;33m         predictions = Parallel(n_jobs=self.n_jobs)(\n\u001b[0m\u001b[0;32m    194\u001b[0m             delayed(cross_val_predict)(\n\u001b[0;32m    195\u001b[0m                 \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1039\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1041\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_predict\u001b[1;34m(estimator, X, y, groups, cv, n_jobs, verbose, fit_params, pre_dispatch, method)\u001b[0m\n\u001b[0;32m    961\u001b[0m     \u001b[1;31m# independent, and that it is pickle-able.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 963\u001b[1;33m     predictions = parallel(\n\u001b[0m\u001b[0;32m    964\u001b[0m         delayed(_fit_and_predict)(\n\u001b[0;32m    965\u001b[0m             \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1039\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1041\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_predict\u001b[1;34m(estimator, X, y, train, test, verbose, fit_params, method)\u001b[0m\n\u001b[0;32m   1066\u001b[0m             \u001b[1;31m# A 2D y array should be a binary label indicator matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1067\u001b[0m             \u001b[0mn_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1068\u001b[1;33m             predictions = _enforce_prediction_order(\n\u001b[0m\u001b[0;32m   1069\u001b[0m                 \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m             )\n",
      "\u001b[1;32mc:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_enforce_prediction_order\u001b[1;34m(classes, predictions, n_classes, method)\u001b[0m\n\u001b[0;32m   1133\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         )\n\u001b[1;32m-> 1135\u001b[1;33m         \u001b[0mpredictions_for_all_classes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredictions_for_all_classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# sixth test\n",
    "\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# class FixedBR(BinaryRelevance):\n",
    "#     def fit(self, X: Any, y: Any):\n",
    "#         self.classes_ = [0,1,2,3,4,5]\n",
    "#         super().fit(X, y)\n",
    "\n",
    "base_rf_br = FixedBR(\n",
    "    classifier=RandomForestClassifier(random_state=42),\n",
    "    require_dense=[False, True]\n",
    ")\n",
    "# base_rf_br.classes_ = [0,1,2,3,4,5]\n",
    "\n",
    "final_rf_br = FixedBR(\n",
    "    classifier=RandomForestClassifier(random_state=42),\n",
    "    require_dense=[False, True]\n",
    ")\n",
    "# final_rf_br.classes_ = base_rf_br.classes_\n",
    "# notice: these commented out lines show attempts to set the `classes_` property\n",
    "# of the BinaryRelevance class, but it does not work as expected\n",
    "\n",
    "clf = StackingClassifier(\n",
    "    estimators=[ (\"base_rf_br\", base_rf_br) ],\n",
    "    final_estimator= final_rf_br,\n",
    "    passthrough=True\n",
    "    )\n",
    "\n",
    "wrapped_clf = OneVsRestClassifier(clf)\n",
    "# StackingClassifier does not natively support multilabel classification\n",
    "# so we have to wrap it in a OneVsRestClassifier (https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html)\n",
    "# seen here: https://stackoverflow.com/questions/61309527/unable-to-do-stacking-for-a-multi-label-classifier\n",
    "\n",
    "wrapped_clf.fit(datasets[\"scene\"][\"X_train\"], datasets[\"scene\"][\"y_train\"])\n",
    "preds = wrapped_clf.predict(datasets[\"scene\"][\"X_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another attempt, without using `BinaryRelevance`\n",
    "\n",
    "starting_classifier = RandomForestClassifier(random_state=42)\n",
    "final_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "clf = StackingClassifier(\n",
    "    estimators=[ (\"rf\", starting_classifier) ],\n",
    "    final_estimator= final_classifier,\n",
    "    passthrough=True\n",
    "    )\n",
    "\n",
    "wrapped_clf = OneVsRestClassifier(clf)\n",
    "# StackingClassifier does not natively support multilabel classification\n",
    "# so we have to wrap it in a OneVsRestClassifier (https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html)\n",
    "# seen here: https://stackoverflow.com/questions/61309527/unable-to-do-stacking-for-a-multi-label-classifier\n",
    "\n",
    "wrapped_clf.fit(datasets[\"scene\"][\"X_train\"], datasets[\"scene\"][\"y_train\"])\n",
    "preds = wrapped_clf.predict(datasets[\"scene\"][\"X_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0907190635451505"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6830985019980916"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(metrics.hamming_loss(datasets[\"scene\"][\"y_test\"], preds))\n",
    "display(metrics.f1_score(datasets[\"scene\"][\"y_test\"], preds, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    5.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    5.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    4.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    4.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    5.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    5.0s finished\n"
     ]
    }
   ],
   "source": [
    "# trying to better understand the \"architecture\" being used here\n",
    "\n",
    "starting_classifier = RandomForestClassifier(random_state=42)\n",
    "final_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "clf = StackingClassifier(\n",
    "    estimators=[ (\"rf\", starting_classifier) ],\n",
    "    final_estimator= final_classifier,\n",
    "    passthrough=True,\n",
    "    verbose=1,\n",
    "    )\n",
    "\n",
    "wrapped_clf = OneVsRestClassifier(clf)\n",
    "# StackingClassifier does not natively support multilabel classification\n",
    "# so we have to wrap it in a OneVsRestClassifier (https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html)\n",
    "# seen here: https://stackoverflow.com/questions/61309527/unable-to-do-stacking-for-a-multi-label-classifier\n",
    "\n",
    "wrapped_clf.fit(datasets[\"scene\"][\"X_train\"], datasets[\"scene\"][\"y_train\"])\n",
    "preds = wrapped_clf.predict(datasets[\"scene\"][\"X_test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StackingClassifier(estimators=[('rf', RandomForestClassifier(random_state=42))],\n",
       "                    final_estimator=RandomForestClassifier(random_state=42),\n",
       "                    passthrough=True, verbose=1),\n",
       " StackingClassifier(estimators=[('rf', RandomForestClassifier(random_state=42))],\n",
       "                    final_estimator=RandomForestClassifier(random_state=42),\n",
       "                    passthrough=True, verbose=1),\n",
       " StackingClassifier(estimators=[('rf', RandomForestClassifier(random_state=42))],\n",
       "                    final_estimator=RandomForestClassifier(random_state=42),\n",
       "                    passthrough=True, verbose=1),\n",
       " StackingClassifier(estimators=[('rf', RandomForestClassifier(random_state=42))],\n",
       "                    final_estimator=RandomForestClassifier(random_state=42),\n",
       "                    passthrough=True, verbose=1),\n",
       " StackingClassifier(estimators=[('rf', RandomForestClassifier(random_state=42))],\n",
       "                    final_estimator=RandomForestClassifier(random_state=42),\n",
       "                    passthrough=True, verbose=1),\n",
       " StackingClassifier(estimators=[('rf', RandomForestClassifier(random_state=42))],\n",
       "                    final_estimator=RandomForestClassifier(random_state=42),\n",
       "                    passthrough=True, verbose=1)]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_clf.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RandomForestClassifier(random_state=42)]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_clf.estimators_[0].estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_clf.estimators_[0].estimators_[0].n_features_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_clf.estimators_[0].final_estimator_.n_features_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08974358974358974"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.684223851314326"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# trying to replicate simple binary relevance results\n",
    "\n",
    "base_classifier = RandomForestClassifier(random_state=42)\n",
    "br_simulator = OneVsRestClassifier(base_classifier)\n",
    "br_simulator.fit(datasets[\"scene\"][\"X_train\"], datasets[\"scene\"][\"y_train\"])\n",
    "preds = br_simulator.predict(datasets[\"scene\"][\"X_test\"])\n",
    "\n",
    "display(metrics.hamming_loss(datasets[\"scene\"][\"y_test\"], preds))\n",
    "display(metrics.f1_score(datasets[\"scene\"][\"y_test\"], preds, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294\n",
      "294\n",
      "294\n",
      "294\n",
      "294\n",
      "294\n"
     ]
    }
   ],
   "source": [
    "for clf in br_simulator.estimators_:\n",
    "    print(clf.n_features_in_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "br_simulator.multilabel_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08974358974358974"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.684223851314326"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "actual_br = BinaryRelevance(\n",
    "    classifier=RandomForestClassifier(random_state=42),\n",
    "    require_dense=[False, True]\n",
    ")\n",
    "\n",
    "actual_br.fit(datasets[\"scene\"][\"X_train\"], datasets[\"scene\"][\"y_train\"])\n",
    "preds = actual_br.predict(datasets[\"scene\"][\"X_test\"])\n",
    "\n",
    "display(metrics.hamming_loss(datasets[\"scene\"][\"y_test\"], preds))\n",
    "display(metrics.f1_score(datasets[\"scene\"][\"y_test\"], preds, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same object check False\n",
      "FIT: X shape is (1211, 294)\n",
      "FIT: summing the values (for first layer): 1286\n",
      "FIT: X_extended shape is (1211, 300)\n",
      "PREDICT: X shape is (1196, 294)\n",
      "PREDICT: summing the values (for first layer): 791\n",
      "PREDICT: X_extended shape is (1196, 300)\n",
      "PREDICT: summing the values (for second layer): 790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.08960423634336678"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.684628539219014"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_stacking = DebuggingBasicStacking(\n",
    "    classifier=RandomForestClassifier(random_state=42),\n",
    "    require_dense=[False, True]\n",
    ")\n",
    "\n",
    "my_stacking.fit(datasets[\"scene\"][\"X_train\"], datasets[\"scene\"][\"y_train\"])\n",
    "preds = my_stacking.predict(datasets[\"scene\"][\"X_test\"])\n",
    "\n",
    "display(metrics.hamming_loss(datasets[\"scene\"][\"y_test\"], preds))\n",
    "display(metrics.f1_score(datasets[\"scene\"][\"y_test\"], preds, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First test: check shape of the features (shape of X)\n",
    "\n",
    "A few `print`s were added to the code to revel the shape of `X`, both at the first layer and at the second layer. Result:\n",
    "\n",
    "```\n",
    "X shape is (1211, 294)\n",
    "X_extended shape is (1211, 300)\n",
    "```\n",
    "\n",
    "**So, the second layer is indeed receiving more features than the first layer. This is good.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second test: check if the base classifier itself is being trained with the new features\n",
    "\n",
    "The existing properties being set by `BinaryRelevance` already make it possible to investigate each classifier of either the first or the second layer.\n",
    "\n",
    "The property `n_features_in`, from the [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html), inform how many features were observed during `fit()`. This is the property we are going to use to check if the base classifiers are being trained with the new features.\n",
    "\n",
    "```\n",
    "first layer\n",
    "294\n",
    "294\n",
    "294\n",
    "294\n",
    "294\n",
    "294\n",
    "second layer\n",
    "300\n",
    "300\n",
    "300\n",
    "300\n",
    "300\n",
    "300\n",
    "```\n",
    "\n",
    "**The base classifiers from the second layer are really receiving all the new features. This is good.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third test: check if the new features have values\n",
    "\n",
    "```\n",
    "FIT: X shape is (1211, 294)\n",
    "FIT: Summing the values: 1019\n",
    "FIT: X_extended shape is (1211, 300)\n",
    "PREDICT: X shape is (1196, 294)\n",
    "Summing the values: 901\n",
    "PREDICT: X_extended shape is (1196, 300)\n",
    "```\n",
    "\n",
    "**The extended features really have values, and they are not all zeros. This is good**. We can also see that they differ from `fit` to `predict`, which is also showing that we are _not_ doing a simple mistake such as using the same data in both places.\n",
    "\n",
    "The share of zeros, however, is something to consider. For the `fit` phase, only 14% of the values have a value different from zero. For the `predict` phase, this share is 12%. Both values are less than the share found in the full dataset, which is around ~17%.\n",
    "\n",
    "Is it possible that the \"stacking part\" is making the model drop its performance? **It might be worthy to check a pure `BinaryRelevance` output**.\n",
    "\n",
    "At first, iy might be tempting to think that the problem is the dataset being used. But remember that in the first notebook, we conduct a very similar test, comparing the output of `BinaryRelevance` and `BasicStacking`, and the results were identical for all datasets tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth test: testing a pure `BinaryRelevance` model\n",
    "\n",
    "~A pure `BinaryRelevance` manages to be _worse_ than the first layer of the `BasicStacking` model. This is really weird. It only gets 901 labels, instead of 1019.~ A pure `BinaryRelevance` is performing exactly equal to the first layer of the `BasicStacking`, which is actually expected. What is weird is that the number of labels found is exactly the same: 901. **What is weird is that both the first and second layer return the exact same number of labels, regardless of the extra features being added to the second layer.**\n",
    "\n",
    "Could it be something with the `random_stage`? **But even after changing the `random_state` to absurd numbers, the results are always the same**. This is totally weird. It is like `SVC` is caching the results or something like that. Or maybe `BinaryRelevance` is keeping a cache of some sort. This is weird, as the source code of `scikit-multilearn` does _not_ describe any cache. It uses `copy.deepcopy`. Maybe this function is the one doing the cache thing? I made a quick Google search about this, but I found nothing conclusive.\n",
    "\n",
    "Or maybe this is down to how `SVC` works. With all its default parameters, it really _is_ stopping at the same point. ~But why it finds more labels when being used via the `BasicStacking` model? This cannot be explained by the `SVC` internal workings alone.~ Actually it is finding the same quantity of labels. I made some confusion because I compared the labels found during `fit` to those found during `predict`. If I compare the labels from `predict` only, then the results are exactly the same.\n",
    "\n",
    "When using different base classifiers, such as the `RandomForestClassifier`, the results are different. **It finds _less_ labels than `SVC`, but at least the results differ when running with different `random_state`s**.\n",
    "\n",
    "Perhaps the way forward is to use `RandomForestClassifier` instead of `SVC` as the base classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fifth test: using `RandomForestClassifier` as the base classifier\n",
    "\n",
    "The usage of `RandomForestClassifier` show that, by using other base classifier, it is possible to achieve different results for the first and second layer. In this case, the `BasicStacking` really differs from the `BinaryRelevance` model, which is what we wanted to achieve in the first place.\n",
    "\n",
    "The results are still **not** that great, as even the `RandomForestClassifier` generate actually little difference from the first to the second layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sixth test: use scikit's `StackingClassifier`\n",
    "\n",
    "The purpose of this is to sort of \"sanity check\" my own implementation of _stacking_. However, it is **not** possible to conduct such a test as the `BinaryRelevance` does not seem to be really ready to be used in the `StackingClassifier` pipeline. Or, as the `StackingClassifier` does not support multilabel and we have to wrap in on a `OneVsRestClassifier`, maybe it does not really work for the `BinaryRelevance` as we are _not_ training one `BinaryRelevance` for each label (each class). Instead, we should be training one `BinaryRelevance` for all labels.\n",
    "\n",
    "With that in mind, I also tried to just use the base classifier, which is the `RandomForestClassifier`, instead of the `BinaryRelevance`. In this case the pipeline works, and in theory, the `OneVsRest` make it behave just like a `BinaryRelevance`.\n",
    "\n",
    "The final scores are actually quite good. Much better than either the `BinaryRelevance` or my `BasicStacking`, or the `StackingWithFTest`. It is **not** immediately clear the actual architecture that is going on in this case. So I am not sure if I am comparing apples to apples here.\n",
    "\n",
    "To actually get info for this, I tried to replicate the simple `BinaryRelevance` model, with `OneVsRestClassifier`, and then replicate the actual stacking, and checking if the results are similar in all places.\n",
    "\n",
    "When investigating the `OneVsRestClassifier`, by using it as a wrapper to the `StackingClassifier`, it shows that is not doing a stacking as per the regular way of having a first and a second layer. Instead, it trains one \"stacked classifier\" for each label. Therefore, we have 6 \"stacked classifier\". And on each \"stacked classifier\", the first base classifier (in this case, a `RandomForestClassifier`) receives 294 features, while the final base classifier takes 295 features, the additional feature being the prediction of the first base classifier.\n",
    "\n",
    "When attempting to replicate the `BinaryRelevance` with `OneVsRest`, we actually get the very same results. For `BinaryRelevance`, we get 0.684223851314326. For `OneVsRest` (with a single layer), we get 0.684223851314326. Which is the same.\n",
    "\n",
    "For the stacking... When using `OneVsRest`, we get 0.6830985019980916. For my own stacking implementation, we get 0.684628539219014, which is different and it is a little bit more. It is also a little bit over the `BinaryRelevance`.\n",
    "\n",
    "This alone does _not_ confirm that I am doing stacking correctly. But I can confirm that the `StackingClassifier` / `OneVsRest` **cannot** replicate the stacking expected, where all the predicted labels from the first layer are used as input for the second layer.\n",
    "\n",
    "However, considering that the metrics are so far are adding up (the results are different on each iteration), and that my `BasicStacking` is so far doing better (even if just a bit) than all the rest, shows that I am on the right track.\n",
    "\n",
    "Considering that all of this is not really the focus of my research, right now I see no reason in trying many other alternatives. The only two things I still want to try is to compare metrics to `StackingWithFTest` with `alpha=1`, and to make sure that the `BasicStacking` is really expanding all labels but the label being predicted.\n",
    "\n",
    "### Seventh test: make sure that the `BasicStacking` expands all labels but the label being predicted\n",
    "\n",
    "...\n",
    "\n",
    "### Eighth test: compare metrics to `StackingWithFTest` when `alpha=1`\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

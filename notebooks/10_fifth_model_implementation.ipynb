{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Fifth model implementation\n",
    "\n",
    "Let's dive into the last model, which is a derived version of the model studied in the notebook 9. It uses partial orders instead of full orders in the chain. It is based on the same paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.dataset import load_dataset\n",
    "import numpy as np\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "import pygad\n",
    "from typing import List\n",
    "import sklearn.metrics as metrics\n",
    "from typing import Any, Optional\n",
    "import copy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from numpy.typing import NDArray\n",
    "from typing import Dict\n",
    "import pandas as pd\n",
    "from typing import cast\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting dataset `scene`\n",
      "scene:undivided - exists, not redownloading\n",
      "scene:train - exists, not redownloading\n",
      "scene:test - exists, not redownloading\n",
      "getting dataset `emotions`\n",
      "emotions:undivided - exists, not redownloading\n",
      "emotions:train - exists, not redownloading\n",
      "emotions:test - exists, not redownloading\n",
      "getting dataset `birds`\n",
      "birds:undivided - exists, not redownloading\n",
      "birds:train - exists, not redownloading\n",
      "birds:test - exists, not redownloading\n",
      "===\n",
      "information for dataset `scene`\n",
      "rows: 2407, labels: 6\n",
      "===\n",
      "information for dataset `emotions`\n",
      "rows: 593, labels: 6\n",
      "===\n",
      "information for dataset `birds`\n",
      "rows: 645, labels: 19\n"
     ]
    }
   ],
   "source": [
    "desired_datasets = [\"scene\", \"emotions\", \"birds\"]\n",
    "\n",
    "datasets = {}\n",
    "for dataset_name in desired_datasets:\n",
    "    print(f\"getting dataset `{dataset_name}`\")\n",
    "    \n",
    "    full_dataset = load_dataset(dataset_name, \"undivided\")\n",
    "    X, y, _, _ = full_dataset\n",
    "\n",
    "    train_dataset = load_dataset(dataset_name, \"train\")\n",
    "    X_train, y_train, _, _ = train_dataset\n",
    "\n",
    "    test_dataset = load_dataset(dataset_name, \"test\")\n",
    "    X_test, y_test, _, _ = test_dataset\n",
    "\n",
    "    datasets[dataset_name] = {\n",
    "        \"X\": X,\n",
    "        \"y\": y,\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_test\": y_test,\n",
    "        \"rows\": X.shape[0],\n",
    "        \"labels_count\": y.shape[1]\n",
    "    }\n",
    "\n",
    "for name, info in datasets.items():\n",
    "    print(\"===\")\n",
    "    print(f\"information for dataset `{name}`\")\n",
    "    print(f\"rows: {info['rows']}, labels: {info['labels_count']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3. Entropy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Probabilities = Dict[int, Dict[int, float]]\n",
    "\n",
    "def calculate_probabilities(y: NDArray[np.int64]) -> Probabilities:\n",
    "    dense_y = y.todense()\n",
    "\n",
    "    label_count = dense_y.shape[1]\n",
    "    rows_count = dense_y.shape[0]\n",
    "\n",
    "    probs = {}\n",
    "\n",
    "    for label in range(label_count):\n",
    "        probs[label] = {}\n",
    "        y_label_specific = np.asarray(dense_y[:, label]).reshape(-1)\n",
    "        # convert_matrix_to_vector\n",
    "\n",
    "        possible_values = np.unique(y_label_specific)\n",
    "\n",
    "        for value in possible_values:\n",
    "            instances_with_label = np.count_nonzero(y_label_specific == value)\n",
    "            probs[label][value] = instances_with_label / rows_count\n",
    "    \n",
    "    return probs\n",
    "\n",
    "Entropies = Dict[int, float]\n",
    "\n",
    "def calculate_entropies(probabilities: Probabilities) -> Entropies:\n",
    "    entropies = {}\n",
    "\n",
    "    for label, calculated_probabilities in probabilities.items():\n",
    "        results = []\n",
    "        for _, prob in calculated_probabilities.items():\n",
    "            summand = prob * math.log(prob, 2)\n",
    "            results.append(summand)\n",
    "        \n",
    "        entropy = -1 * sum(results)\n",
    "        entropies[label] = entropy\n",
    "\n",
    "    return entropies\n",
    "\n",
    "def calculate_joint_probability(probabilities: Probabilities, label_x: int, label_y: int):\n",
    "    results = []\n",
    "    \n",
    "    for _, prob_i in probabilities[label_x].items():\n",
    "        for _, prob_j in probabilities[label_y].items():\n",
    "            and_prob = prob_i * prob_j\n",
    "\n",
    "            if and_prob > 0:  # avoid taking the log of 0\n",
    "                summand = and_prob * np.log2(and_prob)\n",
    "                results.append(summand)\n",
    "    \n",
    "    joint_probability = -1 * sum(results)\n",
    "    return joint_probability\n",
    "\n",
    "def calculate_conditional_entropy(probabilities: Probabilities, entropies: Entropies, label_x: int, label_y: int):\n",
    "    joint_entropy = calculate_joint_probability(probabilities, label_x, label_y)\n",
    "    entropy = entropies[label_y]\n",
    "    return joint_entropy - entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOPMatrix = Dict[int, Dict[int, float]]\n",
    "\n",
    "def build_lop_matrix(\n",
    "    label_order: List[int],\n",
    "    probabilities: Probabilities,\n",
    "    entropies: Entropies\n",
    ") -> LOPMatrix:\n",
    "    matrix = {}\n",
    "\n",
    "    for row_i in label_order:\n",
    "        matrix[row_i] = {}\n",
    "        for row_j in label_order:\n",
    "            if row_i == row_j:\n",
    "                matrix[row_i][row_j] = 0\n",
    "                # this is to match the table described in the paper\n",
    "                # but in reality we _have_ a >0 conditional entropy for a label with itself\n",
    "                continue\n",
    "\n",
    "            cond_entropy = calculate_conditional_entropy(probabilities, entropies, row_i, row_j)\n",
    "            matrix[row_i][row_j] = cond_entropy\n",
    "        \n",
    "    return matrix\n",
    "\n",
    "def calculate_lop(lop_matrix: LOPMatrix) -> float:\n",
    "    matrix_size_n = len(lop_matrix)\n",
    "    lop_df = pd.DataFrame(lop_matrix)\n",
    "\n",
    "    upper_triangle_sum = 0\n",
    "    for row_position in range(matrix_size_n):\n",
    "        for column_position in range(matrix_size_n):\n",
    "            if column_position > row_position:\n",
    "                conditional_probability = lop_df.iloc[row_position, column_position]\n",
    "                upper_triangle_sum += cast(float, conditional_probability)\n",
    "                # the conversion to a dataframe is not necessary\n",
    "                # but makes it easier to find the element we want\n",
    "                # by their order in the rows or columns\n",
    "                # instead of the actual column or row index\n",
    "    \n",
    "    return upper_triangle_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5. New entropy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information(probabilities: Probabilities, entropies: Entropies, label_x: int, label_y: int):\n",
    "    entropy = entropies[label_x]\n",
    "    conditional_entropy = calculate_conditional_entropy(probabilities, entropies, label_x, label_y)\n",
    "\n",
    "    # return entropy - conditional_entropy\n",
    "\n",
    "    a = entropies[label_x]\n",
    "    b = entropies[label_y]\n",
    "\n",
    "    calculate_joint_entropy = calculate_joint_probability(probabilities, label_x, label_y)\n",
    "\n",
    "    return a + b - calculate_joint_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== mutual information ===\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>-2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>2.220446e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-4.440892e-16</td>\n",
       "      <td>-2.220446e-16</td>\n",
       "      <td>-2.220446e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-2.220446e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.220446e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.220446e-16</td>\n",
       "      <td>-2.220446e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0             1             2             3             4  \\\n",
       "0  0.000000e+00  2.220446e-16 -2.220446e-16  2.220446e-16  0.000000e+00   \n",
       "1  2.220446e-16  0.000000e+00  0.000000e+00  0.000000e+00  2.220446e-16   \n",
       "2  0.000000e+00  0.000000e+00 -4.440892e-16 -2.220446e-16 -2.220446e-16   \n",
       "3  2.220446e-16  0.000000e+00 -2.220446e-16  0.000000e+00  0.000000e+00   \n",
       "4  0.000000e+00  2.220446e-16 -2.220446e-16  0.000000e+00  0.000000e+00   \n",
       "5  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "\n",
       "              5  \n",
       "0  0.000000e+00  \n",
       "1  2.220446e-16  \n",
       "2  0.000000e+00  \n",
       "3  2.220446e-16  \n",
       "4  0.000000e+00  \n",
       "5  0.000000e+00  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = calculate_probabilities(datasets[\"emotions\"][\"y\"])\n",
    "entropies = calculate_entropies(probs)\n",
    "\n",
    "print(\"=== mutual information ===\")\n",
    "print(mutual_information(probs, entropies, 2, 0))\n",
    "\n",
    "res = []\n",
    "for i in range(len(probs)):\n",
    "    res.append([])\n",
    "    for j in range(len(probs)):\n",
    "        res[i].append(mutual_information(probs, entropies, i, j))\n",
    "\n",
    "pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.6. Results so far\n",
    "\n",
    "The results are weird because they are very, very small. And some of them are negative. But [this Wikipedia page](https://en.wikipedia.org/wiki/Mutual_information) tells that the mutual information **cannot** be negative.\n",
    "\n",
    "It might be because my calculations of the entropy are wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.7. Checking the results by looking for other implementations\n",
    "\n",
    "I found [this implementation](https://github.com/pafoster/pyitlib). Let's see if it works as expected.\n",
    "\n",
    "I also found [this other implementation](https://github.com/nikdon/pyEntropy), but it seems limited; and [this other from scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html), but it also seems limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyitlib\n",
      "  Downloading pyitlib-0.2.3.tar.gz (30 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pandas>=0.20.2 in c:\\users\\edgard\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyitlib) (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.9.2 in c:\\users\\edgard\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyitlib) (1.22.4)\n",
      "Collecting scikit-learn<=0.24,>=0.16.0 (from pyitlib)\n",
      "  Downloading scikit_learn-0.24.0-cp39-cp39-win_amd64.whl (6.9 MB)\n",
      "     ---------------------------------------- 6.9/6.9 MB 3.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy>=1.0.1 in c:\\users\\edgard\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyitlib) (1.6.2)\n",
      "Collecting future>=0.16.0 (from pyitlib)\n",
      "  Downloading future-0.18.3.tar.gz (840 kB)\n",
      "     ------------------------------------- 840.9/840.9 kB 26.8 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\edgard\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas>=0.20.2->pyitlib) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\edgard\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas>=0.20.2->pyitlib) (2021.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\edgard\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn<=0.24,>=0.16.0->pyitlib) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\edgard\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn<=0.24,>=0.16.0->pyitlib) (2.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\edgard\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=0.20.2->pyitlib) (1.15.0)\n",
      "Building wheels for collected packages: pyitlib, future\n",
      "  Building wheel for pyitlib (setup.py): started\n",
      "  Building wheel for pyitlib (setup.py): finished with status 'done'\n",
      "  Created wheel for pyitlib: filename=pyitlib-0.2.3-py3-none-any.whl size=29367 sha256=b37a01ce175782c490f57abcd4816ad5fa382d2a3fde2ef0ec9a5cb244871996\n",
      "  Stored in directory: c:\\users\\edgard\\appdata\\local\\pip\\cache\\wheels\\c4\\d1\\dc\\ac69412c0dc60ee3fc207f07b6f15abda55c70b7b3e96315aa\n",
      "  Building wheel for future (setup.py): started\n",
      "  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492036 sha256=8ed46af11f690042729b9ea7ba26b7b7b7d91ca5e4dfb370c0b555012c202b3e\n",
      "  Stored in directory: c:\\users\\edgard\\appdata\\local\\pip\\cache\\wheels\\bf\\5d\\6a\\2e53874f7ec4e2bede522385439531fafec8fafe005b5c3d1b\n",
      "Successfully built pyitlib future\n",
      "Installing collected packages: future, scikit-learn, pyitlib\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0\n",
      "    Uninstalling scikit-learn-1.0:\n",
      "      Successfully uninstalled scikit-learn-1.0\n",
      "Successfully installed future-0.18.3 pyitlib-0.2.3 scikit-learn-0.24.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\Lib\\site-packages\\~klearn'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip install pyitlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyitlib import discrete_random_variable as drv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1.)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([0,1,0,1])\n",
    "drv.entropy([0,1,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for label 0 the entropy is 0.8709543977484913\n",
      "for label 1 the entropy is 0.855358883986916\n",
      "for label 2 the entropy is 0.9913156991505125\n",
      "for label 3 the entropy is 0.8106092437567831\n",
      "for label 4 the entropy is 0.8599154189768907\n",
      "for label 5 the entropy is 0.9029822958790428\n"
     ]
    }
   ],
   "source": [
    "y = datasets[\"emotions\"][\"y\"]\n",
    "dense_y = y.todense()\n",
    "\n",
    "for label in range(dense_y.shape[1]):\n",
    "    y_label_specific = np.asarray(dense_y[:, label]).reshape(-1)\n",
    "    e = drv.entropy(y_label_specific.tolist())\n",
    "\n",
    "    print(f\"for label {label} the entropy is {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.8709543977484913,\n",
       " 1: 0.8553588839869162,\n",
       " 2: 0.9913156991505125,\n",
       " 3: 0.8106092437567831,\n",
       " 4: 0.8599154189768907,\n",
       " 5: 0.9029822958790428}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far my calculation for entropy is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for (0, 0), we got 0.0\n",
      "for (0, 1), we got 0.8681775017071309\n",
      "for (0, 2), we got 0.6805721429652296\n",
      "for (0, 3), we got 0.7234043892136215\n",
      "for (0, 4), we got 0.7806134241301019\n",
      "for (0, 5), we got 0.8110675830696776\n",
      "for (1, 0), we got 0.8525819879455556\n",
      "for (1, 1), we got 0.0\n",
      "for (1, 2), we got 0.8433595973068642\n",
      "for (1, 3), we got 0.774297908543917\n",
      "for (1, 4), we got 0.7055354159044847\n",
      "for (1, 5), we got 0.7619971777464194\n",
      "for (2, 0), we got 0.8009334443672507\n",
      "for (2, 1), we got 0.9793164124704608\n",
      "for (2, 2), we got 0.0\n",
      "for (2, 3), we got 0.9262497452773448\n",
      "for (2, 4), we got 0.9746674787371223\n",
      "for (2, 5), we got 0.7172239534657157\n",
      "for (3, 0), we got 0.6630592352219131\n",
      "for (3, 1), we got 0.7295482683137842\n",
      "for (3, 2), we got 0.7455432898836154\n",
      "for (3, 3), we got 0.0\n",
      "for (3, 4), we got 0.6091876963156833\n",
      "for (3, 5), we got 0.6699847819811764\n",
      "for (4, 0), we got 0.7695744453585014\n",
      "for (4, 1), we got 0.7100919508944596\n",
      "for (4, 2), we got 0.8432671985635005\n",
      "for (4, 3), we got 0.6584938715357909\n",
      "for (4, 4), we got 0.0\n",
      "for (4, 5), we got 0.8010033262512426\n",
      "for (5, 0), we got 0.8430954812002294\n",
      "for (5, 1), we got 0.8096205896385462\n",
      "for (5, 2), we got 0.6288905501942459\n",
      "for (5, 3), we got 0.762357834103436\n",
      "for (5, 4), we got 0.8440702031533946\n",
      "for (5, 5), we got 0.0\n"
     ]
    }
   ],
   "source": [
    "for label_x in range(dense_y.shape[1]):\n",
    "    for label_y in range(dense_y.shape[1]):\n",
    "        y_label_specific_x = np.asarray(dense_y[:, label_x]).reshape(-1)\n",
    "        y_label_specific_y = np.asarray(dense_y[:, label_y]).reshape(-1)\n",
    "        \n",
    "        e = drv.entropy_conditional(y_label_specific_x.tolist(), y_label_specific_y.tolist())\n",
    "\n",
    "        print(f\"for ({label_x}, {label_y}), we got {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for (0, 0), we got 0.8709543977484913\n",
      "for (0, 1), we got 0.0027768960413603327\n",
      "for (0, 2), we got 0.19038225478326165\n",
      "for (0, 3), we got 0.14755000853486977\n",
      "for (0, 4), we got 0.09034097361838933\n",
      "for (0, 5), we got 0.05988681467881363\n",
      "for (1, 0), we got 0.0027768960413603327\n",
      "for (1, 1), we got 0.855358883986916\n",
      "for (1, 2), we got 0.01199928668005179\n",
      "for (1, 3), we got 0.08106097544299895\n",
      "for (1, 4), we got 0.1498234680824313\n",
      "for (1, 5), we got 0.09336170624049656\n",
      "for (2, 0), we got 0.19038225478326187\n",
      "for (2, 1), we got 0.01199928668005179\n",
      "for (2, 2), we got 0.9913156991505125\n",
      "for (2, 3), we got 0.06506595387316771\n",
      "for (2, 4), we got 0.016648220413390202\n",
      "for (2, 5), we got 0.2740917456847969\n",
      "for (3, 0), we got 0.14755000853487\n",
      "for (3, 1), we got 0.08106097544299895\n",
      "for (3, 2), we got 0.06506595387316771\n",
      "for (3, 3), we got 0.8106092437567831\n",
      "for (3, 4), we got 0.20142154744109986\n",
      "for (3, 5), we got 0.14062446177560672\n",
      "for (4, 0), we got 0.09034097361838933\n",
      "for (4, 1), we got 0.14982346808243108\n",
      "for (4, 2), we got 0.016648220413390202\n",
      "for (4, 3), we got 0.20142154744109986\n",
      "for (4, 4), we got 0.8599154189768907\n",
      "for (4, 5), we got 0.058912092725648124\n",
      "for (5, 0), we got 0.059886814678813405\n",
      "for (5, 1), we got 0.09336170624049656\n",
      "for (5, 2), we got 0.2740917456847969\n",
      "for (5, 3), we got 0.14062446177560672\n",
      "for (5, 4), we got 0.058912092725648124\n",
      "for (5, 5), we got 0.9029822958790428\n"
     ]
    }
   ],
   "source": [
    "for label_x in range(dense_y.shape[1]):\n",
    "    for label_y in range(dense_y.shape[1]):\n",
    "        y_label_specific_x = np.asarray(dense_y[:, label_x]).reshape(-1)\n",
    "        y_label_specific_y = np.asarray(dense_y[:, label_y]).reshape(-1)\n",
    "        \n",
    "        e = drv.information_mutual(y_label_specific_x.tolist(), y_label_specific_y.tolist())\n",
    "\n",
    "        print(f\"for ({label_x}, {label_y}), we got {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculations for these other \"entropy properties\" are very different to those of my own implementation. **My own implementation is clearly wrong**. I will adopt this new library.\n",
    "\n",
    "**Something else that is nice is that this calculation is already resulting in zero when we test a label against itself**. This is good because it means that the entropy is zero when the label is fully determined by the other label. And it matches exactly what the article describes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.8 Re-implementing the fourth model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConditionalEntropies = List[List[float]]\n",
    "\n",
    "def calculate_conditional_entropies(y: Any) -> ConditionalEntropies:\n",
    "    dense_y = y.todense()\n",
    "\n",
    "    label_count = dense_y.shape[1]\n",
    "\n",
    "    results = []\n",
    "    for label_x in range(label_count):\n",
    "        results.append([])\n",
    "        for label_y in range(label_count):\n",
    "            y_label_specific_x = np.asarray(dense_y[:, label_x]).reshape(-1)\n",
    "            y_label_specific_y = np.asarray(dense_y[:, label_y]).reshape(-1)\n",
    "            \n",
    "            conditional_entropy = drv.entropy_conditional(y_label_specific_x.tolist(), y_label_specific_y.tolist())\n",
    "            results[label_x].append(float(conditional_entropy))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0,\n",
       "  0.8681775017071309,\n",
       "  0.6805721429652296,\n",
       "  0.7234043892136215,\n",
       "  0.7806134241301019,\n",
       "  0.8110675830696776],\n",
       " [0.8525819879455556,\n",
       "  0.0,\n",
       "  0.8433595973068642,\n",
       "  0.774297908543917,\n",
       "  0.7055354159044847,\n",
       "  0.7619971777464194],\n",
       " [0.8009334443672507,\n",
       "  0.9793164124704608,\n",
       "  0.0,\n",
       "  0.9262497452773448,\n",
       "  0.9746674787371223,\n",
       "  0.7172239534657157],\n",
       " [0.6630592352219131,\n",
       "  0.7295482683137842,\n",
       "  0.7455432898836154,\n",
       "  0.0,\n",
       "  0.6091876963156833,\n",
       "  0.6699847819811764],\n",
       " [0.7695744453585014,\n",
       "  0.7100919508944596,\n",
       "  0.8432671985635005,\n",
       "  0.6584938715357909,\n",
       "  0.0,\n",
       "  0.8010033262512426],\n",
       " [0.8430954812002294,\n",
       "  0.8096205896385462,\n",
       "  0.6288905501942459,\n",
       "  0.762357834103436,\n",
       "  0.8440702031533946,\n",
       "  0.0]]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditional_entropies = calculate_conditional_entropies(datasets[\"emotions\"][\"y\"])\n",
    "conditional_entropies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <th>5</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.680572</td>\n",
       "      <td>0.843360</td>\n",
       "      <td>0.745543</td>\n",
       "      <td>0.628891</td>\n",
       "      <td>0.843267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.800933</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.852582</td>\n",
       "      <td>0.663059</td>\n",
       "      <td>0.843095</td>\n",
       "      <td>0.769574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.979316</td>\n",
       "      <td>0.868178</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.729548</td>\n",
       "      <td>0.809621</td>\n",
       "      <td>0.710092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.926250</td>\n",
       "      <td>0.723404</td>\n",
       "      <td>0.774298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.762358</td>\n",
       "      <td>0.658494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.717224</td>\n",
       "      <td>0.811068</td>\n",
       "      <td>0.761997</td>\n",
       "      <td>0.669985</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.801003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.974667</td>\n",
       "      <td>0.780613</td>\n",
       "      <td>0.705535</td>\n",
       "      <td>0.609188</td>\n",
       "      <td>0.844070</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          2         0         1         3         5         4\n",
       "2  0.000000  0.680572  0.843360  0.745543  0.628891  0.843267\n",
       "0  0.800933  0.000000  0.852582  0.663059  0.843095  0.769574\n",
       "1  0.979316  0.868178  0.000000  0.729548  0.809621  0.710092\n",
       "3  0.926250  0.723404  0.774298  0.000000  0.762358  0.658494\n",
       "5  0.717224  0.811068  0.761997  0.669985  0.000000  0.801003\n",
       "4  0.974667  0.780613  0.705535  0.609188  0.844070  0.000000"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_lop_matrix(\n",
    "    label_order: List[int],\n",
    "    conditional_entropies: ConditionalEntropies\n",
    ") -> LOPMatrix:\n",
    "    matrix = {}\n",
    "\n",
    "    for row_i in label_order:\n",
    "        matrix[row_i] = {}\n",
    "        for row_j in label_order:\n",
    "            conditional_entropy = conditional_entropies[row_i][row_j]\n",
    "            matrix[row_i][row_j] = conditional_entropy\n",
    "        \n",
    "    return matrix\n",
    "\n",
    "\n",
    "lop_matrix = build_lop_matrix([2, 0, 1, 3, 5, 4], conditional_entropies)\n",
    "pd.DataFrame(lop_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.341059769376917"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_lop(lop_matrix: LOPMatrix) -> float:\n",
    "    matrix_size_n = len(lop_matrix)\n",
    "    lop_df = pd.DataFrame(lop_matrix)\n",
    "\n",
    "    upper_triangle_sum = 0\n",
    "    for row_position in range(matrix_size_n):\n",
    "        for column_position in range(matrix_size_n):\n",
    "            if column_position > row_position:\n",
    "                conditional_probability = lop_df.iloc[row_position, column_position]\n",
    "                upper_triangle_sum += cast(float, conditional_probability)\n",
    "                # the conversion to a data frame is not necessary\n",
    "                # but makes it easier to find the element we want\n",
    "                # by their order in the rows or columns\n",
    "                # instead of the actual column or row index\n",
    "    \n",
    "    return upper_triangle_sum\n",
    "\n",
    "calculate_lop(lop_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(fixed) hamming loss: 0.0883500557413601\n",
      "(fixed) f1 score: 0.696385030865942\n"
     ]
    }
   ],
   "source": [
    "class FixedClassifierChain:\n",
    "    \"\"\"\n",
    "    Works just like the `ClassifierChain` class from `scikit-multilearn`, but ensures\n",
    "    that the output is in the same order as the labels in the dataset\n",
    "    regardless of the custom order of the classifiers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_classifier: Any, order: List[int]) -> None:\n",
    "        self.base_classifier = base_classifier\n",
    "        self.order = order\n",
    "        # TODO: should have a validation for the order, as it should not contain duplicated values\n",
    "    \n",
    "    def fit(self, X: Any, y: Any):\n",
    "        label_count = y.shape[1]\n",
    "        # TODO: maybe add some validation, sa the label count must match the order length\n",
    "\n",
    "        self.classifiers = {}\n",
    "        # using a dict instead of a list as the index has to be the label following the custom order\n",
    "        # a list would also work, but it is more readable this way\n",
    "\n",
    "        self.metrics = []\n",
    "\n",
    "        X_extended = np.asarray(X.todense())\n",
    "        for label in self.order:\n",
    "            y_label = y[:, label].todense()\n",
    "\n",
    "            y_label_vector = np.asarray(y_label).reshape(-1)\n",
    "            # convert_matrix_to_vector\n",
    "\n",
    "            meta_classifier = copy.deepcopy(self.base_classifier)\n",
    "            meta_classifier.fit(X_extended, y_label_vector)\n",
    "            self.classifiers[label] = meta_classifier\n",
    "\n",
    "            X_extended = np.hstack([X_extended, y_label])\n",
    "            X_extended = np.asarray(X_extended)\n",
    "\n",
    "            # the following section gathers metrics throughout the training\n",
    "            # it has no impact in the actual model performance, but it might\n",
    "            # be interesting to study how to model behaves as it is being trained\n",
    "            metrics_X_train, metrics_X_test, metrics_y_train, metrics_y_test = train_test_split(\n",
    "                X_extended, y_label_vector, test_size=0.33, random_state=42\n",
    "            )\n",
    "\n",
    "            meta_classifier_for_metrics = copy.deepcopy(self.base_classifier)\n",
    "            meta_classifier_for_metrics.fit(metrics_X_train, metrics_y_train)\n",
    "            metrics_predictions = meta_classifier_for_metrics.predict(metrics_X_test)\n",
    "\n",
    "            hl = metrics.hamming_loss(metrics_y_test, metrics_predictions)\n",
    "            f1 = metrics.f1_score(metrics_y_test, metrics_predictions, average=\"macro\")\n",
    "\n",
    "            self.metrics.append({\n",
    "                \"training_for_label\": label,\n",
    "                \"hamming_loss\": hl,\n",
    "                \"f1_score\": f1\n",
    "            })\n",
    "        \n",
    "    def predict(self, X: Any) -> Any:\n",
    "        X_extended = np.asarray(X.todense())\n",
    "\n",
    "        predicted_labels = {}\n",
    "        for label in self.order:\n",
    "            prediction = self.classifiers[label].predict(X_extended)\n",
    "            predicted_labels[label] = prediction\n",
    "            \n",
    "            reshaped_prediction = np.asarray(prediction).reshape(-1, 1)\n",
    "            X_extended = np.hstack([X_extended, reshaped_prediction])\n",
    "            X_extended = np.asarray(X_extended)\n",
    "        \n",
    "        predictions_in_original_label_order = []\n",
    "        original_label_order = np.arange(len(self.order))\n",
    "        for label in original_label_order:\n",
    "            predictions_in_original_label_order.append(predicted_labels[label])\n",
    "        # this is **very important**, otherwise the output of the model will not\n",
    "        # be comparable to the testing dataset \n",
    "\n",
    "        reshaped_array = np.asarray(predictions_in_original_label_order).T\n",
    "        return reshaped_array\n",
    "\n",
    "\n",
    "fixed_model = FixedClassifierChain(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    order=[4, 5, 2, 0, 3, 1]\n",
    "    )\n",
    "\n",
    "X = datasets[\"scene\"][\"X_train\"]\n",
    "y = datasets[\"scene\"][\"y_train\"]\n",
    "X_test = datasets[\"scene\"][\"X_test\"]\n",
    "y_test = datasets[\"scene\"][\"y_test\"]\n",
    "\n",
    "fixed_model.fit(X, y)\n",
    "fixed_preds = fixed_model.predict(X_test)\n",
    "\n",
    "hamming_loss = metrics.hamming_loss(y_test, fixed_preds)\n",
    "f1_score = metrics.f1_score(y_test, fixed_preds, average=\"macro\")\n",
    "\n",
    "print(f\"(fixed) hamming loss: {hamming_loss}\")\n",
    "print(f\"(fixed) f1 score: {f1_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedClassifierChainWithLOPAndGA():\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_classifier: Any,\n",
    "        num_generations: int = 5,\n",
    "        random_state: Optional[int] = None\n",
    "    ) -> None:\n",
    "        self.base_classifier = base_classifier\n",
    "        self.num_generations = num_generations\n",
    "\n",
    "        if random_state is None:\n",
    "            self.random_state = np.random.randint(0, 1000)\n",
    "        else:\n",
    "            self.random_state = random_state\n",
    "\n",
    "        self.conditional_entropies = None\n",
    "    \n",
    "    def fit(self, X: Any, y: Any):\n",
    "        self.conditional_entropies = calculate_conditional_entropies(y)\n",
    "        \n",
    "        label_count = y.shape[1]\n",
    "        label_space = np.arange(label_count)\n",
    "        # solutions_per_population = math.ceil(label_count / 2)\n",
    "        solutions_per_population = 50\n",
    "\n",
    "        ga_model = pygad.GA( #type:ignore\n",
    "            gene_type=int,\n",
    "            gene_space=label_space,\n",
    "            random_seed=self.random_state,\n",
    "            save_best_solutions=False,\n",
    "            fitness_func=self.model_fitness_func,\n",
    "            allow_duplicate_genes=False, # very important, otherwise we will have duplicate labels in the ordering\n",
    "            num_genes=label_count,\n",
    "\n",
    "            # set up\n",
    "            num_generations=self.num_generations,\n",
    "            sol_per_pop=solutions_per_population,\n",
    "\n",
    "            # following what the article describes\n",
    "            keep_elitism=5,\n",
    "            parent_selection_type=\"rws\",\n",
    "            num_parents_mating=2,\n",
    "            crossover_probability=0.9,\n",
    "            crossover_type=\"two_points\",\n",
    "            mutation_type=\"swap\",\n",
    "            mutation_probability=0.01,\n",
    "        )\n",
    "\n",
    "        ga_model.run()\n",
    "\n",
    "        solution, _, _ = ga_model.best_solution()\n",
    "\n",
    "        best_classifier = FixedClassifierChain(\n",
    "            base_classifier=copy.deepcopy(self.base_classifier),\n",
    "            order=solution,\n",
    "        )\n",
    "\n",
    "        best_classifier.fit(X, y)\n",
    "\n",
    "        self.best_classifier = best_classifier\n",
    "    \n",
    "    def model_fitness_func(self, ga_instance: Any, solution: Any, solution_idx: Any) -> float:\n",
    "        return self.test_solution(solution)\n",
    "\n",
    "    def test_solution(self, label_order: List[int]) -> float:\n",
    "        if self.conditional_entropies is None:\n",
    "            raise Exception(\"probabilities and entropies must be calculated before testing a solution\")\n",
    "        \n",
    "        lop_matrix = build_lop_matrix(label_order, self.conditional_entropies)\n",
    "        return calculate_lop(lop_matrix)\n",
    "    \n",
    "    def predict(self, X: Any) -> Any:\n",
    "        if self.best_classifier is None:\n",
    "            raise Exception(\"model was not trained yet\")\n",
    "\n",
    "        return self.best_classifier.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(fixed lop) hamming loss: 0.08821070234113712\n",
      "(fixed lop) f1 score: 0.6991415107601321\n"
     ]
    }
   ],
   "source": [
    "fixed_lop_model = FixedClassifierChainWithLOPAndGA(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    num_generations=20)\n",
    "\n",
    "fixed_lop_model.fit(X, y)\n",
    "fixed_lop_preds = fixed_lop_model.predict(X_test)\n",
    "\n",
    "hamming_loss = metrics.hamming_loss(y_test, fixed_lop_preds)\n",
    "f1_score = metrics.f1_score(y_test, fixed_lop_preds, average=\"macro\")\n",
    "\n",
    "print(f\"(fixed lop) hamming loss: {hamming_loss}\")\n",
    "print(f\"(fixed lop) f1 score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.9. Back to the partial orders\n",
    "\n",
    "Now, let's get back to the model with partial orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.696103</td>\n",
       "      <td>0.044246</td>\n",
       "      <td>0.053738</td>\n",
       "      <td>0.053436</td>\n",
       "      <td>0.020453</td>\n",
       "      <td>0.024041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.044246</td>\n",
       "      <td>0.574336</td>\n",
       "      <td>0.037794</td>\n",
       "      <td>0.037582</td>\n",
       "      <td>0.055496</td>\n",
       "      <td>0.043591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.053738</td>\n",
       "      <td>0.037794</td>\n",
       "      <td>0.640672</td>\n",
       "      <td>0.019622</td>\n",
       "      <td>0.042546</td>\n",
       "      <td>0.052941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.053436</td>\n",
       "      <td>0.037582</td>\n",
       "      <td>0.019622</td>\n",
       "      <td>0.638716</td>\n",
       "      <td>0.007179</td>\n",
       "      <td>0.046674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.020453</td>\n",
       "      <td>0.055496</td>\n",
       "      <td>0.042546</td>\n",
       "      <td>0.007179</td>\n",
       "      <td>0.775802</td>\n",
       "      <td>0.071355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.024041</td>\n",
       "      <td>0.043591</td>\n",
       "      <td>0.052941</td>\n",
       "      <td>0.046674</td>\n",
       "      <td>0.071355</td>\n",
       "      <td>0.690832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5\n",
       "0  0.696103  0.044246  0.053738  0.053436  0.020453  0.024041\n",
       "1  0.044246  0.574336  0.037794  0.037582  0.055496  0.043591\n",
       "2  0.053738  0.037794  0.640672  0.019622  0.042546  0.052941\n",
       "3  0.053436  0.037582  0.019622  0.638716  0.007179  0.046674\n",
       "4  0.020453  0.055496  0.042546  0.007179  0.775802  0.071355\n",
       "5  0.024041  0.043591  0.052941  0.046674  0.071355  0.690832"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MutualInformation = List[List[float]]\n",
    "\n",
    "def calculate_mutual_information(y: Any) -> MutualInformation:\n",
    "    dense_y = y.todense()\n",
    "\n",
    "    results = []\n",
    "    for label_x in range(dense_y.shape[1]):\n",
    "        results.append([])\n",
    "        for label_y in range(dense_y.shape[1]):\n",
    "            y_label_specific_x = np.asarray(dense_y[:, label_x]).reshape(-1)\n",
    "            y_label_specific_y = np.asarray(dense_y[:, label_y]).reshape(-1)\n",
    "            \n",
    "            e = drv.information_mutual(y_label_specific_x.tolist(), y_label_specific_y.tolist())\n",
    "            results[label_x].append(e)\n",
    "\n",
    "    return results\n",
    "\n",
    "r = calculate_mutual_information(y)\n",
    "pd.DataFrame(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: [], 0: [2], 1: [0], 3: [0], 5: [2, 1, 3], 4: [2, 1, 5]}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_partial_orders(order: List[int], mutual_information: MutualInformation, threshold: float) -> Any:\n",
    "    labels_to_expand = {}\n",
    "    for pos_i in range(len(order)):\n",
    "        label_i = order[pos_i]\n",
    "        labels_to_expand[label_i] = []\n",
    "\n",
    "        for label_j in order[:pos_i]:\n",
    "            mi = mutual_information[label_i][label_j]\n",
    "            if mi > threshold:\n",
    "                labels_to_expand[label_i].append(label_j)\n",
    "    \n",
    "    return labels_to_expand\n",
    "\n",
    "build_partial_orders([2, 0, 1, 3, 5, 4], r, 0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will start training for label 2\n",
      "input size is (1211, 294)\n",
      "finished training for label 2\n",
      "will start training for label 0\n",
      "input size is (1211, 295)\n",
      "finished training for label 0\n",
      "will start training for label 1\n",
      "input size is (1211, 295)\n",
      "finished training for label 1\n",
      "will start training for label 3\n",
      "input size is (1211, 295)\n",
      "finished training for label 3\n",
      "will start training for label 5\n",
      "input size is (1211, 297)\n",
      "finished training for label 5\n",
      "will start training for label 4\n",
      "input size is (1211, 297)\n",
      "finished training for label 4\n",
      "(partial) hamming loss: 0.09002229654403568\n",
      "(partial) f1 score: 0.6898447036253601\n"
     ]
    }
   ],
   "source": [
    "class FixedClassifierChainWithPartial:\n",
    "    \"\"\"\n",
    "    Allows for partial orders to be used in the classifier chain.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_classifier: Any, order: List[int], partial_orders: Dict[int, List[int]]) -> None:\n",
    "        self.base_classifier = base_classifier\n",
    "        self.order = order\n",
    "        self.partial_orders = partial_orders\n",
    "        # TODO: should have a validation for the order, as it should not contain duplicated values\n",
    "    \n",
    "    def fit(self, X: Any, y: Any):\n",
    "        label_count = y.shape[1]\n",
    "        # TODO: maybe add some validation, sa the label count must match the order length\n",
    "\n",
    "        self.classifiers = {}\n",
    "        # using a dict instead of a list as the index has to be the label following the custom order\n",
    "        # a list would also work, but it is more readable this way\n",
    "\n",
    "        self.metrics = []\n",
    "\n",
    "        label_values_obtained = {}\n",
    "\n",
    "        for label in self.order:\n",
    "            y_label = y[:, label].todense()\n",
    "            y_label_vector = np.asarray(y_label).reshape(-1)\n",
    "            # convert_matrix_to_vector\n",
    "\n",
    "            X_local_extended = X.todense()\n",
    "            \n",
    "            partial_order = self.partial_orders[label]\n",
    "            if len(partial_order) > 0:\n",
    "                labels_to_expand = pd.DataFrame(label_values_obtained).loc[:, partial_order].values\n",
    "                X_local_extended = np.hstack([X.todense(), labels_to_expand])\n",
    "\n",
    "            X_local_extended = np.asarray(X_local_extended)\n",
    "\n",
    "            print(f\"will start training for label {label}\")\n",
    "            print(f\"input size is {X_local_extended.shape}\")\n",
    "\n",
    "            meta_classifier = copy.deepcopy(self.base_classifier)\n",
    "            meta_classifier.fit(X_local_extended, y_label_vector)\n",
    "            self.classifiers[label] = meta_classifier\n",
    "            label_values_obtained[label] = y_label_vector\n",
    "\n",
    "            print(f\"finished training for label {label}\")\n",
    "\n",
    "            # the following section gathers metrics throughout the training\n",
    "            # it has no impact in the actual model performance, but it might\n",
    "            # be interesting to study how to model behaves as it is being trained\n",
    "            metrics_X_train, metrics_X_test, metrics_y_train, metrics_y_test = train_test_split(\n",
    "                X_local_extended, y_label_vector, test_size=0.33, random_state=42\n",
    "            )\n",
    "\n",
    "            meta_classifier_for_metrics = copy.deepcopy(self.base_classifier)\n",
    "            meta_classifier_for_metrics.fit(metrics_X_train, metrics_y_train)\n",
    "            metrics_predictions = meta_classifier_for_metrics.predict(metrics_X_test)\n",
    "\n",
    "            hl = metrics.hamming_loss(metrics_y_test, metrics_predictions)\n",
    "            f1 = metrics.f1_score(metrics_y_test, metrics_predictions, average=\"macro\")\n",
    "\n",
    "            self.metrics.append({\n",
    "                \"training_for_label\": label,\n",
    "                \"hamming_loss\": hl,\n",
    "                \"f1_score\": f1\n",
    "            })\n",
    "        \n",
    "    def predict(self, X: Any) -> Any:\n",
    "        predicted_labels = {}\n",
    "        for label in self.order:\n",
    "            partial_order = self.partial_orders[label]\n",
    "            X_local_extended = np.asarray(X.todense())\n",
    "            if len(partial_order) > 0:\n",
    "                labels_to_expand = pd.DataFrame(predicted_labels).loc[:, partial_order].values\n",
    "                X_local_extended = np.hstack([X.todense(), labels_to_expand])\n",
    "            X_local_extended = np.asarray(X_local_extended)\n",
    "\n",
    "            prediction = self.classifiers[label].predict(X_local_extended)\n",
    "            predicted_labels[label] = prediction\n",
    "        \n",
    "        predictions_in_original_label_order = []\n",
    "        original_label_order = np.arange(len(self.order))\n",
    "        for label in original_label_order:\n",
    "            predictions_in_original_label_order.append(predicted_labels[label])\n",
    "        # this is **very important**, otherwise the output of the model will not\n",
    "        # be comparable to the testing dataset \n",
    "\n",
    "        reshaped_array = np.asarray(predictions_in_original_label_order).T\n",
    "        return reshaped_array\n",
    "\n",
    "\n",
    "partial_model = FixedClassifierChainWithPartial(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    order=[2, 0, 1, 3, 5, 4],\n",
    "    partial_orders={2: [], 0: [2], 1: [0], 3: [0], 5: [2, 1, 3], 4: [2, 1, 5]}\n",
    "    )\n",
    "\n",
    "partial_model.fit(X, y)\n",
    "partial_preds = partial_model.predict(X_test)\n",
    "\n",
    "hamming_loss = metrics.hamming_loss(y_test, partial_preds)\n",
    "f1_score = metrics.f1_score(y_test, partial_preds, average=\"macro\")\n",
    "\n",
    "print(f\"(partial) hamming loss: {hamming_loss}\")\n",
    "print(f\"(partial) f1 score: {f1_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'training_for_label': 2,\n",
       "  'hamming_loss': 0.0575,\n",
       "  'f1_score': 0.890962962962963},\n",
       " {'training_for_label': 0,\n",
       "  'hamming_loss': 0.0975,\n",
       "  'f1_score': 0.8019676293240241},\n",
       " {'training_for_label': 1,\n",
       "  'hamming_loss': 0.03,\n",
       "  'f1_score': 0.9395039322444041},\n",
       " {'training_for_label': 3,\n",
       "  'hamming_loss': 0.065,\n",
       "  'f1_score': 0.8742685816528846},\n",
       " {'training_for_label': 5,\n",
       "  'hamming_loss': 0.1075,\n",
       "  'f1_score': 0.7334118631720888},\n",
       " {'training_for_label': 4,\n",
       "  'hamming_loss': 0.1225,\n",
       "  'f1_score': 0.7437204984374796}]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_model.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCPartialOrder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_classifier: Any,\n",
    "        threshold: float = 0.5,\n",
    "        num_generations: int = 5,\n",
    "        random_state: Optional[int] = None\n",
    "    ) -> None:\n",
    "        self.base_classifier = base_classifier\n",
    "        self.threshold = threshold\n",
    "        self.num_generations = num_generations\n",
    "\n",
    "        if random_state is None:\n",
    "            self.random_state = np.random.randint(0, 1000)\n",
    "        else:\n",
    "            self.random_state = random_state\n",
    "\n",
    "        self.conditional_entropies = None\n",
    "    \n",
    "    def fit(self, X: Any, y: Any):\n",
    "        self.conditional_entropies = calculate_conditional_entropies(y)\n",
    "        self.mutual_information = calculate_mutual_information(y)\n",
    "        \n",
    "        label_count = y.shape[1]\n",
    "        label_space = np.arange(label_count)\n",
    "        solutions_per_population = 50\n",
    "\n",
    "        ga_model = pygad.GA( #type:ignore\n",
    "            gene_type=int,\n",
    "            gene_space=label_space,\n",
    "            random_seed=self.random_state,\n",
    "            save_best_solutions=False,\n",
    "            fitness_func=self.model_fitness_func,\n",
    "            allow_duplicate_genes=False, # very important, otherwise we will have duplicate labels in the ordering\n",
    "            num_genes=label_count,\n",
    "\n",
    "            # set up\n",
    "            num_generations=self.num_generations,\n",
    "            sol_per_pop=solutions_per_population,\n",
    "\n",
    "            # following what the article describes\n",
    "            keep_elitism=5,\n",
    "            parent_selection_type=\"rws\",\n",
    "            num_parents_mating=2,\n",
    "            crossover_probability=0.9,\n",
    "            crossover_type=\"two_points\",\n",
    "            mutation_type=\"swap\",\n",
    "            mutation_probability=0.01,\n",
    "        )\n",
    "\n",
    "        ga_model.run()\n",
    "        \n",
    "        solution, _, _ = ga_model.best_solution()\n",
    "\n",
    "        partial_order = build_partial_orders(solution, self.mutual_information, self.threshold)\n",
    "\n",
    "        best_classifier = FixedClassifierChainWithPartial(\n",
    "            base_classifier=copy.deepcopy(self.base_classifier),\n",
    "            order=solution,\n",
    "            partial_orders=partial_order\n",
    "        )\n",
    "\n",
    "        best_classifier.fit(X, y)\n",
    "\n",
    "        self.best_classifier = best_classifier\n",
    "    \n",
    "    def model_fitness_func(self, ga_instance: Any, solution: Any, solution_idx: Any) -> float:\n",
    "        return self.test_solution(solution)\n",
    "\n",
    "    def test_solution(self, label_order: List[int]) -> float:\n",
    "        if self.conditional_entropies is None:\n",
    "            raise Exception(\"probabilities and entropies must be calculated before testing a solution\")\n",
    "        \n",
    "        lop_matrix = build_lop_matrix(label_order, self.conditional_entropies)\n",
    "        return calculate_lop(lop_matrix)\n",
    "    \n",
    "    def predict(self, X: Any) -> Any:\n",
    "        if self.best_classifier is None:\n",
    "            raise Exception(\"model was not trained yet\")\n",
    "\n",
    "        return self.best_classifier.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will start training for label 1\n",
      "input size is (1211, 294)\n",
      "finished training for label 1\n",
      "will start training for label 2\n",
      "input size is (1211, 295)\n",
      "finished training for label 2\n",
      "will start training for label 3\n",
      "input size is (1211, 296)\n",
      "finished training for label 3\n",
      "will start training for label 5\n",
      "input size is (1211, 297)\n",
      "finished training for label 5\n",
      "will start training for label 0\n",
      "input size is (1211, 298)\n",
      "finished training for label 0\n",
      "will start training for label 4\n",
      "input size is (1211, 298)\n",
      "finished training for label 4\n",
      "(partial_lop) hamming loss: 0.0898829431438127\n",
      "(partial_lop) f1 score: 0.6878812182022297\n"
     ]
    }
   ],
   "source": [
    "partial_lop = CCPartialOrder(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    threshold=0.01,\n",
    "    num_generations=25,\n",
    "    random_state=42,\n",
    "    )\n",
    "\n",
    "partial_lop.fit(X, y)\n",
    "partial_lop_preds = partial_lop.predict(X_test)\n",
    "\n",
    "hamming_loss = metrics.hamming_loss(y_test, partial_lop_preds)\n",
    "f1_score = metrics.f1_score(y_test, partial_lop_preds, average=\"macro\")\n",
    "\n",
    "print(f\"(partial_lop) hamming loss: {hamming_loss}\")\n",
    "print(f\"(partial_lop) f1 score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.10. Conclusion\n",
    "\n",
    "Aaaaaaand we have a fifth model done!\n",
    "\n",
    "Now it is all about adding to a Python file of its own and then then adding unit tests.\n",
    "\n",
    "**Something to mention here is that the paper is _not_ so clear about how it achieves the partial orderings**. The pseudo-code provided is very simplistic considering what it wants to achieve. I had to read the paper several times to understand what it was doing, and even so I cannot confidently say that I implemented it 100% according to the paper.\n",
    "\n",
    "Still, the results are ok, the logic makes sense and the code is not too complex. I am happy with this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

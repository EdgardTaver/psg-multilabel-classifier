{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting metrics for the first model\n",
    "\n",
    "Let's try to update my report with the metrics for the first model.\n",
    "\n",
    "As per the tests conducted on the first notebook, regarding the datasets (to find which datasets can be run in a reasonable time), we are going to use these default datasets:\n",
    "\n",
    "* `birds`\n",
    "* `emotions`\n",
    "* `scene`\n",
    "\n",
    "The **baseline** will be the regular Binary Relevance. Then we will compare those to the Basic Stacking approach. Finally, we will run the Stacking With F-Test, with `alpha=0.5`. All other parameters, of the other models, will be the default ones, using the `SVC` as the base classifier.\n",
    "\n",
    "The metrics we will use, as per what was defined in my report, are the hamming loss and the f1 score. We will use the `EvaluationPipeline` class to run the experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.dataset import load_dataset, available_data_sets\n",
    "from sklearn.svm import SVC\n",
    "from skmultilearn.base.problem_transformation import ProblemTransformationBase\n",
    "from typing import List, Optional, Any, Tuple, Dict\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "from lib.evaluation import EvaluationPipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move this to an actual python file\n",
    "\n",
    "class BasicStacking(ProblemTransformationBase):\n",
    "    first_layer_classifiers: BinaryRelevance\n",
    "    second_layer_classifiers: BinaryRelevance\n",
    "\n",
    "    def __init__(self, classifier: Any = None, require_dense: Optional[List[bool]] = None):\n",
    "        super(BasicStacking, self).__init__(classifier, require_dense)\n",
    "\n",
    "        self.first_layer_classifiers = BinaryRelevance(\n",
    "            classifier=SVC(),\n",
    "            require_dense=[False, True]\n",
    "        )\n",
    "\n",
    "        self.second_layer_classifiers = BinaryRelevance(\n",
    "            classifier=SVC(),\n",
    "            require_dense=[False, True]\n",
    "        )\n",
    "    \n",
    "    def fit(self, X: Any, y: Any):\n",
    "        self.first_layer_classifiers.fit(X, y)\n",
    "\n",
    "        first_layer_predictions = self.first_layer_classifiers.predict(X)\n",
    "        X_expanded = np.hstack([X.todense(), first_layer_predictions.todense()])\n",
    "\n",
    "        self.second_layer_classifiers.fit(X_expanded, y)\n",
    "    \n",
    "    def predict(self, X: Any):\n",
    "        first_layer_predictions = self.first_layer_classifiers.predict(X)\n",
    "        X_expanded = np.hstack([X.todense(), first_layer_predictions.todense()])\n",
    "        return self.second_layer_classifiers.predict(X_expanded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO move this to an actual python file\n",
    "\n",
    "class StackingWithFTests(ProblemTransformationBase):\n",
    "    alpha: float\n",
    "    use_first_layer_to_calculate_correlations: bool\n",
    "    \n",
    "    first_layer_classifiers: BinaryRelevance\n",
    "    second_layer_classifiers: List[Any] # TODO should be any generic type of classifier\n",
    "    labels_count: int\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha: float = 0.5,\n",
    "        use_first_layer_to_calculate_correlations: bool = False,\n",
    "        classifier: Any = None,\n",
    "        require_dense: Optional[List[bool]] = None\n",
    "    ):\n",
    "        super(StackingWithFTests, self).__init__(classifier, require_dense)\n",
    "\n",
    "        if alpha < 0.0 or alpha > 1.0:\n",
    "            raise Exception(\"alpha must be >= 0.0 and <= 1.0\")\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.use_first_layer_to_calculate_correlations = use_first_layer_to_calculate_correlations\n",
    "        \n",
    "        self.first_layer_classifiers = BinaryRelevance(\n",
    "            classifier=SVC(),\n",
    "            require_dense=[False, True]\n",
    "        )\n",
    "        # TODO: allow for any base model (base classifier) to be used\n",
    "        # right now I am forcing the use of SVC\n",
    "\n",
    "        self.second_layer_classifiers = []\n",
    "        self.correlated_labels_map = pd.DataFrame()\n",
    "        self.labels_count = 0\n",
    "\n",
    "\n",
    "    def fit(self, X: Any, y: Any):\n",
    "        self.labels_count = y.shape[1]\n",
    "\n",
    "        self.first_layer_classifiers.fit(X, y)\n",
    "        \n",
    "        label_classifications = y\n",
    "        if self.use_first_layer_to_calculate_correlations:\n",
    "            label_classifications = self.first_layer_classifiers.predict(X)\n",
    "\n",
    "        f_tested_label_pairs = self.calculate_f_test_for_all_label_pairs(label_classifications)\n",
    "        self.correlated_labels_map = self.get_map_of_correlated_labels(f_tested_label_pairs)\n",
    "\n",
    "        for i in range(self.labels_count):\n",
    "            mask = self.correlated_labels_map[\"for_label\"] == i\n",
    "            split_df = self.correlated_labels_map[mask].reset_index(drop=True)\n",
    "            labels_to_expand = split_df[\"expand_this_label\"].to_list()\n",
    "\n",
    "            additional_input = label_classifications.todense()[:, labels_to_expand]\n",
    "            \n",
    "            X_expanded = np.hstack([X.todense(), additional_input])\n",
    "            X_expanded = np.asarray(X_expanded)\n",
    "\n",
    "            y_label_specific = y.todense()[:, i]\n",
    "            y_label_specific = self.convert_matrix_to_vector(y_label_specific)\n",
    "\n",
    "            meta_classifier = SVC()\n",
    "            meta_classifier.fit(X_expanded, y_label_specific)\n",
    "\n",
    "            self.second_layer_classifiers.append(meta_classifier)\n",
    "            print(f\"finished training meta classifier for label {i}\")\n",
    "    \n",
    "    def calculate_f_test_for_all_label_pairs(self, label_classifications: Any) -> List[Dict[str, Any]]:\n",
    "        results = []\n",
    "\n",
    "        for i in range(0, self.labels_count):\n",
    "            for j in range(0, self.labels_count):\n",
    "                if i == j:\n",
    "                    continue\n",
    "\n",
    "                X = label_classifications.todense()[:, i]\n",
    "                base_label = self.convert_matrix_to_array(X)\n",
    "\n",
    "                y = label_classifications.todense()[:, j]\n",
    "                against_label = self.convert_matrix_to_vector(y)\n",
    "\n",
    "                f_test_result = f_classif(base_label, against_label)[0]\n",
    "\n",
    "                results.append({\n",
    "                    \"label_being_tested\": i,\n",
    "                    \"against_label\": j,\n",
    "                    \"f_test_result\": float(f_test_result)\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def convert_matrix_to_array(self, matrix: Any):\n",
    "        return np.asarray(matrix).reshape(-1, 1)\n",
    "\n",
    "    def convert_matrix_to_vector(self, matrix: Any):\n",
    "        return np.asarray(matrix).reshape(-1)\n",
    "    \n",
    "    def get_map_of_correlated_labels(self, f_test_results: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "        temp_df = pd.DataFrame(f_test_results)\n",
    "        \n",
    "        sorted_temp_df = temp_df.sort_values(\n",
    "            by=[\"label_being_tested\", \"f_test_result\"],\n",
    "            ascending=[True, False])\n",
    "        # ordering in descending order by the F-test result,\n",
    "        # following what the main article describes\n",
    "\n",
    "        selected_features = []\n",
    "\n",
    "        for i in range(0, self.labels_count):\n",
    "            mask = sorted_temp_df[\"label_being_tested\"] == i\n",
    "            split_df = sorted_temp_df[mask].reset_index(drop=True)\n",
    "\n",
    "            big_f = split_df[\"f_test_result\"].sum()\n",
    "            max_cum_f = self.alpha * big_f\n",
    "\n",
    "            cum_f = 0\n",
    "            for _, row in split_df.iterrows():\n",
    "                cum_f += row[\"f_test_result\"]\n",
    "                if cum_f > max_cum_f:\n",
    "                    break\n",
    "\n",
    "                selected_features.append({\n",
    "                    \"for_label\": i,\n",
    "                    \"expand_this_label\": int(row[\"against_label\"]),\n",
    "                    \"f_test_result\": float(row[\"f_test_result\"]),\n",
    "                })\n",
    "        \n",
    "        cols = [\"for_label\", \"expand_this_label\", \"f_test_result\"]\n",
    "        return pd.DataFrame(selected_features, columns=cols)\n",
    "    \n",
    "    def predict(self, X: Any) -> np.ndarray[Any,Any]:\n",
    "        if self.correlated_labels_map.columns.size == 0:\n",
    "            raise Exception(\"model was not trained yet\")\n",
    "\n",
    "        predictions = self.first_layer_classifiers.predict(X)\n",
    "        local_labels_count = predictions.shape[1]\n",
    "\n",
    "        second_layer_predictions = []\n",
    "\n",
    "        for i in range(local_labels_count):\n",
    "            mask = self.correlated_labels_map[\"for_label\"] == i\n",
    "            split_df = self.correlated_labels_map[mask].reset_index(drop=True)\n",
    "            labels_to_expand = split_df[\"expand_this_label\"].to_list()\n",
    "\n",
    "            additional_input = predictions.todense()[:, labels_to_expand]\n",
    "\n",
    "            X_expanded = np.hstack([X.todense(), additional_input])\n",
    "            X_expanded = np.asarray(X_expanded)\n",
    "\n",
    "            temp_preds = self.second_layer_classifiers[i].predict(X_expanded)\n",
    "            second_layer_predictions.append(temp_preds)\n",
    "\n",
    "        reshaped_array = np.asarray(second_layer_predictions).T\n",
    "        return reshaped_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting dataset `scene`\n",
      "scene:undivided - exists, not redownloading\n",
      "scene:train - exists, not redownloading\n",
      "scene:test - exists, not redownloading\n",
      "getting dataset `emotions`\n",
      "emotions:undivided - exists, not redownloading\n",
      "emotions:train - exists, not redownloading\n",
      "emotions:test - exists, not redownloading\n",
      "getting dataset `birds`\n",
      "birds:undivided - exists, not redownloading\n",
      "birds:train - exists, not redownloading\n",
      "birds:test - exists, not redownloading\n",
      "===\n",
      "information for dataset `scene`\n",
      "rows: 2407, labels: 6\n",
      "===\n",
      "information for dataset `emotions`\n",
      "rows: 593, labels: 6\n",
      "===\n",
      "information for dataset `birds`\n",
      "rows: 645, labels: 19\n"
     ]
    }
   ],
   "source": [
    "desired_datasets = [\"scene\", \"emotions\", \"birds\"]\n",
    "\n",
    "datasets = {}\n",
    "for dataset_name in desired_datasets:\n",
    "    print(f\"getting dataset `{dataset_name}`\")\n",
    "    \n",
    "    full_dataset = load_dataset(dataset_name, \"undivided\")\n",
    "    X, y, _, _ = full_dataset\n",
    "\n",
    "    train_dataset = load_dataset(dataset_name, \"train\")\n",
    "    X_train, y_train, _, _ = train_dataset\n",
    "\n",
    "    test_dataset = load_dataset(dataset_name, \"test\")\n",
    "    X_test, y_test, _, _ = test_dataset\n",
    "\n",
    "    datasets[dataset_name] = {\n",
    "        \"X\": X,\n",
    "        \"y\": y,\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_test\": y_test,\n",
    "        \"rows\": X.shape[0],\n",
    "        \"labels_count\": y.shape[1]\n",
    "    }\n",
    "\n",
    "\n",
    "for name, info in datasets.items():\n",
    "    print(\"===\")\n",
    "    print(f\"information for dataset `{name}`\")\n",
    "    print(f\"rows: {info['rows']}, labels: {info['labels_count']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "qf_x_train, qf_x_test, qf_y_train, qf_y_test = train_test_split(datasets[\"emotions\"][\"X\"].toarray(), datasets[\"emotions\"][\"y\"].toarray(), test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_binary_relevance_model = BinaryRelevance(\n",
    "    classifier=SVC(),\n",
    "    require_dense=[False, True]\n",
    ")\n",
    "\n",
    "basic_stacking_model = BasicStacking()\n",
    "stacking_with_f_tests_model = StackingWithFTests(alpha=0.5)\n",
    "\n",
    "models = {\n",
    "    \"baseline_binary_relevance_model\": baseline_binary_relevance_model,\n",
    "    \"basic_stacking_model\": basic_stacking_model,\n",
    "    \"stacking_with_f_tests_model\": stacking_with_f_tests_model\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# running model `baseline_binary_relevance_model`\n",
      "## running dataset `scene`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results obtained:\n",
      "Accuracy: 0.4910 ± 0.21\n",
      "Hamming Loss: -0.1107 ± 0.05\n",
      "F1 score: 0.3016 ± 0.09\n",
      "## running dataset `emotions`\n",
      "results obtained:\n",
      "Accuracy: 0.0186 ± 0.02\n",
      "Hamming Loss: -0.3016 ± 0.03\n",
      "F1 score: 0.0640 ± 0.02\n",
      "## running dataset `birds`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results obtained:\n",
      "Accuracy: 0.4637 ± 0.07\n",
      "Hamming Loss: -0.0535 ± 0.01\n",
      "F1 score: 0.0126 ± 0.01\n",
      "# running model `basic_stacking_model`\n",
      "## running dataset `scene`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results obtained:\n",
      "Accuracy: 0.4910 ± 0.21\n",
      "Hamming Loss: -0.1107 ± 0.05\n",
      "F1 score: 0.3016 ± 0.09\n",
      "## running dataset `emotions`\n",
      "results obtained:\n",
      "Accuracy: 0.0186 ± 0.02\n",
      "Hamming Loss: -0.3016 ± 0.03\n",
      "F1 score: 0.0640 ± 0.02\n",
      "## running dataset `birds`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results obtained:\n",
      "Accuracy: 0.4637 ± 0.07\n",
      "Hamming Loss: -0.0535 ± 0.01\n",
      "F1 score: 0.0126 ± 0.01\n",
      "# running model `stacking_with_f_tests_model`\n",
      "## running dataset `scene`\n",
      "finished training meta classifier for label 0\n",
      "finished training meta classifier for label 1\n",
      "finished training meta classifier for label 2\n",
      "finished training meta classifier for label 3\n",
      "finished training meta classifier for label 4\n",
      "finished training meta classifier for label 5\n",
      "finished training meta classifier for label 0\n",
      "finished training meta classifier for label 1\n",
      "finished training meta classifier for label 2\n",
      "finished training meta classifier for label 3\n",
      "finished training meta classifier for label 4\n",
      "finished training meta classifier for label 5\n",
      "finished training meta classifier for label 0\n",
      "finished training meta classifier for label 1\n",
      "finished training meta classifier for label 2\n",
      "finished training meta classifier for label 3\n",
      "finished training meta classifier for label 4\n",
      "finished training meta classifier for label 5\n",
      "finished training meta classifier for label 0\n",
      "finished training meta classifier for label 1\n",
      "finished training meta classifier for label 2\n",
      "finished training meta classifier for label 3\n",
      "finished training meta classifier for label 4\n",
      "finished training meta classifier for label 5\n",
      "finished training meta classifier for label 0\n",
      "finished training meta classifier for label 1\n",
      "finished training meta classifier for label 2\n",
      "finished training meta classifier for label 3\n",
      "finished training meta classifier for label 4\n",
      "finished training meta classifier for label 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training meta classifier for label 0\n",
      "finished training meta classifier for label 1\n",
      "finished training meta classifier for label 2\n",
      "finished training meta classifier for label 3\n",
      "finished training meta classifier for label 4\n",
      "finished training meta classifier for label 5\n",
      "finished training meta classifier for label 0\n",
      "finished training meta classifier for label 1\n",
      "finished training meta classifier for label 2\n",
      "finished training meta classifier for label 3\n",
      "finished training meta classifier for label 4\n",
      "finished training meta classifier for label 5\n",
      "finished training meta classifier for label 0\n",
      "finished training meta classifier for label 1\n",
      "finished training meta classifier for label 2\n",
      "finished training meta classifier for label 3\n",
      "finished training meta classifier for label 4\n",
      "finished training meta classifier for label 5\n",
      "finished training meta classifier for label 0\n",
      "finished training meta classifier for label 1\n",
      "finished training meta classifier for label 2\n",
      "finished training meta classifier for label 3\n",
      "finished training meta classifier for label 4\n",
      "finished training meta classifier for label 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training meta classifier for label 0\n",
      "finished training meta classifier for label 1\n",
      "finished training meta classifier for label 2\n",
      "finished training meta classifier for label 3\n",
      "finished training meta classifier for label 4\n",
      "finished training meta classifier for label 5\n",
      "results obtained:\n",
      "Accuracy: 0.5982 ± 0.13\n",
      "Hamming Loss: -0.0985 ± 0.04\n",
      "F1 score: 0.3224 ± 0.07\n",
      "## running dataset `emotions`\n",
      "finished training meta classifier for label 0\n",
      "finished training meta classifier for label 1\n",
      "finished training meta classifier for label 2\n",
      "finished training meta classifier for label 3\n",
      "finished training meta classifier for label 4\n",
      "finished training meta classifier for label 5\n",
      "finished training meta classifier for label 0\n",
      "finished training meta classifier for label 1\n",
      "finished training meta classifier for label 2\n",
      "finished training meta classifier for label 3\n",
      "finished training meta classifier for label 4\n",
      "finished training meta classifier for label 5\n",
      "finished training meta classifier for label 0\n",
      "finished training meta classifier for label 1\n",
      "finished training meta classifier for label 2\n",
      "finished training meta classifier for label 3\n",
      "finished training meta classifier for label 4\n",
      "finished training meta classifier for label 5\n",
      "finished training meta classifier for label 0\n",
      "finished training meta classifier for label 1\n",
      "finished training meta classifier for label 2\n",
      "finished training meta classifier for label 3\n",
      "finished training meta classifier for label 4\n",
      "finished training meta classifier for label 5\n",
      "finished training meta classifier for label 0\n",
      "finished training meta classifier for label 1\n",
      "finished training meta classifier for label 2\n",
      "finished training meta classifier for label 3\n",
      "finished training meta classifier for label 4\n",
      "finished training meta classifier for label 5\n",
      "finished training meta classifier for label 0\n",
      "finished training meta classifier for label 1\n",
      "finished training meta classifier for label 2\n",
      "finished training meta classifier for label 3\n",
      "finished training meta classifier for label 4\n",
      "finished training meta classifier for label 5\n",
      "finished training meta classifier for label 0\n",
      "finished training meta classifier for label 1\n",
      "finished training meta classifier for label 2\n",
      "finished training meta classifier for label 3\n",
      "finished training meta classifier for label 4\n",
      "finished training meta classifier for label 5\n",
      "finished training meta classifier for label 0\n",
      "finished training meta classifier for label 1\n",
      "finished training meta classifier for label 2\n",
      "finished training meta classifier for label 3\n",
      "finished training meta classifier for label 4\n",
      "finished training meta classifier for label 5\n",
      "finished training meta classifier for label 0\n",
      "finished training meta classifier for label 1\n",
      "finished training meta classifier for label 2\n",
      "finished training meta classifier for label 3\n",
      "finished training meta classifier for label 4\n",
      "finished training meta classifier for label 5\n",
      "finished training meta classifier for label 0\n",
      "finished training meta classifier for label 1\n",
      "finished training meta classifier for label 2\n",
      "finished training meta classifier for label 3\n",
      "finished training meta classifier for label 4\n",
      "finished training meta classifier for label 5\n",
      "results obtained:\n",
      "Accuracy: 0.0186 ± 0.02\n",
      "Hamming Loss: -0.3016 ± 0.03\n",
      "F1 score: 0.0640 ± 0.02\n",
      "## running dataset `birds`\n",
      "finished training meta classifier for label 0\n",
      "finished training meta classifier for label 1\n",
      "finished training meta classifier for label 2\n",
      "finished training meta classifier for label 3\n",
      "finished training meta classifier for label 4\n",
      "finished training meta classifier for label 5\n",
      "finished training meta classifier for label 6\n",
      "finished training meta classifier for label 7\n",
      "finished training meta classifier for label 8\n",
      "finished training meta classifier for label 9\n",
      "finished training meta classifier for label 10\n",
      "finished training meta classifier for label 11\n",
      "finished training meta classifier for label 12\n",
      "finished training meta classifier for label 13\n",
      "finished training meta classifier for label 14\n",
      "finished training meta classifier for label 15\n",
      "finished training meta classifier for label 16\n",
      "finished training meta classifier for label 17\n",
      "finished training meta classifier for label 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training meta classifier for label 0\n",
      "finished training meta classifier for label 1\n",
      "finished training meta classifier for label 2\n",
      "finished training meta classifier for label 3\n",
      "finished training meta classifier for label 4\n",
      "finished training meta classifier for label 5\n",
      "finished training meta classifier for label 6\n",
      "finished training meta classifier for label 7\n",
      "finished training meta classifier for label 8\n",
      "finished training meta classifier for label 9\n",
      "finished training meta classifier for label 10\n",
      "finished training meta classifier for label 11\n",
      "finished training meta classifier for label 12\n",
      "finished training meta classifier for label 13\n",
      "finished training meta classifier for label 14\n",
      "finished training meta classifier for label 15\n",
      "finished training meta classifier for label 16\n",
      "finished training meta classifier for label 17\n",
      "finished training meta classifier for label 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training meta classifier for label 0\n",
      "finished training meta classifier for label 1\n",
      "finished training meta classifier for label 2\n",
      "finished training meta classifier for label 3\n",
      "finished training meta classifier for label 4\n",
      "finished training meta classifier for label 5\n",
      "finished training meta classifier for label 6\n",
      "finished training meta classifier for label 7\n",
      "finished training meta classifier for label 8\n",
      "finished training meta classifier for label 9\n",
      "finished training meta classifier for label 10\n",
      "finished training meta classifier for label 11\n",
      "finished training meta classifier for label 12\n",
      "finished training meta classifier for label 13\n",
      "finished training meta classifier for label 14\n",
      "finished training meta classifier for label 15\n",
      "finished training meta classifier for label 16\n",
      "finished training meta classifier for label 17\n",
      "finished training meta classifier for label 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training meta classifier for label 0\n",
      "finished training meta classifier for label 1\n",
      "finished training meta classifier for label 2\n",
      "finished training meta classifier for label 3\n",
      "finished training meta classifier for label 4\n",
      "finished training meta classifier for label 5\n",
      "finished training meta classifier for label 6\n",
      "finished training meta classifier for label 7\n",
      "finished training meta classifier for label 8\n",
      "finished training meta classifier for label 9\n",
      "finished training meta classifier for label 10\n",
      "finished training meta classifier for label 11\n",
      "finished training meta classifier for label 12\n",
      "finished training meta classifier for label 13\n",
      "finished training meta classifier for label 14\n",
      "finished training meta classifier for label 15\n",
      "finished training meta classifier for label 16\n",
      "finished training meta classifier for label 17\n",
      "finished training meta classifier for label 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training meta classifier for label 0\n",
      "finished training meta classifier for label 1\n",
      "finished training meta classifier for label 2\n",
      "finished training meta classifier for label 3\n",
      "finished training meta classifier for label 4\n",
      "finished training meta classifier for label 5\n",
      "finished training meta classifier for label 6\n",
      "finished training meta classifier for label 7\n",
      "finished training meta classifier for label 8\n",
      "finished training meta classifier for label 9\n",
      "finished training meta classifier for label 10\n",
      "finished training meta classifier for label 11\n",
      "finished training meta classifier for label 12\n",
      "finished training meta classifier for label 13\n",
      "finished training meta classifier for label 14\n",
      "finished training meta classifier for label 15\n",
      "finished training meta classifier for label 16\n",
      "finished training meta classifier for label 17\n",
      "finished training meta classifier for label 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training meta classifier for label 0\n",
      "finished training meta classifier for label 1\n",
      "finished training meta classifier for label 2\n",
      "finished training meta classifier for label 3\n",
      "finished training meta classifier for label 4\n",
      "finished training meta classifier for label 5\n",
      "finished training meta classifier for label 6\n",
      "finished training meta classifier for label 7\n",
      "finished training meta classifier for label 8\n",
      "finished training meta classifier for label 9\n",
      "finished training meta classifier for label 10\n",
      "finished training meta classifier for label 11\n",
      "finished training meta classifier for label 12\n",
      "finished training meta classifier for label 13\n",
      "finished training meta classifier for label 14\n",
      "finished training meta classifier for label 15\n",
      "finished training meta classifier for label 16\n",
      "finished training meta classifier for label 17\n",
      "finished training meta classifier for label 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training meta classifier for label 0\n",
      "finished training meta classifier for label 1\n",
      "finished training meta classifier for label 2\n",
      "finished training meta classifier for label 3\n",
      "finished training meta classifier for label 4\n",
      "finished training meta classifier for label 5\n",
      "finished training meta classifier for label 6\n",
      "finished training meta classifier for label 7\n",
      "finished training meta classifier for label 8\n",
      "finished training meta classifier for label 9\n",
      "finished training meta classifier for label 10\n",
      "finished training meta classifier for label 11\n",
      "finished training meta classifier for label 12\n",
      "finished training meta classifier for label 13\n",
      "finished training meta classifier for label 14\n",
      "finished training meta classifier for label 15\n",
      "finished training meta classifier for label 16\n",
      "finished training meta classifier for label 17\n",
      "finished training meta classifier for label 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training meta classifier for label 0\n",
      "finished training meta classifier for label 1\n",
      "finished training meta classifier for label 2\n",
      "finished training meta classifier for label 3\n",
      "finished training meta classifier for label 4\n",
      "finished training meta classifier for label 5\n",
      "finished training meta classifier for label 6\n",
      "finished training meta classifier for label 7\n",
      "finished training meta classifier for label 8\n",
      "finished training meta classifier for label 9\n",
      "finished training meta classifier for label 10\n",
      "finished training meta classifier for label 11\n",
      "finished training meta classifier for label 12\n",
      "finished training meta classifier for label 13\n",
      "finished training meta classifier for label 14\n",
      "finished training meta classifier for label 15\n",
      "finished training meta classifier for label 16\n",
      "finished training meta classifier for label 17\n",
      "finished training meta classifier for label 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training meta classifier for label 0\n",
      "finished training meta classifier for label 1\n",
      "finished training meta classifier for label 2\n",
      "finished training meta classifier for label 3\n",
      "finished training meta classifier for label 4\n",
      "finished training meta classifier for label 5\n",
      "finished training meta classifier for label 6\n",
      "finished training meta classifier for label 7\n",
      "finished training meta classifier for label 8\n",
      "finished training meta classifier for label 9\n",
      "finished training meta classifier for label 10\n",
      "finished training meta classifier for label 11\n",
      "finished training meta classifier for label 12\n",
      "finished training meta classifier for label 13\n",
      "finished training meta classifier for label 14\n",
      "finished training meta classifier for label 15\n",
      "finished training meta classifier for label 16\n",
      "finished training meta classifier for label 17\n",
      "finished training meta classifier for label 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training meta classifier for label 0\n",
      "finished training meta classifier for label 1\n",
      "finished training meta classifier for label 2\n",
      "finished training meta classifier for label 3\n",
      "finished training meta classifier for label 4\n",
      "finished training meta classifier for label 5\n",
      "finished training meta classifier for label 6\n",
      "finished training meta classifier for label 7\n",
      "finished training meta classifier for label 8\n",
      "finished training meta classifier for label 9\n",
      "finished training meta classifier for label 10\n",
      "finished training meta classifier for label 11\n",
      "finished training meta classifier for label 12\n",
      "finished training meta classifier for label 13\n",
      "finished training meta classifier for label 14\n",
      "finished training meta classifier for label 15\n",
      "finished training meta classifier for label 16\n",
      "finished training meta classifier for label 17\n",
      "finished training meta classifier for label 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Edgard\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1570: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results obtained:\n",
      "Accuracy: 0.4637 ± 0.07\n",
      "Hamming Loss: -0.0535 ± 0.01\n",
      "F1 score: 0.0126 ± 0.01\n"
     ]
    }
   ],
   "source": [
    "for model_name, model in models.items():\n",
    "    print(f\"# running model `{model_name}`\")\n",
    "\n",
    "    evaluation_results[model_name] = {}\n",
    "\n",
    "    n_folds = 10\n",
    "    evaluation_pipeline = EvaluationPipeline(model, n_folds)\n",
    "\n",
    "    for dataset_name, info in datasets.items():\n",
    "        print(f\"## running dataset `{dataset_name}`\")\n",
    "\n",
    "        result = evaluation_pipeline.run(info[\"X\"], info[\"y\"])\n",
    "        evaluation_results[model_name][dataset_name] = result\n",
    "\n",
    "        print(f\"results obtained:\")\n",
    "        result.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'baseline_binary_relevance_model': {'scene': <evaluation.EvaluationPipelineResult at 0x2359ed1c760>,\n",
       "  'emotions': <evaluation.EvaluationPipelineResult at 0x2359b14d280>,\n",
       "  'birds': <evaluation.EvaluationPipelineResult at 0x2359b1672e0>},\n",
       " 'basic_stacking_model': {'scene': <evaluation.EvaluationPipelineResult at 0x2359b130130>,\n",
       "  'emotions': <evaluation.EvaluationPipelineResult at 0x23596b515e0>,\n",
       "  'birds': <evaluation.EvaluationPipelineResult at 0x2359ed2c730>},\n",
       " 'stacking_with_f_tests_model': {'scene': <evaluation.EvaluationPipelineResult at 0x2359ed15c40>,\n",
       "  'emotions': <evaluation.EvaluationPipelineResult at 0x2359ed1f760>,\n",
       "  'birds': <evaluation.EvaluationPipelineResult at 0x2358a348790>}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4637 ± 0.07\n",
      "Hamming Loss: -0.0535 ± 0.01\n",
      "F1 score: 0.0126 ± 0.01\n"
     ]
    }
   ],
   "source": [
    "evaluation_results[\"baseline_binary_relevance_model\"][\"birds\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4637 ± 0.07\n",
      "Hamming Loss: -0.0535 ± 0.01\n",
      "F1 score: 0.0126 ± 0.01\n"
     ]
    }
   ],
   "source": [
    "evaluation_results[\"basic_stacking_model\"][\"birds\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4637 ± 0.07\n",
      "Hamming Loss: -0.0535 ± 0.01\n",
      "F1 score: 0.0126 ± 0.01\n"
     ]
    }
   ],
   "source": [
    "evaluation_results[\"stacking_with_f_tests_model\"][\"birds\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results so far\n",
    "\n",
    "It is very weird, but even with different number of folds, the results are always identical for binary relevance and stacking. Stacking with F-test is usually a bit better, but only by a tiny bit. I am not sure if this is a bug or not, but let's try to check the results with a simple train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# running model `baseline_binary_relevance_model`\n",
      "## running dataset `scene`\n",
      "# running model `basic_stacking_model`\n",
      "## running dataset `scene`\n",
      "# running model `stacking_with_f_tests_model`\n",
      "## running dataset `scene`\n",
      "finished training meta classifier for label 0\n",
      "finished training meta classifier for label 1\n",
      "finished training meta classifier for label 2\n",
      "finished training meta classifier for label 3\n",
      "finished training meta classifier for label 4\n",
      "finished training meta classifier for label 5\n"
     ]
    }
   ],
   "source": [
    "simple_evaluation_results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"# running model `{model_name}`\")\n",
    "\n",
    "    simple_evaluation_results[model_name] = {}\n",
    "\n",
    "    for dataset_name, info in datasets.items():\n",
    "        if dataset_name == \"emotions\":\n",
    "            continue\n",
    "\n",
    "        if dataset_name == \"birds\":\n",
    "            continue\n",
    "\n",
    "        print(f\"## running dataset `{dataset_name}`\")\n",
    "        \n",
    "        model.fit(info[\"X_train\"], info[\"y_train\"])\n",
    "        predictions = model.predict(info[\"X_test\"])\n",
    "\n",
    "        simple_evaluation_results[model_name][dataset_name] = {\n",
    "            \"accuracy\": metrics.accuracy_score(info[\"y_test\"], predictions),\n",
    "            \"f1\": metrics.f1_score(info[\"y_test\"], predictions, average=\"macro\"),\n",
    "            \"hamming_loss\": metrics.hamming_loss(info[\"y_test\"], predictions)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'baseline_binary_relevance_model': {'scene': {'accuracy': 0.5869565217391305,\n",
       "   'f1': 0.7237789962754925,\n",
       "   'hamming_loss': 0.08416945373467112}},\n",
       " 'basic_stacking_model': {'scene': {'accuracy': 0.5869565217391305,\n",
       "   'f1': 0.7237789962754925,\n",
       "   'hamming_loss': 0.08416945373467112}},\n",
       " 'stacking_with_f_tests_model': {'scene': {'accuracy': 0.6279264214046822,\n",
       "   'f1': 0.761565955949257,\n",
       "   'hamming_loss': 0.08235785953177258}}}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_evaluation_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. First Model Experiments\n",
    "\n",
    "Experiments to handle the very first model for my thesis.\n",
    "T\n",
    "The plan is to implement a **basic stacking algorithm** using the existing Binary Relevance implementation, found in the `scikit-multilearn` library, and then implement the actual new model, which runs a specific feature selection process between each layer of the stacking algorithm.\n",
    "\n",
    "The initial experimentation follows [this page](http://scikit.ml/tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.dataset import load_dataset, available_data_sets\n",
    "from sklearn.svm import SVC\n",
    "from skmultilearn.base.problem_transformation import ProblemTransformationBase\n",
    "from typing import List, Optional, Any, Tuple, Dict\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import f_classif\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Setting up a basic stacking classifier\n",
    "\n",
    "At first, let's build a simple stacking implementation, so we can have a baseline to compare with the specialized stacking implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {('bibtex', 'undivided'): ['5c1e474c2fd026519aec931a26ad18a6',\n",
       "              'bibtex-undivided.scikitml.bz2'],\n",
       "             ('bibtex', 'test'): ['d250caa297d060374f59318ad6b93771',\n",
       "              'bibtex-test.scikitml.bz2'],\n",
       "             ('bibtex', 'train'): ['1dd15daca7b8b2c17d692bdadce5dc31',\n",
       "              'bibtex-train.scikitml.bz2'],\n",
       "             ('birds', 'undivided'): ['1da06f4ae896800547dabf89044584e1',\n",
       "              'birds-undivided.scikitml.bz2'],\n",
       "             ('birds', 'test'): ['77fbfcc66d77040d3806c2a5ea4ff829',\n",
       "              'birds-test.scikitml.bz2'],\n",
       "             ('birds', 'train'): ['5c2bacaa5506e904b6501cc50ddfefe2',\n",
       "              'birds-train.scikitml.bz2'],\n",
       "             ('Corel5k', 'undivided'): ['062ea897821608035748a2a3b200d382',\n",
       "              'Corel5k-undivided.scikitml.bz2'],\n",
       "             ('Corel5k', 'test'): ['cb91444418a2f8b9814d10d4696af9f0',\n",
       "              'Corel5k-test.scikitml.bz2'],\n",
       "             ('Corel5k', 'train'): ['1863ec41b872f75b5f07bdc4cdc419fd',\n",
       "              'Corel5k-train.scikitml.bz2'],\n",
       "             ('delicious', 'undivided'): ['45abc065f123f2ac4d4f6ea5c5a5f940',\n",
       "              'delicious-undivided.scikitml.bz2'],\n",
       "             ('delicious', 'test'): ['6dfb4b0c2ae56010ba0166c9c592cfb6',\n",
       "              'delicious-test.scikitml.bz2'],\n",
       "             ('delicious', 'train'): ['31964bf875bc359ed1460536cb85859e',\n",
       "              'delicious-train.scikitml.bz2'],\n",
       "             ('emotions', 'undivided'): ['569a6c4dad0fdd2d470b9ad41fd18b6b',\n",
       "              'emotions-undivided.scikitml.bz2'],\n",
       "             ('emotions', 'test'): ['0090d9d829f20f2d112dbf942511b199',\n",
       "              'emotions-test.scikitml.bz2'],\n",
       "             ('emotions', 'train'): ['d558ed3cca99742cf7e80731c4c04b8e',\n",
       "              'emotions-train.scikitml.bz2'],\n",
       "             ('enron', 'undivided'): ['54b5f5d53695920274d5b8e2ca986513',\n",
       "              'enron-undivided.scikitml.bz2'],\n",
       "             ('enron', 'test'): ['37a240406757efc03c0cac474d5438e9',\n",
       "              'enron-test.scikitml.bz2'],\n",
       "             ('enron', 'train'): ['0fc03a1837e9e65b68f5d6f849b1773f',\n",
       "              'enron-train.scikitml.bz2'],\n",
       "             ('genbase', 'undivided'): ['46af04a7f0554eb2e64a9fcf2b9d3a9c',\n",
       "              'genbase-undivided.scikitml.bz2'],\n",
       "             ('genbase', 'test'): ['129876a9411ed1adf2366c3c7f6c1dd8',\n",
       "              'genbase-test.scikitml.bz2'],\n",
       "             ('genbase', 'train'): ['a674eb31684fe824d80025c414b884d4',\n",
       "              'genbase-train.scikitml.bz2'],\n",
       "             ('mediamill', 'undivided'): ['e2f91f4ed10df70414622a1001e0c447',\n",
       "              'mediamill-undivided.scikitml.bz2'],\n",
       "             ('mediamill', 'test'): ['d17a675cf8c78674557f000739f893a7',\n",
       "              'mediamill-test.scikitml.bz2'],\n",
       "             ('mediamill', 'train'): ['a96c5d0ce916d724397b793e75baa0cb',\n",
       "              'mediamill-train.scikitml.bz2'],\n",
       "             ('medical', 'undivided'): ['b3eac7e58595898904a8e7aa148b259c',\n",
       "              'medical-undivided.scikitml.bz2'],\n",
       "             ('medical', 'test'): ['cd91418a6a616db88529482729cbb60b',\n",
       "              'medical-test.scikitml.bz2'],\n",
       "             ('medical', 'train'): ['cb91bcdba133b33ec53195a345e3ebba',\n",
       "              'medical-train.scikitml.bz2'],\n",
       "             ('rcv1subset1', 'undivided'): ['2d1906310260144a87f5de89edb402b0',\n",
       "              'rcv1subset1-undivided.scikitml.bz2'],\n",
       "             ('rcv1subset1', 'test'): ['30d0c3b155ece588073c1a7a609a2b7b',\n",
       "              'rcv1subset1-test.scikitml.bz2'],\n",
       "             ('rcv1subset1', 'train'): ['2482c6e57ff578b8b9f429df140c3866',\n",
       "              'rcv1subset1-train.scikitml.bz2'],\n",
       "             ('rcv1subset2', 'undivided'): ['e42a7f75eafd5adfe6452454c6cffee2',\n",
       "              'rcv1subset2-undivided.scikitml.bz2'],\n",
       "             ('rcv1subset2', 'test'): ['a16a22a36912d3388b94c2aba77d4462',\n",
       "              'rcv1subset2-test.scikitml.bz2'],\n",
       "             ('rcv1subset2', 'train'): ['2359343930e2010d0dc90b904430370c',\n",
       "              'rcv1subset2-train.scikitml.bz2'],\n",
       "             ('rcv1subset3', 'undivided'): ['2b2381df4801f4045df907e1ddac8238',\n",
       "              'rcv1subset3-undivided.scikitml.bz2'],\n",
       "             ('rcv1subset3', 'test'): ['e60511202e5688648205200477176f70',\n",
       "              'rcv1subset3-test.scikitml.bz2'],\n",
       "             ('rcv1subset3', 'train'): ['643b3d0a393d72ccd5e80e3ce97343ec',\n",
       "              'rcv1subset3-train.scikitml.bz2'],\n",
       "             ('rcv1subset4', 'undivided'): ['d28954a8d716de4d636600e80e2abb68',\n",
       "              'rcv1subset4-undivided.scikitml.bz2'],\n",
       "             ('rcv1subset4', 'test'): ['2a479638c7e4f280cb230d5bdb2365a8',\n",
       "              'rcv1subset4-test.scikitml.bz2'],\n",
       "             ('rcv1subset4', 'train'): ['a0a1ed6c94157cfe4722d351937abc8f',\n",
       "              'rcv1subset4-train.scikitml.bz2'],\n",
       "             ('rcv1subset5', 'undivided'): ['287a07504a8e6f9b61991d99d04cb7f8',\n",
       "              'rcv1subset5-undivided.scikitml.bz2'],\n",
       "             ('rcv1subset5', 'test'): ['6ab3fd6456d297b919e94a36d7754b85',\n",
       "              'rcv1subset5-test.scikitml.bz2'],\n",
       "             ('rcv1subset5', 'train'): ['b2c69f099486b0f39ed8630f20eba2d5',\n",
       "              'rcv1subset5-train.scikitml.bz2'],\n",
       "             ('scene', 'undivided'): ['627fad77dd17b7b236b6561c18457e5e',\n",
       "              'scene-undivided.scikitml.bz2'],\n",
       "             ('scene', 'test'): ['7e03886ed1c8a49cdb552c89ecb7185f',\n",
       "              'scene-test.scikitml.bz2'],\n",
       "             ('scene', 'train'): ['49a1866da9a56125dd32fc3ce0eeacf9',\n",
       "              'scene-train.scikitml.bz2'],\n",
       "             ('tmc2007_500', 'undivided'): ['963b8594395f5c0eec128b029dd2035a',\n",
       "              'tmc2007_500-undivided.scikitml.bz2'],\n",
       "             ('tmc2007_500', 'test'): ['1a17737bfe8cb24a9a8b487a443e68f2',\n",
       "              'tmc2007_500-test.scikitml.bz2'],\n",
       "             ('tmc2007_500', 'train'): ['93d50cff83fecade6991b2882ced553a',\n",
       "              'tmc2007_500-train.scikitml.bz2'],\n",
       "             ('yeast', 'undivided'): ['81996c76ec858ae4343f6f33ea1f0504',\n",
       "              'yeast-undivided.scikitml.bz2'],\n",
       "             ('yeast', 'test'): ['d868f6ea5f0c2b78a96477c6722fee13',\n",
       "              'yeast-test.scikitml.bz2'],\n",
       "             ('yeast', 'train'): ['90be9e0e636b624269794865e82c2667',\n",
       "              'yeast-train.scikitml.bz2']})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_data_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scene:train - exists, not redownloading\n",
      "scene:test - exists, not redownloading\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, feature_names, label_names = load_dataset(\"scene\", \"train\")\n",
    "X_test, y_test, _, _ = load_dataset(\"scene\", \"test\")\n",
    "\n",
    "# using `scene` as this dataset seems to be a little bit more comprehensible than the others I tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Att1', 'NUMERIC'),\n",
       " ('Att2', 'NUMERIC'),\n",
       " ('Att3', 'NUMERIC'),\n",
       " ('Att4', 'NUMERIC'),\n",
       " ('Att5', 'NUMERIC'),\n",
       " ('Att6', 'NUMERIC'),\n",
       " ('Att7', 'NUMERIC'),\n",
       " ('Att8', 'NUMERIC'),\n",
       " ('Att9', 'NUMERIC'),\n",
       " ('Att10', 'NUMERIC'),\n",
       " ('Att11', 'NUMERIC'),\n",
       " ('Att12', 'NUMERIC'),\n",
       " ('Att13', 'NUMERIC'),\n",
       " ('Att14', 'NUMERIC'),\n",
       " ('Att15', 'NUMERIC'),\n",
       " ('Att16', 'NUMERIC'),\n",
       " ('Att17', 'NUMERIC'),\n",
       " ('Att18', 'NUMERIC'),\n",
       " ('Att19', 'NUMERIC'),\n",
       " ('Att20', 'NUMERIC'),\n",
       " ('Att21', 'NUMERIC'),\n",
       " ('Att22', 'NUMERIC'),\n",
       " ('Att23', 'NUMERIC'),\n",
       " ('Att24', 'NUMERIC'),\n",
       " ('Att25', 'NUMERIC'),\n",
       " ('Att26', 'NUMERIC'),\n",
       " ('Att27', 'NUMERIC'),\n",
       " ('Att28', 'NUMERIC'),\n",
       " ('Att29', 'NUMERIC'),\n",
       " ('Att30', 'NUMERIC'),\n",
       " ('Att31', 'NUMERIC'),\n",
       " ('Att32', 'NUMERIC'),\n",
       " ('Att33', 'NUMERIC'),\n",
       " ('Att34', 'NUMERIC'),\n",
       " ('Att35', 'NUMERIC'),\n",
       " ('Att36', 'NUMERIC'),\n",
       " ('Att37', 'NUMERIC'),\n",
       " ('Att38', 'NUMERIC'),\n",
       " ('Att39', 'NUMERIC'),\n",
       " ('Att40', 'NUMERIC'),\n",
       " ('Att41', 'NUMERIC'),\n",
       " ('Att42', 'NUMERIC'),\n",
       " ('Att43', 'NUMERIC'),\n",
       " ('Att44', 'NUMERIC'),\n",
       " ('Att45', 'NUMERIC'),\n",
       " ('Att46', 'NUMERIC'),\n",
       " ('Att47', 'NUMERIC'),\n",
       " ('Att48', 'NUMERIC'),\n",
       " ('Att49', 'NUMERIC'),\n",
       " ('Att50', 'NUMERIC'),\n",
       " ('Att51', 'NUMERIC'),\n",
       " ('Att52', 'NUMERIC'),\n",
       " ('Att53', 'NUMERIC'),\n",
       " ('Att54', 'NUMERIC'),\n",
       " ('Att55', 'NUMERIC'),\n",
       " ('Att56', 'NUMERIC'),\n",
       " ('Att57', 'NUMERIC'),\n",
       " ('Att58', 'NUMERIC'),\n",
       " ('Att59', 'NUMERIC'),\n",
       " ('Att60', 'NUMERIC'),\n",
       " ('Att61', 'NUMERIC'),\n",
       " ('Att62', 'NUMERIC'),\n",
       " ('Att63', 'NUMERIC'),\n",
       " ('Att64', 'NUMERIC'),\n",
       " ('Att65', 'NUMERIC'),\n",
       " ('Att66', 'NUMERIC'),\n",
       " ('Att67', 'NUMERIC'),\n",
       " ('Att68', 'NUMERIC'),\n",
       " ('Att69', 'NUMERIC'),\n",
       " ('Att70', 'NUMERIC'),\n",
       " ('Att71', 'NUMERIC'),\n",
       " ('Att72', 'NUMERIC'),\n",
       " ('Att73', 'NUMERIC'),\n",
       " ('Att74', 'NUMERIC'),\n",
       " ('Att75', 'NUMERIC'),\n",
       " ('Att76', 'NUMERIC'),\n",
       " ('Att77', 'NUMERIC'),\n",
       " ('Att78', 'NUMERIC'),\n",
       " ('Att79', 'NUMERIC'),\n",
       " ('Att80', 'NUMERIC'),\n",
       " ('Att81', 'NUMERIC'),\n",
       " ('Att82', 'NUMERIC'),\n",
       " ('Att83', 'NUMERIC'),\n",
       " ('Att84', 'NUMERIC'),\n",
       " ('Att85', 'NUMERIC'),\n",
       " ('Att86', 'NUMERIC'),\n",
       " ('Att87', 'NUMERIC'),\n",
       " ('Att88', 'NUMERIC'),\n",
       " ('Att89', 'NUMERIC'),\n",
       " ('Att90', 'NUMERIC'),\n",
       " ('Att91', 'NUMERIC'),\n",
       " ('Att92', 'NUMERIC'),\n",
       " ('Att93', 'NUMERIC'),\n",
       " ('Att94', 'NUMERIC'),\n",
       " ('Att95', 'NUMERIC'),\n",
       " ('Att96', 'NUMERIC'),\n",
       " ('Att97', 'NUMERIC'),\n",
       " ('Att98', 'NUMERIC'),\n",
       " ('Att99', 'NUMERIC'),\n",
       " ('Att100', 'NUMERIC'),\n",
       " ('Att101', 'NUMERIC'),\n",
       " ('Att102', 'NUMERIC'),\n",
       " ('Att103', 'NUMERIC'),\n",
       " ('Att104', 'NUMERIC'),\n",
       " ('Att105', 'NUMERIC'),\n",
       " ('Att106', 'NUMERIC'),\n",
       " ('Att107', 'NUMERIC'),\n",
       " ('Att108', 'NUMERIC'),\n",
       " ('Att109', 'NUMERIC'),\n",
       " ('Att110', 'NUMERIC'),\n",
       " ('Att111', 'NUMERIC'),\n",
       " ('Att112', 'NUMERIC'),\n",
       " ('Att113', 'NUMERIC'),\n",
       " ('Att114', 'NUMERIC'),\n",
       " ('Att115', 'NUMERIC'),\n",
       " ('Att116', 'NUMERIC'),\n",
       " ('Att117', 'NUMERIC'),\n",
       " ('Att118', 'NUMERIC'),\n",
       " ('Att119', 'NUMERIC'),\n",
       " ('Att120', 'NUMERIC'),\n",
       " ('Att121', 'NUMERIC'),\n",
       " ('Att122', 'NUMERIC'),\n",
       " ('Att123', 'NUMERIC'),\n",
       " ('Att124', 'NUMERIC'),\n",
       " ('Att125', 'NUMERIC'),\n",
       " ('Att126', 'NUMERIC'),\n",
       " ('Att127', 'NUMERIC'),\n",
       " ('Att128', 'NUMERIC'),\n",
       " ('Att129', 'NUMERIC'),\n",
       " ('Att130', 'NUMERIC'),\n",
       " ('Att131', 'NUMERIC'),\n",
       " ('Att132', 'NUMERIC'),\n",
       " ('Att133', 'NUMERIC'),\n",
       " ('Att134', 'NUMERIC'),\n",
       " ('Att135', 'NUMERIC'),\n",
       " ('Att136', 'NUMERIC'),\n",
       " ('Att137', 'NUMERIC'),\n",
       " ('Att138', 'NUMERIC'),\n",
       " ('Att139', 'NUMERIC'),\n",
       " ('Att140', 'NUMERIC'),\n",
       " ('Att141', 'NUMERIC'),\n",
       " ('Att142', 'NUMERIC'),\n",
       " ('Att143', 'NUMERIC'),\n",
       " ('Att144', 'NUMERIC'),\n",
       " ('Att145', 'NUMERIC'),\n",
       " ('Att146', 'NUMERIC'),\n",
       " ('Att147', 'NUMERIC'),\n",
       " ('Att148', 'NUMERIC'),\n",
       " ('Att149', 'NUMERIC'),\n",
       " ('Att150', 'NUMERIC'),\n",
       " ('Att151', 'NUMERIC'),\n",
       " ('Att152', 'NUMERIC'),\n",
       " ('Att153', 'NUMERIC'),\n",
       " ('Att154', 'NUMERIC'),\n",
       " ('Att155', 'NUMERIC'),\n",
       " ('Att156', 'NUMERIC'),\n",
       " ('Att157', 'NUMERIC'),\n",
       " ('Att158', 'NUMERIC'),\n",
       " ('Att159', 'NUMERIC'),\n",
       " ('Att160', 'NUMERIC'),\n",
       " ('Att161', 'NUMERIC'),\n",
       " ('Att162', 'NUMERIC'),\n",
       " ('Att163', 'NUMERIC'),\n",
       " ('Att164', 'NUMERIC'),\n",
       " ('Att165', 'NUMERIC'),\n",
       " ('Att166', 'NUMERIC'),\n",
       " ('Att167', 'NUMERIC'),\n",
       " ('Att168', 'NUMERIC'),\n",
       " ('Att169', 'NUMERIC'),\n",
       " ('Att170', 'NUMERIC'),\n",
       " ('Att171', 'NUMERIC'),\n",
       " ('Att172', 'NUMERIC'),\n",
       " ('Att173', 'NUMERIC'),\n",
       " ('Att174', 'NUMERIC'),\n",
       " ('Att175', 'NUMERIC'),\n",
       " ('Att176', 'NUMERIC'),\n",
       " ('Att177', 'NUMERIC'),\n",
       " ('Att178', 'NUMERIC'),\n",
       " ('Att179', 'NUMERIC'),\n",
       " ('Att180', 'NUMERIC'),\n",
       " ('Att181', 'NUMERIC'),\n",
       " ('Att182', 'NUMERIC'),\n",
       " ('Att183', 'NUMERIC'),\n",
       " ('Att184', 'NUMERIC'),\n",
       " ('Att185', 'NUMERIC'),\n",
       " ('Att186', 'NUMERIC'),\n",
       " ('Att187', 'NUMERIC'),\n",
       " ('Att188', 'NUMERIC'),\n",
       " ('Att189', 'NUMERIC'),\n",
       " ('Att190', 'NUMERIC'),\n",
       " ('Att191', 'NUMERIC'),\n",
       " ('Att192', 'NUMERIC'),\n",
       " ('Att193', 'NUMERIC'),\n",
       " ('Att194', 'NUMERIC'),\n",
       " ('Att195', 'NUMERIC'),\n",
       " ('Att196', 'NUMERIC'),\n",
       " ('Att197', 'NUMERIC'),\n",
       " ('Att198', 'NUMERIC'),\n",
       " ('Att199', 'NUMERIC'),\n",
       " ('Att200', 'NUMERIC'),\n",
       " ('Att201', 'NUMERIC'),\n",
       " ('Att202', 'NUMERIC'),\n",
       " ('Att203', 'NUMERIC'),\n",
       " ('Att204', 'NUMERIC'),\n",
       " ('Att205', 'NUMERIC'),\n",
       " ('Att206', 'NUMERIC'),\n",
       " ('Att207', 'NUMERIC'),\n",
       " ('Att208', 'NUMERIC'),\n",
       " ('Att209', 'NUMERIC'),\n",
       " ('Att210', 'NUMERIC'),\n",
       " ('Att211', 'NUMERIC'),\n",
       " ('Att212', 'NUMERIC'),\n",
       " ('Att213', 'NUMERIC'),\n",
       " ('Att214', 'NUMERIC'),\n",
       " ('Att215', 'NUMERIC'),\n",
       " ('Att216', 'NUMERIC'),\n",
       " ('Att217', 'NUMERIC'),\n",
       " ('Att218', 'NUMERIC'),\n",
       " ('Att219', 'NUMERIC'),\n",
       " ('Att220', 'NUMERIC'),\n",
       " ('Att221', 'NUMERIC'),\n",
       " ('Att222', 'NUMERIC'),\n",
       " ('Att223', 'NUMERIC'),\n",
       " ('Att224', 'NUMERIC'),\n",
       " ('Att225', 'NUMERIC'),\n",
       " ('Att226', 'NUMERIC'),\n",
       " ('Att227', 'NUMERIC'),\n",
       " ('Att228', 'NUMERIC'),\n",
       " ('Att229', 'NUMERIC'),\n",
       " ('Att230', 'NUMERIC'),\n",
       " ('Att231', 'NUMERIC'),\n",
       " ('Att232', 'NUMERIC'),\n",
       " ('Att233', 'NUMERIC'),\n",
       " ('Att234', 'NUMERIC'),\n",
       " ('Att235', 'NUMERIC'),\n",
       " ('Att236', 'NUMERIC'),\n",
       " ('Att237', 'NUMERIC'),\n",
       " ('Att238', 'NUMERIC'),\n",
       " ('Att239', 'NUMERIC'),\n",
       " ('Att240', 'NUMERIC'),\n",
       " ('Att241', 'NUMERIC'),\n",
       " ('Att242', 'NUMERIC'),\n",
       " ('Att243', 'NUMERIC'),\n",
       " ('Att244', 'NUMERIC'),\n",
       " ('Att245', 'NUMERIC'),\n",
       " ('Att246', 'NUMERIC'),\n",
       " ('Att247', 'NUMERIC'),\n",
       " ('Att248', 'NUMERIC'),\n",
       " ('Att249', 'NUMERIC'),\n",
       " ('Att250', 'NUMERIC'),\n",
       " ('Att251', 'NUMERIC'),\n",
       " ('Att252', 'NUMERIC'),\n",
       " ('Att253', 'NUMERIC'),\n",
       " ('Att254', 'NUMERIC'),\n",
       " ('Att255', 'NUMERIC'),\n",
       " ('Att256', 'NUMERIC'),\n",
       " ('Att257', 'NUMERIC'),\n",
       " ('Att258', 'NUMERIC'),\n",
       " ('Att259', 'NUMERIC'),\n",
       " ('Att260', 'NUMERIC'),\n",
       " ('Att261', 'NUMERIC'),\n",
       " ('Att262', 'NUMERIC'),\n",
       " ('Att263', 'NUMERIC'),\n",
       " ('Att264', 'NUMERIC'),\n",
       " ('Att265', 'NUMERIC'),\n",
       " ('Att266', 'NUMERIC'),\n",
       " ('Att267', 'NUMERIC'),\n",
       " ('Att268', 'NUMERIC'),\n",
       " ('Att269', 'NUMERIC'),\n",
       " ('Att270', 'NUMERIC'),\n",
       " ('Att271', 'NUMERIC'),\n",
       " ('Att272', 'NUMERIC'),\n",
       " ('Att273', 'NUMERIC'),\n",
       " ('Att274', 'NUMERIC'),\n",
       " ('Att275', 'NUMERIC'),\n",
       " ('Att276', 'NUMERIC'),\n",
       " ('Att277', 'NUMERIC'),\n",
       " ('Att278', 'NUMERIC'),\n",
       " ('Att279', 'NUMERIC'),\n",
       " ('Att280', 'NUMERIC'),\n",
       " ('Att281', 'NUMERIC'),\n",
       " ('Att282', 'NUMERIC'),\n",
       " ('Att283', 'NUMERIC'),\n",
       " ('Att284', 'NUMERIC'),\n",
       " ('Att285', 'NUMERIC'),\n",
       " ('Att286', 'NUMERIC'),\n",
       " ('Att287', 'NUMERIC'),\n",
       " ('Att288', 'NUMERIC'),\n",
       " ('Att289', 'NUMERIC'),\n",
       " ('Att290', 'NUMERIC'),\n",
       " ('Att291', 'NUMERIC'),\n",
       " ('Att292', 'NUMERIC'),\n",
       " ('Att293', 'NUMERIC'),\n",
       " ('Att294', 'NUMERIC')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Beach', ['0', '1']),\n",
       " ('Sunset', ['0', '1']),\n",
       " ('FallFoliage', ['0', '1']),\n",
       " ('Field', ['0', '1']),\n",
       " ('Mountain', ['0', '1']),\n",
       " ('Urban', ['0', '1'])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `scene` as this dataset seems to be a little bit more comprehensible than the others I tested.\n",
    "\n",
    "For more information regarding the datasets and how `scikit-multilearn` handles them, [read this page](http://scikit.ml/datasets.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = BinaryRelevance(\n",
    "    classifier=SVC(),\n",
    "    require_dense=[False, True]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryRelevance(classifier=SVC(), require_dense=[False, True])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Beach', ['0', '1']),\n",
       " ('Sunset', ['0', '1']),\n",
       " ('FallFoliage', ['0', '1']),\n",
       " ('Field', ['0', '1']),\n",
       " ('Mountain', ['0', '1']),\n",
       " ('Urban', ['0', '1'])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08416945373467112"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_br_hamming_loss = metrics.hamming_loss(y_test, prediction)\n",
    "regular_br_hamming_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5869565217391305"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_br_accuracy_score = metrics.accuracy_score(y_test, prediction)\n",
    "regular_br_accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicStacking(ProblemTransformationBase):\n",
    "    first_layer_classifiers: BinaryRelevance\n",
    "    second_layer_classifiers: BinaryRelevance\n",
    "\n",
    "    def __init__(self, classifier: Any = None, require_dense: Optional[List[bool]] = None):\n",
    "        super(BasicStacking, self).__init__(classifier, require_dense)\n",
    "\n",
    "        self.first_layer_classifiers = BinaryRelevance(\n",
    "            classifier=SVC(),\n",
    "            require_dense=[False, True]\n",
    "        )\n",
    "\n",
    "        self.second_layer_classifiers = BinaryRelevance(\n",
    "            classifier=SVC(),\n",
    "            require_dense=[False, True]\n",
    "        )\n",
    "    \n",
    "    def fit(self, X: Any, y: Any):\n",
    "        self.first_layer_classifiers.fit(X, y)\n",
    "\n",
    "        first_layer_predictions = self.first_layer_classifiers.predict(X)\n",
    "        X_expanded = np.hstack([X.todense(), first_layer_predictions.todense()])\n",
    "\n",
    "        self.second_layer_classifiers.fit(X_expanded, y)\n",
    "    \n",
    "    def predict(self, X: Any):\n",
    "        first_layer_predictions = self.first_layer_classifiers.predict(X)\n",
    "        X_expanded = np.hstack([X.todense(), first_layer_predictions.todense()])\n",
    "        return self.second_layer_classifiers.predict(X_expanded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BasicStacking()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking_prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "br hamming_loss 0.08416945373467112\n",
      "br accuracy 0.5869565217391305\n",
      "===\n",
      "stacking hamming_loss 0.08416945373467112\n",
      "stacking accuracy 0.5869565217391305\n"
     ]
    }
   ],
   "source": [
    "stacking_hamming_loss = metrics.hamming_loss(y_test, stacking_prediction)\n",
    "stacking_accuracy_score = metrics.accuracy_score(y_test, stacking_prediction)\n",
    "\n",
    "print(\"br hamming_loss\", regular_br_hamming_loss)\n",
    "print(\"br accuracy\", regular_br_accuracy_score)\n",
    "print(\"===\")\n",
    "print(\"stacking hamming_loss\", stacking_hamming_loss)\n",
    "print(\"stacking accuracy\", stacking_accuracy_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Both the Binary Relevance and the Stacking approaches resulted in the exact same performance.\n",
    "\n",
    "Possibilites:\n",
    "* The stacking implementation is wrong.\n",
    "  * To test this, we have to review the code and compare it to other stacking implementations; for instance: the stacking implementation in the `utiml` library for R.\n",
    "* The stacking implementation is correct, but the labels are not correlated at all, meaning that the stacking approach is not useful.\n",
    "  * To test this, we have to check other datasets and see if the stacking approach is useful in those cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Using other datasets\n",
    "\n",
    "I will try to list all available datasets, then try all of them using Binary Relevance as a baseline and my Basic Stacking approach. The objective is to see if the performance metrics change for any of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SplitDataset = Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]\n",
    "\n",
    "class BasicStackingAgainstBinaryRelevanceBaselineTest:\n",
    "    def run(self):\n",
    "        results = []\n",
    "\n",
    "        for name in self.get_unique_available_data_set_names():\n",
    "            data_sets = self.get_data_set(name)\n",
    "\n",
    "            baseline_metrics = self.get_metrics_for_baseline_model(data_sets)\n",
    "            stacking_metrics = self.get_metrics_for_basic_stacking(data_sets)\n",
    "\n",
    "            results.append({\n",
    "                \"dataset_name\": name,\n",
    "                \"baseline_accuracy\": baseline_metrics[0],\n",
    "                \"baseline_hamming_loss\": baseline_metrics[1],\n",
    "                \"stacking_accuracy\": stacking_metrics[0],\n",
    "                \"stacking_hamming_loss\": stacking_metrics[1]\n",
    "            })\n",
    "\n",
    "            self.save_results(results)\n",
    "            print(f\"finished baseline for {name}\")\n",
    "\n",
    "    def get_metrics_for_baseline_model(self, split_dataset: SplitDataset) -> Tuple[float, float]:\n",
    "        clf = BinaryRelevance(\n",
    "            classifier=SVC(),\n",
    "            require_dense=[False, True]\n",
    "        )\n",
    "        return self.get_metrics_for_model(clf, split_dataset)\n",
    "\n",
    "    def get_metrics_for_basic_stacking(self, split_dataset: SplitDataset) -> Tuple[float, float]:\n",
    "        clf = BasicStacking()\n",
    "        return self.get_metrics_for_model(clf, split_dataset)\n",
    "\n",
    "    def get_metrics_for_model(self, clf: Any, split_dataset: SplitDataset) -> Tuple[float, float]:\n",
    "        X_train, y_train, X_test, y_test = split_dataset\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        prediction = clf.predict(X_test)\n",
    "        accuracy_score = metrics.accuracy_score(y_test, prediction)\n",
    "        hamming_loss = metrics.hamming_loss(y_test, prediction)\n",
    "\n",
    "        return accuracy_score, hamming_loss\n",
    "\n",
    "    def save_results(self, results: List[Dict[str, Any]]):\n",
    "        with open(\"1_first_model_experiments_other_datasets_metrics.json\", \"w\") as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "\n",
    "    def get_unique_available_data_set_names(self) -> List[str]:\n",
    "        dataset_names = []\n",
    "\n",
    "        _available_data_sets = available_data_sets()\n",
    "        if _available_data_sets is None:\n",
    "            raise Exception(\"could not load available data sets\")\n",
    "\n",
    "        skip = [\"bibtex\", \"Corel5k\", \"delicious\", \"enron\", \"genbase\", \"mediamill\", \"medical\", \"rcv1subset1\"]\n",
    "        # bibtex -> takes too long to train and test\n",
    "        # Corel5k -> apparently it has only a single class, which is not accepted by the classifier\n",
    "        # delicious -> takes too long to train and test\n",
    "        # enron -> apparently it has only a single class, which is not accepted by the classifier\n",
    "        # genbase -> apparently it has only a single class, which is not accepted by the classifier\n",
    "        # mediamill -> takes too long to train and test\n",
    "        # medical -> apparently it has only a single class, which is not accepted by the classifier\n",
    "        # rcv1subset1 -> takes too long to train and test\n",
    "\n",
    "        for dataset_name, variant in _available_data_sets:\n",
    "            if dataset_name in skip:\n",
    "                continue\n",
    "\n",
    "            if dataset_name not in dataset_names:\n",
    "                dataset_names.append(dataset_name)\n",
    "\n",
    "        return dataset_names\n",
    "\n",
    "    def get_data_set(self, name: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        train_data = load_dataset(name, \"train\")\n",
    "        if train_data is None:\n",
    "            raise Exception(f\"could not load data set {name}\")\n",
    "\n",
    "        test_data = load_dataset(name, \"test\")\n",
    "        if test_data is None:\n",
    "            raise Exception(f\"could not load data set {name}\")\n",
    "        \n",
    "        X_train, y_train, _, _ = train_data\n",
    "        X_test, y_test, _, _ = test_data\n",
    "        \n",
    "        return X_train, y_train, X_test, y_test\n",
    "\n",
    "# using `scene` as this dataset seems to be a little bit more comprehensible than the others I tested\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "birds:train - exists, not redownloading\n",
      "birds:test - exists, not redownloading\n",
      "finished baseline for birds\n",
      "emotions:train - exists, not redownloading\n",
      "emotions:test - exists, not redownloading\n",
      "finished baseline for emotions\n",
      "rcv1subset2:train - does not exists downloading\n",
      "Downloaded rcv1subset2-train\n",
      "rcv1subset2:test - does not exists downloading\n",
      "Downloaded rcv1subset2-test\n",
      "finished baseline for rcv1subset2\n",
      "rcv1subset3:train - does not exists downloading\n",
      "Downloaded rcv1subset3-train\n",
      "rcv1subset3:test - does not exists downloading\n",
      "Downloaded rcv1subset3-test\n",
      "finished baseline for rcv1subset3\n",
      "rcv1subset4:train - does not exists downloading\n",
      "Downloaded rcv1subset4-train\n",
      "rcv1subset4:test - does not exists downloading\n",
      "Downloaded rcv1subset4-test\n",
      "finished baseline for rcv1subset4\n",
      "rcv1subset5:train - does not exists downloading\n",
      "Downloaded rcv1subset5-train\n",
      "rcv1subset5:test - does not exists downloading\n",
      "Downloaded rcv1subset5-test\n",
      "finished baseline for rcv1subset5\n",
      "scene:train - exists, not redownloading\n",
      "scene:test - exists, not redownloading\n",
      "finished baseline for scene\n",
      "tmc2007_500:train - does not exists downloading\n",
      "Downloaded tmc2007_500-train\n",
      "tmc2007_500:test - does not exists downloading\n",
      "Downloaded tmc2007_500-test\n"
     ]
    }
   ],
   "source": [
    "# notice: this cell takes a long time to run\n",
    "# the results are already saved in the file `1_first_model_experiments_other_datasets_metrics.json`\n",
    "# so you can skip this cell\n",
    "\n",
    "pipeline = BasicStackingAgainstBinaryRelevanceBaselineTest()\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The pipeline compares metrics obtained for a regular `BinaryRelevance` model and my basic implementation for a stacking model.\n",
    "\n",
    "After running this comparison for almost all datasets, we can conclude that there's nearly no difference between using the regular `BinaryRelevance` and the stacking implementation.\n",
    "\n",
    "These are the datasets that were tested:\n",
    "\n",
    "- birds\n",
    "- emotions\n",
    "- rcv1subset2\n",
    "- rcv1subset3\n",
    "- rcv1subset4\n",
    "- rcv1subset5\n",
    "- scene\n",
    "\n",
    "Other datasets were skipped for taking an abnormally long time to run. Also, some final datasets were not used as I considered I had enough results that were representative of the whole.\n",
    "\n",
    "Some datasets, for some reason, take quite some more time to train and they end up resulting in 0.00 _accuracy_. However, for these datasets, the _hamming loss_ has a value `>0`, and there's a very tiny difference between the stacking and the regular binary relevance models. **So we can say that the implementation seems to be doing at least _something_, even if completely minimal**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Comparing to other stacking implementations\n",
    "\n",
    "**TODO: move this section to after the \"using other datasets\".**\n",
    "\n",
    "Let's compare my implementation of the stacking classifier to the one from [utiml](https://github.com/rivolli/utiml).\n",
    "\n",
    "[Here is a guide](https://cran.r-project.org/web/packages/utiml/vignettes/utiml-overview.html) to get started with the `utiml` library.\n",
    "\n",
    "2023-08-17: I tried to read the source code, but I could not spot exactly how the stacking is being implemented. As a matter of fact, I don't even know if my understanding of the stacking is correct.\n",
    "\n",
    "For now, I will proceed with implementing the actual model, and then I will try to go back and debug all of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Implementing the specialized model\n",
    "\n",
    "Model based on this article: [\"An efficient stacking model with label selection for multi-label classification\"](https://link.springer.com/article/10.1007/s10489-020-01807-z).\n",
    "\n",
    "Let's start with some basic experiments to understand how to code certain sections of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scene:train - exists, not redownloading\n",
      "scene:test - exists, not redownloading\n",
      "Accuracy score:  0.5869565217391305\n",
      "Hamming loss:  0.08416945373467112\n"
     ]
    }
   ],
   "source": [
    "# doing some short experiments below\n",
    "# lets start with a baseline model\n",
    "\n",
    "name = \"scene\"\n",
    "\n",
    "train_data = load_dataset(name, \"train\")\n",
    "test_data = load_dataset(name, \"test\")\n",
    "\n",
    "X_train, y_train, _, _ = train_data\n",
    "X_test, y_test, _, _ = test_data\n",
    "\n",
    "clf = BinaryRelevance(\n",
    "    classifier=SVC(),\n",
    "    require_dense=[False, True]\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy score: \", metrics.accuracy_score(y_test, preds))\n",
    "print(\"Hamming loss: \", metrics.hamming_loss(y_test, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1196, 6)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.todense().shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_count = preds.shape[1]\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(0, labels_count):\n",
    "    for j in range(0, labels_count):\n",
    "        if i == j:\n",
    "            continue\n",
    "\n",
    "        X = preds.todense()[:, i]\n",
    "        X = np.asarray(X).reshape(-1, 1)\n",
    "        y = preds.todense()[:, j]\n",
    "        y = np.asarray(y).reshape(-1)\n",
    "\n",
    "        f_test_result = float(f_classif(X, y)[0])\n",
    "        # as per the documentation: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html\n",
    "        # `f_classif` returns a tuple of two arrays, the first one is the F-value and the second one is the p-value\n",
    "        # we only need the F-value\n",
    "\n",
    "        results.append({\n",
    "            \"label_being_tested\": i,\n",
    "            \"against_label\": j,\n",
    "            \"f_test_result\": f_test_result\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label_being_tested': 0,\n",
       "  'against_label': 1,\n",
       "  'f_test_result': 28.525597269624583},\n",
       " {'label_being_tested': 0,\n",
       "  'against_label': 2,\n",
       "  'f_test_result': 30.483421750663112},\n",
       " {'label_being_tested': 0,\n",
       "  'against_label': 3,\n",
       "  'f_test_result': 33.838056680161955},\n",
       " {'label_being_tested': 0,\n",
       "  'against_label': 4,\n",
       "  'f_test_result': 17.263665283934625},\n",
       " {'label_being_tested': 0,\n",
       "  'against_label': 5,\n",
       "  'f_test_result': 19.69910729184047},\n",
       " {'label_being_tested': 1,\n",
       "  'against_label': 0,\n",
       "  'f_test_result': 28.52559726962455},\n",
       " {'label_being_tested': 1,\n",
       "  'against_label': 2,\n",
       "  'f_test_result': 26.642728910601583},\n",
       " {'label_being_tested': 1,\n",
       "  'against_label': 3,\n",
       "  'f_test_result': 32.59656218402426},\n",
       " {'label_being_tested': 1,\n",
       "  'against_label': 4,\n",
       "  'f_test_result': 10.297689123645533},\n",
       " {'label_being_tested': 1,\n",
       "  'against_label': 5,\n",
       "  'f_test_result': 18.98438159560994},\n",
       " {'label_being_tested': 2,\n",
       "  'against_label': 0,\n",
       "  'f_test_result': 30.48342175066313},\n",
       " {'label_being_tested': 2,\n",
       "  'against_label': 1,\n",
       "  'f_test_result': 26.642728910601594},\n",
       " {'label_being_tested': 2,\n",
       "  'against_label': 3,\n",
       "  'f_test_result': 19.79370510346035},\n",
       " {'label_being_tested': 2,\n",
       "  'against_label': 4,\n",
       "  'f_test_result': 22.93396034664349},\n",
       " {'label_being_tested': 2,\n",
       "  'against_label': 5,\n",
       "  'f_test_result': 17.721247826490103},\n",
       " {'label_being_tested': 3,\n",
       "  'against_label': 0,\n",
       "  'f_test_result': 33.83805668016194},\n",
       " {'label_being_tested': 3,\n",
       "  'against_label': 1,\n",
       "  'f_test_result': 32.59656218402426},\n",
       " {'label_being_tested': 3,\n",
       "  'against_label': 2,\n",
       "  'f_test_result': 19.79370510346035},\n",
       " {'label_being_tested': 3,\n",
       "  'against_label': 4,\n",
       "  'f_test_result': 11.755093524743321},\n",
       " {'label_being_tested': 3,\n",
       "  'against_label': 5,\n",
       "  'f_test_result': 22.487249858331744},\n",
       " {'label_being_tested': 4,\n",
       "  'against_label': 0,\n",
       "  'f_test_result': 17.26366528393462},\n",
       " {'label_being_tested': 4,\n",
       "  'against_label': 1,\n",
       "  'f_test_result': 10.297689123645531},\n",
       " {'label_being_tested': 4,\n",
       "  'against_label': 2,\n",
       "  'f_test_result': 22.933960346643513},\n",
       " {'label_being_tested': 4,\n",
       "  'against_label': 3,\n",
       "  'f_test_result': 11.755093524743314},\n",
       " {'label_being_tested': 4,\n",
       "  'against_label': 5,\n",
       "  'f_test_result': 2.495364053299142},\n",
       " {'label_being_tested': 5,\n",
       "  'against_label': 0,\n",
       "  'f_test_result': 19.699107291840463},\n",
       " {'label_being_tested': 5,\n",
       "  'against_label': 1,\n",
       "  'f_test_result': 18.984381595609946},\n",
       " {'label_being_tested': 5,\n",
       "  'against_label': 2,\n",
       "  'f_test_result': 17.721247826490107},\n",
       " {'label_being_tested': 5,\n",
       "  'against_label': 3,\n",
       "  'f_test_result': 22.487249858331758},\n",
       " {'label_being_tested': 5,\n",
       "  'against_label': 4,\n",
       "  'f_test_result': 2.495364053299164}]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackingWithFTests(ProblemTransformationBase):\n",
    "    first_layer_classifiers: BinaryRelevance\n",
    "    second_layer_classifiers: List[Any] # TODO should be any generic type of classifier\n",
    "\n",
    "    def __init__(self, classifier: Any = None, require_dense: Optional[List[bool]] = None):\n",
    "        super(StackingWithFTests, self).__init__(classifier, require_dense)\n",
    "\n",
    "        self.first_layer_classifiers = BinaryRelevance(\n",
    "            classifier=SVC(),\n",
    "            require_dense=[False, True]\n",
    "        )\n",
    "        # TODO: allow for any base model (base classifier) to be used\n",
    "        # right now I am forcing the use of SVC\n",
    "\n",
    "        self.second_layer_classifiers = []\n",
    "        self.correlated_labels_map = pd.DataFrame()\n",
    "\n",
    "    def fit(self, X: Any, y: Any):\n",
    "        self.first_layer_classifiers.fit(X, y)\n",
    "\n",
    "        predictions = self.first_layer_classifiers.predict(X)\n",
    "        # TODO: does it really make sense to take predictions over the same set of data used for training?\n",
    "        \n",
    "        alpha = 0.6\n",
    "\n",
    "        f_tested_label_pairs = self.calculate_f_test_for_all_label_pairs(predictions)\n",
    "        self.correlated_labels_map = self.get_map_of_correlated_labels(f_tested_label_pairs, alpha)\n",
    "\n",
    "        labels_count = predictions.shape[1]\n",
    "\n",
    "        for i in range(labels_count):\n",
    "            mask = self.correlated_labels_map[\"for_label\"] == i\n",
    "            split_df = self.correlated_labels_map[mask].reset_index(drop=True)\n",
    "            labels_to_expand = split_df[\"expand_this_label\"].to_list()\n",
    "\n",
    "            additional_input = predictions.todense()[:, labels_to_expand]\n",
    "            \n",
    "            X_expanded = np.hstack([X.todense(), additional_input])\n",
    "            X_expanded = np.asarray(X_expanded)\n",
    "\n",
    "            y_label_specific = y.todense()[:, i]\n",
    "            y_label_specific = self.convert_matrix_to_vector(y_label_specific)\n",
    "\n",
    "            meta_classifier = SVC()\n",
    "            meta_classifier.fit(X_expanded, y_label_specific)\n",
    "\n",
    "            self.second_layer_classifiers.append(meta_classifier)\n",
    "            print(f\"finished training meta classifier for label {i}\")\n",
    "    \n",
    "    def calculate_f_test_for_all_label_pairs(self, predictions: Any) -> List[Dict[str, Any]]:\n",
    "        labels_count = predictions.shape[1]\n",
    "        results = []\n",
    "\n",
    "        for i in range(0, labels_count):\n",
    "            for j in range(0, labels_count):\n",
    "                if i == j:\n",
    "                    continue\n",
    "\n",
    "                X = predictions.todense()[:, i]\n",
    "                base_label = self.convert_matrix_to_array(X)\n",
    "\n",
    "                y = predictions.todense()[:, j]\n",
    "                against_label = self.convert_matrix_to_vector(y)\n",
    "\n",
    "                f_test_result = float(f_classif(base_label, against_label)[0])\n",
    "\n",
    "                results.append({\n",
    "                    \"label_being_tested\": i,\n",
    "                    \"against_label\": j,\n",
    "                    \"f_test_result\": f_test_result\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def convert_matrix_to_array(self, matrix: Any):\n",
    "        return np.asarray(matrix).reshape(-1, 1)\n",
    "\n",
    "    def convert_matrix_to_vector(self, matrix: Any):\n",
    "        return np.asarray(matrix).reshape(-1)\n",
    "    \n",
    "    def get_map_of_correlated_labels(self, f_test_results: List[Dict[str, Any]], alpha: float) -> pd.DataFrame:\n",
    "        # TODO must ensure alpha > 0.0 && <= 1.0\n",
    "\n",
    "        temp_df = pd.DataFrame(f_test_results)\n",
    "        \n",
    "        sorted_temp_df = temp_df.sort_values(\n",
    "            by=[\"label_being_tested\", \"f_test_result\"],\n",
    "            ascending=[True, False])\n",
    "        # this will make it easier to select the features afterwards\n",
    "\n",
    "        selected_features = []\n",
    "\n",
    "        for i in range(0, 6):\n",
    "            mask = sorted_temp_df[\"label_being_tested\"] == i\n",
    "            split_df = sorted_temp_df[mask].reset_index(drop=True)\n",
    "\n",
    "            big_f = split_df[\"f_test_result\"].sum()\n",
    "            max_cum_f = alpha * big_f\n",
    "\n",
    "            cum_f = 0\n",
    "            for _, row in split_df.iterrows():\n",
    "                cum_f += row[\"f_test_result\"]\n",
    "                if cum_f > max_cum_f:\n",
    "                    break\n",
    "\n",
    "                selected_features.append({\n",
    "                    \"for_label\": i,\n",
    "                    \"expand_this_label\": int(row[\"against_label\"]),\n",
    "                    \"f_test_result\": float(row[\"f_test_result\"]),\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(selected_features)\n",
    "    \n",
    "    def predict(self, X: Any) -> List[Any]:\n",
    "        if self.correlated_labels_map.empty:\n",
    "            raise Exception(\"model was not trained yet\")\n",
    "\n",
    "\n",
    "        predictions = self.first_layer_classifiers.predict(X)\n",
    "        labels_count = predictions.shape[1]\n",
    "\n",
    "        second_layer_predictions = []\n",
    "\n",
    "        for i in range(labels_count):\n",
    "            mask = self.correlated_labels_map[\"for_label\"] == i\n",
    "            split_df = self.correlated_labels_map[mask].reset_index(drop=True)\n",
    "            labels_to_expand = split_df[\"expand_this_label\"].to_list()\n",
    "\n",
    "            additional_input = predictions.todense()[:, labels_to_expand]\n",
    "\n",
    "            X_expanded = np.hstack([X.todense(), additional_input])\n",
    "            X_expanded = np.asarray(X_expanded)\n",
    "\n",
    "            temp_preds = self.second_layer_classifiers[i].predict(X_expanded)\n",
    "            second_layer_predictions.append(temp_preds)\n",
    "\n",
    "        return second_layer_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = StackingWithFTests()\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.asarray(a).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.5994983277591973\n",
      "Hamming loss:  0.08319397993311037\n"
     ]
    }
   ],
   "source": [
    "accuracy_score = metrics.accuracy_score(y_test, b)\n",
    "hamming_loss = metrics.hamming_loss(y_test, b)\n",
    "\n",
    "print(\"Accuracy score: \", accuracy_score)\n",
    "print(\"Hamming loss: \", hamming_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
